{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pavitra-khare/DA6401_Deep_learning/blob/main/DL_Assignment1_ALL_QUES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xo8cqH5AZtpw",
        "outputId": "c2525dee-637d-4505-e8ae-2dfb924a6c62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.8)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.22.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPqGqpf8ZyTJ",
        "outputId": "d8fa7ffd-b011-42c6-8bc4-a5231220a6d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "!wandb login --relogin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        },
        "id": "YTwbwB2PRtv5",
        "outputId": "001259ff-507c-4d64-f889-221c6a7ff3bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m3628-pavitrakhare\u001b[0m (\u001b[33m3628-pavitrakhare-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_043951-5fw2a33d</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/5fw2a33d' target=\"_blank\">fancy-eon-218</a></strong> to <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/5fw2a33d' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/5fw2a33d</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fancy-eon-218</strong> at: <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/5fw2a33d' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/5fw2a33d</a><br> View project at: <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice</a><br>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_043951-5fw2a33d/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "# Initialize Weights & Biases (W&B) for experiment tracking\n",
        "wandb.init(project=\"DA6401_ASS-practice\")\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Define class names for Fashion MNIST\n",
        "def get_class_names():\n",
        "    return [\n",
        "        'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
        "    ]\n",
        "\n",
        "# Select one sample image for each class\n",
        "def select_sample_images(images, labels, class_names):\n",
        "    chosen_images = []\n",
        "    seen_labels = set()\n",
        "    for idx, lbl in enumerate(labels):\n",
        "        if lbl not in seen_labels:\n",
        "            chosen_images.append((images[idx], class_names[lbl]))\n",
        "            seen_labels.add(lbl)\n",
        "        if len(chosen_images) == len(class_names):\n",
        "            break\n",
        "    return chosen_images\n",
        "\n",
        "# Plot sample images\n",
        "def plot_sample_images(sample_images):\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "    fig.suptitle(\"Sample Images from Fashion MNIST\", fontsize=16)\n",
        "\n",
        "    for ax, (image, label) in zip(axes.flatten(), sample_images):\n",
        "        ax.imshow(image, cmap='gray')\n",
        "        ax.set_title(label)\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "# Main Execution\n",
        "class_names = get_class_names()\n",
        "sample_images = select_sample_images(train_images, train_labels, class_names)\n",
        "fig = plot_sample_images(sample_images)\n",
        "\n",
        "# Log the figure to W&B\n",
        "wandb.log({\"Sample MNIST Images\": fig})\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#constants"
      ],
      "metadata": {
        "id": "oe0Ry8AWL9Xb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "woM9oY91yNp3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "import wandb\n",
        "ep = 10\n",
        "bs = 30\n",
        "lf = 'cross_entropy'\n",
        "op = 'nadam'\n",
        "lr = 1e-3\n",
        "data_set = 'fashion_mnist'\n",
        "\n",
        "m_beta = 0.9\n",
        "rmsprop_beta = 0.9\n",
        "beta_1 = 0.9\n",
        "beta_2 = 0.999\n",
        "epsilon_ = 1e-3\n",
        "wdc = 0\n",
        "winit = 'Xavier'\n",
        "nhls = 3\n",
        "hls = 128\n",
        "af = 'relu'\n",
        "# constants\n",
        "INPUT_KEY = 'input_size'\n",
        "OUTPUT_KEY = 'output_size'\n",
        "FUN_KEY = \"function\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dg-rVgsqyPTz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist, mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def map_data_with_classes(labels):\n",
        "\n",
        "    num_samples = len(labels)\n",
        "    num_classes = max(labels) + 1\n",
        "    one_hot_matrix = np.zeros((num_samples, num_classes))\n",
        "\n",
        "    for idx, label in enumerate(labels):\n",
        "        one_hot_matrix[idx][label] = 1\n",
        "\n",
        "    return one_hot_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset based on the chosen option\n",
        "if data_set == 'fashion_mnist':\n",
        "    (train_X, train_Y), (test_X, test_Y) = fashion_mnist.load_data()\n",
        "elif data_set == 'mnist':\n",
        "    (train_X, train_Y), (test_X, test_Y) = mnist.load_data()\n",
        "else:\n",
        "    raise ValueError(\"Invalid dataset choice! Choose 'fashion_mnist' or 'mnist'.\")\n",
        "\n",
        "train_X, test_X = train_X / 255.0, test_X / 255.0\n",
        "\n",
        "needed_y_train, needed_y_test = train_Y, test_Y\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "trainX, val_X, trainy, valy = train_test_split(train_X, train_Y, test_size=0.1, random_state=40)\n",
        "\n",
        "# Reshape images into 1D feature vectors\n",
        "trainX = trainX.reshape(trainX.shape[0], -1)\n",
        "testX = test_X.reshape(test_X.shape[0], -1)\n",
        "valX = val_X.reshape(val_X.shape[0], -1)\n",
        "\n",
        "# Adjust dataset size to be multiples of 128\n",
        "batch_size = 128\n",
        "trainX, testX, valX = (arr[:(len(arr) // batch_size) * batch_size] for arr in [trainX, testX, valX])\n",
        "trainy, test_Y, valy = (arr[:(len(arr) // batch_size) * batch_size] for arr in [trainy, test_Y, valy])\n",
        "\n",
        "# Convert class labels into one-hot encoded format\n",
        "trainy = map_data_with_classes(trainy)\n",
        "testy = map_data_with_classes(test_Y)\n",
        "valiy = map_data_with_classes(valy)\n",
        "\n",
        "# Determine input and output layer sizes\n",
        "input_layer_size = trainX.shape[1]\n",
        "output_layer_size = trainy.shape[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eQgIUDaLm8u",
        "outputId": "92788d1e-a4ff-4a84-e1d5-a9124a4e41bf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkw3JSs2Er1A"
      },
      "source": [
        "# ACTIVATION FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AsUqQNWJEr1B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"Computes the sigmoid activation function.\"\"\"\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "    \"\"\"Computes the hyperbolic tangent (tanh) activation function.\"\"\"\n",
        "    return np.tanh(x)\n",
        "\n",
        "def relu(x):\n",
        "    \"\"\"Computes the ReLU (Rectified Linear Unit) activation function.\"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def softmax(x):\n",
        "\n",
        "    x_shifted = x - np.max(x, axis=1, keepdims=True)  # Improve numerical stability\n",
        "    exp_x = np.exp(x_shifted)\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy(y_hat, y):\n",
        "\n",
        "    epsilon = 1e-9  # Avoid log(0) errors\n",
        "    return -np.mean(np.sum(y * np.log(y_hat + epsilon), axis=1))\n",
        "\n",
        "def mean_squared_error(y_hat, y):\n",
        "\n",
        "    return np.mean((y - y_hat) ** 2)\n",
        "\n",
        "def activation_functions(x, fn_label=\"sigmoid\"):\n",
        "\n",
        "    activations = {\n",
        "        \"relu\": relu,\n",
        "        \"sigmoid\": sigmoid,\n",
        "        \"tanh\": tanh,\n",
        "        \"softmax\": softmax\n",
        "    }\n",
        "    return activations.get(fn_label, lambda x: \"error\")(x)\n",
        "\n",
        "def activation_derivative(x, fn_label=\"sigmoid\"):\n",
        "\n",
        "    derivatives = {\n",
        "        \"relu\": lambda x: np.where(x > 0, 1, 0),\n",
        "        \"tanh\": lambda x: 1.0 - np.tanh(x) ** 2,\n",
        "        \"sigmoid\": lambda x: sigmoid(x) * (1 - sigmoid(x))\n",
        "    }\n",
        "    return derivatives.get(fn_label, lambda x: \"error\")(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4seIE9KVyYtS"
      },
      "outputs": [],
      "source": [
        "def initialize_weights_and_biases(layers, number_hidden_layers=1, init_type='random'):\n",
        "\n",
        "    weights, biases = [], []\n",
        "\n",
        "    for i in range(number_hidden_layers + 1):\n",
        "        input_dim, output_dim = layers[i][\"input_size\"], layers[i][\"output_size\"]\n",
        "\n",
        "        if init_type == 'random':\n",
        "            w = np.random.randn(output_dim, input_dim) * 0.01\n",
        "            b = np.zeros((output_dim, 1))\n",
        "        else:\n",
        "            bound = np.sqrt(6 / (input_dim + output_dim))\n",
        "            w = np.random.uniform(-bound, bound, (output_dim, input_dim))\n",
        "            b = np.random.uniform(-bound, bound, (output_dim, 1))\n",
        "\n",
        "\n",
        "        weights.append(w)\n",
        "        biases.append(b)\n",
        "\n",
        "    return weights, biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "V9O4fxVCyj1k"
      },
      "outputs": [],
      "source": [
        "def train_accuracy(batch_testy, y_predicted, trainy):\n",
        "\n",
        "    correct_count = 0\n",
        "\n",
        "    for i in range(len(batch_testy)):\n",
        "        for j in range(len(batch_testy[i])):\n",
        "            actual_label = np.argmax(batch_testy[i][j])\n",
        "            predicted_label = np.argmax(y_predicted[i][j])\n",
        "\n",
        "            if predicted_label == actual_label:\n",
        "                correct_count += 1\n",
        "\n",
        "    return correct_count / len(trainy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HtCKIfwDyojW"
      },
      "outputs": [],
      "source": [
        "def test_accuracy(testX, testy, weights, biases, number_hidden_layers, activation_function, output_function):\n",
        "\n",
        "    _, activations = forward_propagation(testX, weights, biases, number_hidden_layers, activation_function, output_function)\n",
        "    y_pred = activations[-1]  # Get final layer activations\n",
        "    y_predicted = np.argmax(y_pred, axis=1)  # Convert probabilities to class indices\n",
        "\n",
        "    # Convert testy to class indices if one-hot encoded\n",
        "    if testy.ndim > 1 and testy.shape[1] > 1:\n",
        "        testy = np.argmax(testy, axis=1)\n",
        "\n",
        "    # Ensure both lists are of the same length\n",
        "    min_len = min(len(y_predicted), len(testy))\n",
        "    return np.mean(y_predicted[:min_len] == testy[:min_len])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OQkX66WIysgN"
      },
      "outputs": [],
      "source": [
        "def calculate_regularizing_term(y, weight_decay_const, number_hidden_layers, weights):\n",
        "\n",
        "    total_weight_sum = sum(np.sum(weight ** 2) for weight in weights)\n",
        "    return (weight_decay_const / (2 * len(y))) * total_weight_sum\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0rmchKtEr1C"
      },
      "source": [
        "# Question-2 Forward Propogation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oUg8hjxWy3id",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a80c221e-58b7-4e89-d170-11e12c439caa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability distribution for 10 test samples:\n",
            "[[0.04607625 0.12301395 0.08769111 0.13195683 0.05925127 0.07121774\n",
            "  0.11371135 0.10199177 0.13111489 0.13397485]\n",
            " [0.0523033  0.04645739 0.07578059 0.15073963 0.12064091 0.08463482\n",
            "  0.15583983 0.07703636 0.1380381  0.09852907]\n",
            " [0.04962176 0.08165709 0.10511082 0.1219829  0.08351949 0.08387817\n",
            "  0.11428186 0.08881315 0.13579927 0.13533549]\n",
            " [0.04386434 0.0973892  0.09445235 0.09711673 0.08326172 0.09234284\n",
            "  0.15116948 0.12484775 0.13343449 0.08212109]\n",
            " [0.04671095 0.07486323 0.11248751 0.10626382 0.09167026 0.10479008\n",
            "  0.14725841 0.10746496 0.13637495 0.07211585]\n",
            " [0.05012159 0.09695575 0.11452272 0.09810505 0.08733495 0.07624954\n",
            "  0.13653945 0.10877083 0.12234192 0.10905821]\n",
            " [0.03793907 0.08494913 0.09647896 0.13022376 0.09186645 0.10302093\n",
            "  0.11909537 0.07267891 0.15469114 0.10905628]\n",
            " [0.03564656 0.10291206 0.08773628 0.16885298 0.07663047 0.07766287\n",
            "  0.09706733 0.09859962 0.14662664 0.10826518]\n",
            " [0.0305557  0.06881072 0.07100663 0.13921823 0.05786646 0.10853255\n",
            "  0.14226148 0.12505562 0.20486038 0.05183223]\n",
            " [0.04790719 0.05633379 0.10443259 0.11347778 0.09507075 0.08437154\n",
            "  0.16778587 0.08937942 0.14448512 0.09675594]]\n"
          ]
        }
      ],
      "source": [
        "def forward_propagation(input_x, W, B, hidden_layers, activ_label, op_label):\n",
        "\n",
        "\n",
        "    # Initialize lists for activations and hidden states\n",
        "    a, h = [], []\n",
        "\n",
        "    # Reshape input if necessary\n",
        "    input_x = input_x.reshape(len(input_x), -1)\n",
        "\n",
        "    # Compute activations for first hidden layer\n",
        "    first_activation = np.dot(W[0], input_x.T) + B[0]\n",
        "    a.append(first_activation)\n",
        "    h.append(activation_functions(first_activation, activ_label))\n",
        "\n",
        "    # Forward propagate through hidden layers\n",
        "    for i in range(1, hidden_layers):\n",
        "        layer_activation = np.dot(W[i], h[i-1]) + B[i]\n",
        "        a.append(layer_activation)\n",
        "        h.append(activation_functions(layer_activation, activ_label))\n",
        "\n",
        "    # Compute activations for the output layer\n",
        "    output_activation = np.dot(W[hidden_layers], h[-1]) + B[hidden_layers]\n",
        "    final_output = activation_functions(output_activation.T, op_label).T\n",
        "    a.append(output_activation)\n",
        "    h.append(final_output)\n",
        "\n",
        "    # Ensure all activations are transposed for consistency\n",
        "    a = [activation.T for activation in a]\n",
        "    h = [hidden.T for hidden in h]\n",
        "\n",
        "    return a, h\n",
        "\n",
        "input_layer_size = trainX.shape[1]  # Get number of features from trainX\n",
        "output_layer_size = trainy.shape[1]\n",
        "layershere = [\n",
        "    {\"input_size\": input_layer_size, \"output_size\": 128},\n",
        "    {\"input_size\": 128, \"output_size\": 64},\n",
        "    {\"input_size\": 64, \"output_size\": output_layer_size}\n",
        "]\n",
        "\n",
        "# Initialize weights and biases\n",
        "We, Bai = initialize_weights_and_biases(layershere, number_hidden_layers=2, init_type='Xavier')\n",
        "\n",
        "# Choose a random sample from the test set for forward propagation\n",
        "# sample_input = trainX[:10]  # Selecting 10 test samples\n",
        "\n",
        "# Perform forward propagation\n",
        "act, hi = forward_propagation(trainX[:10], We, Bai, hidden_layers=2, activ_label='relu', op_label='softmax')\n",
        "\n",
        "# Print the probability distribution of the 10 classes\n",
        "print(\"Probability distribution for 10 test samples:\")\n",
        "print(hi[-1])  # h[-1] contains the output layer activations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BmFElJbcy0Xm"
      },
      "outputs": [],
      "source": [
        "def val_loss(valX, valy, weights, biases, number_hidden_layers, activation_function, output_function, loss_function):\n",
        "\n",
        "\n",
        "    # Perform forward propagation\n",
        "    _, h = forward_propagation(valX, weights, biases, number_hidden_layers, activation_function, output_function)\n",
        "\n",
        "    # Retrieve the predicted values\n",
        "    y_pred = h[-1]\n",
        "\n",
        "    # Compute loss based on the chosen function\n",
        "    if loss_function == 'cross_entropy':\n",
        "        error = cross_entropy(y_pred, valy)\n",
        "    elif loss_function == 'mean_squared_error':\n",
        "        error = mean_squared_error(y_pred, valy)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid loss function. Choose 'cross_entropy' or 'mean_squared_error'.\")\n",
        "\n",
        "    return error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QUES 3"
      ],
      "metadata": {
        "id": "6LpXRgwUMSer"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yVOz85vEy_cP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def backward_propagation(batch_trainy, batch_trainX, y_hat, activations, hidden_states, weights, num_hidden_layers, derivative_function='sigmoid'):\n",
        "\n",
        "\n",
        "    weight_gradients, bias_gradients, activation_gradients, hidden_gradients = {}, {}, {}, {}\n",
        "\n",
        "    # Reshape batch_trainy to ensure correct dimensions\n",
        "    batch_trainy = batch_trainy.reshape(batch_trainy.shape[0], batch_trainy.shape[1])\n",
        "\n",
        "    epsilon = 1e-8  # Small value to prevent division errors\n",
        "    last_activation_key = f'a{num_hidden_layers + 1}'\n",
        "    last_hidden_key = f'h{num_hidden_layers + 1}'\n",
        "\n",
        "    activation_gradients[last_activation_key] = -(batch_trainy - y_hat)\n",
        "    hidden_gradients[last_hidden_key] = -(batch_trainy / (y_hat + epsilon))\n",
        "\n",
        "    num_samples = len(batch_trainX)\n",
        "\n",
        "    # Backpropagation from output layer to first hidden layer\n",
        "    for layer in range(num_hidden_layers + 1, 1, -1):\n",
        "        weight_key = f'W{layer}'\n",
        "        bias_key = f'b{layer}'\n",
        "        activation_key = f'a{layer}'\n",
        "        prev_activation_key = f'a{layer - 1}'\n",
        "        prev_hidden_key = f'h{layer - 1}'\n",
        "\n",
        "        # Compute weight gradient\n",
        "        weight_gradients[weight_key] = np.dot(activation_gradients[activation_key].T, hidden_states[layer - 2])\n",
        "\n",
        "        # Apply L2 regularization\n",
        "        weight_gradients[weight_key] += (wdc * weights[layer - 1])\n",
        "        weight_gradients[weight_key] /= num_samples\n",
        "\n",
        "        # Compute bias gradient\n",
        "        bias_gradients[bias_key] = activation_gradients[activation_key]\n",
        "\n",
        "        # Compute hidden gradients and activation gradients\n",
        "        hidden_gradients[prev_hidden_key] = np.dot(weights[layer - 1].T, activation_gradients[activation_key].T)\n",
        "        activation_gradients[prev_activation_key] = np.multiply(hidden_gradients[prev_hidden_key], activation_derivative(activations[layer - 2].T, derivative_function))\n",
        "        activation_gradients[prev_activation_key] = activation_gradients[prev_activation_key].T\n",
        "\n",
        "    # Compute gradients for the first layer (no hidden gradients needed)\n",
        "    weight_gradients['W1'] = np.dot(activation_gradients['a1'].T, batch_trainX)\n",
        "    bias_gradients['b1'] = activation_gradients['a1']\n",
        "\n",
        "    # Normalize biases across samples\n",
        "    for layer in range(1, len(bias_gradients) + 1):\n",
        "        bias_key = f'b{layer}'\n",
        "        bias_gradients[bias_key] = np.mean(bias_gradients[bias_key], axis=0).reshape(-1, 1)\n",
        "\n",
        "    return weight_gradients, bias_gradients\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QUES 4-6"
      ],
      "metadata": {
        "id": "_JWik6EgMavA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Mhcgh-6azEMu"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(trainX, trainy, number_hidden_layers=1, hidden_layer_size=4, eta=0.1, initial_weights='random', activation_function='sigmoid', epochs=1, output_function='softmax', mini_batch_size=4, loss_function='cross_entropy', weight_decay_const=0, wandb_flag=False):\n",
        "    INPUT_KEY = 'input_size'\n",
        "    OUTPUT_KEY = 'output_size'\n",
        "    FUN_KEY = \"function\"\n",
        "    layers = [{INPUT_KEY: input_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function}]\n",
        "\n",
        "    for _ in range(number_hidden_layers - 1):\n",
        "        layers.append({INPUT_KEY: hidden_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function})\n",
        "\n",
        "    layers.append({INPUT_KEY: hidden_layer_size, OUTPUT_KEY: output_layer_size, FUN_KEY: output_function})\n",
        "\n",
        "    # Initialize model parameters\n",
        "    weights, biases = initialize_weights_and_biases(layers, number_hidden_layers, initial_weights)\n",
        "\n",
        "    x_total = len(trainX)\n",
        "    num_batches = x_total // mini_batch_size\n",
        "    mini_batch_trainX = np.array_split(trainX, num_batches)\n",
        "    mini_batch_trainy = np.array_split(trainy, num_batches)\n",
        "\n",
        "    train_loss_list, val_loss_list, train_acc_list, val_acc_list = [], [], [], []\n",
        "    h = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_train_loss = 0\n",
        "        predictions = []\n",
        "\n",
        "        for batch_idx in range(len(mini_batch_trainX)):\n",
        "            activations, h = forward_propagation(mini_batch_trainX[batch_idx], weights, biases, number_hidden_layers, activation_function, output_function)\n",
        "            predictions.append(h[-1])\n",
        "\n",
        "            if loss_function == 'cross_entropy':\n",
        "                total_train_loss += cross_entropy(h[-1], mini_batch_trainy[batch_idx])\n",
        "            elif loss_function == 'mean_squared_error':\n",
        "                total_train_loss += mean_squared_error(h[-1], mini_batch_trainy[batch_idx])\n",
        "            else:\n",
        "                raise ValueError('Invalid loss function specified')\n",
        "\n",
        "            del_W, del_b = backward_propagation(mini_batch_trainy[batch_idx], mini_batch_trainX[batch_idx], h[-1], activations, h, weights, number_hidden_layers, activation_function)\n",
        "\n",
        "            for idx in range(len(weights)):\n",
        "                weights[idx] -= eta * del_W[f'W{idx + 1}']\n",
        "                biases[idx] -= eta * del_b[f'b{idx + 1}']\n",
        "\n",
        "        reg_train_loss = calculate_regularizing_term(trainy, weight_decay_const, number_hidden_layers, weights)\n",
        "        final_train_loss = total_train_loss / num_batches + reg_train_loss\n",
        "\n",
        "        val_loss_value = val_loss(valX, valiy, weights, biases, number_hidden_layers, activation_function, output_function, loss_function)\n",
        "        reg_val_loss = calculate_regularizing_term(valiy, weight_decay_const, number_hidden_layers, weights)\n",
        "        final_val_loss = val_loss_value + reg_val_loss\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}: Validation Loss = {final_val_loss}\")\n",
        "\n",
        "        train_loss_list.append(final_train_loss)\n",
        "        val_loss_list.append(final_val_loss)\n",
        "\n",
        "        train_acc = train_accuracy(mini_batch_trainy, predictions, trainy)\n",
        "        train_acc_list.append(train_acc)\n",
        "\n",
        "        val_acc = test_accuracy(valX, valy, weights, biases, number_hidden_layers, activation_function, output_function)\n",
        "        val_acc_list.append(val_acc)\n",
        "\n",
        "        if wandb_flag:\n",
        "            wandb.log({\"loss\": final_train_loss, \"val_loss\": final_val_loss, \"accuracy\": train_acc, \"val_accuracy\": val_acc, \"epoch\": epoch})\n",
        "\n",
        "    return h[-1], weights, biases, [train_loss_list, val_loss_list, train_acc_list, val_acc_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8K1wq6IIzJN_"
      },
      "outputs": [],
      "source": [
        "def momentum_based_gradient_descent(trainX, trainy, number_hidden_layers=1, hidden_layer_size=4, eta=0.1, initial_weights='random', activation_function='sigmoid', epochs=1, output_function='softmax', mini_batch_size=4, loss_function='cross_entropy', weight_decay_const=0, wandb_flag=False, m_beta=0.9):\n",
        "\n",
        "    # Define layer configurations\n",
        "    layers = []\n",
        "    input_layer = {'input_size': trainX.shape[1], 'output_size': hidden_layer_size, 'activation': activation_function}\n",
        "    layers.append(input_layer)\n",
        "\n",
        "    for _ in range(number_hidden_layers - 1):\n",
        "        hidden_layer = {'input_size': hidden_layer_size, 'output_size': hidden_layer_size, 'activation': activation_function}\n",
        "        layers.append(hidden_layer)\n",
        "\n",
        "    output_layer = {'input_size': hidden_layer_size, 'output_size': trainy.shape[1], 'activation': output_function}\n",
        "    layers.append(output_layer)\n",
        "\n",
        "    # Initialize weights and biases\n",
        "    weights, biases = initialize_weights_and_biases(layers,number_hidden_layers, initial_weights)\n",
        "\n",
        "    # Split data into mini-batches\n",
        "    num_batches = len(trainX) // mini_batch_size\n",
        "    mini_batches_X = np.array_split(trainX, num_batches)\n",
        "    mini_batches_y = np.array_split(trainy, num_batches)\n",
        "\n",
        "    # Initialize momentum terms\n",
        "    momentum_weights = [np.zeros_like(w) for w in weights]\n",
        "    momentum_biases = [np.zeros_like(b) for b in biases]\n",
        "\n",
        "    # Lists to store training and validation metrics\n",
        "    train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        predictions = []\n",
        "\n",
        "        for batch_X, batch_y in zip(mini_batches_X, mini_batches_y):\n",
        "            # Forward propagation\n",
        "            activations, hidden_states = forward_propagation(batch_X, weights, biases, number_hidden_layers, activation_function, output_function)\n",
        "            predictions.append(hidden_states[-1])\n",
        "\n",
        "            # Compute loss\n",
        "            if loss_function == 'cross_entropy':\n",
        "                batch_loss = cross_entropy(hidden_states[-1], batch_y)\n",
        "            elif loss_function == 'mean_squared_error':\n",
        "                batch_loss = mean_squared_error(hidden_states[-1], batch_y)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported loss function\")\n",
        "\n",
        "            epoch_loss += batch_loss\n",
        "\n",
        "            # Backward propagation\n",
        "            grad_weights, grad_biases = backward_propagation(batch_y, batch_X, hidden_states[-1], activations, hidden_states, weights, number_hidden_layers, activation_function)\n",
        "\n",
        "            # Update weights and biases with momentum\n",
        "            for i in range(len(weights)):\n",
        "                # Access gradients using keys (e.g., 'W1', 'b1', 'W2', 'b2', etc.)\n",
        "                keyW = 'W' + str(i + 1)\n",
        "                keyB = 'b' + str(i + 1)\n",
        "\n",
        "                # Momentum update for weights\n",
        "                momentum_weights[i] = momentum_weights[i] * m_beta + grad_weights[keyW] * eta\n",
        "                weights[i] -= momentum_weights[i]\n",
        "\n",
        "                # Momentum update for biases\n",
        "                momentum_biases[i] = momentum_biases[i] * m_beta + grad_biases[keyB] * eta\n",
        "                biases[i] -= momentum_biases[i]\n",
        "\n",
        "        # Calculate training accuracy and loss\n",
        "        train_acc = train_accuracy(mini_batches_y,predictions,trainy)\n",
        "        reg_term_train = calculate_regularizing_term(trainy,weight_decay_const,number_hidden_layers ,weights)\n",
        "        avg_train_loss = epoch_loss / num_batches + reg_term_train\n",
        "\n",
        "        # Calculate validation accuracy and loss\n",
        "        val_acc = test_accuracy(valX, valiy, weights, biases, number_hidden_layers, activation_function, output_function)\n",
        "        val_loss_value = val_loss(valX, valiy, weights, biases, number_hidden_layers, activation_function, output_function, loss_function)\n",
        "        reg_term_val = calculate_regularizing_term(valiy,weight_decay_const ,number_hidden_layers, weights)\n",
        "        val_loss_value += reg_term_val\n",
        "\n",
        "        # Store metrics\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(val_loss_value)\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        # Log metrics if wandb is enabled\n",
        "        if wandb_flag:\n",
        "            wandb.log({\n",
        "                \"loss\": avg_train_loss,\n",
        "                \"val_loss\": val_loss_value,\n",
        "                \"accuracy\": train_acc,\n",
        "                \"val_accuracy\": val_acc,\n",
        "                \"epoch\": epoch\n",
        "            })\n",
        "\n",
        "        print(f\"Epoch: {epoch + 1}, Validation Loss: {val_loss_value}\")\n",
        "\n",
        "    # Return final predictions, weights, biases, and metrics\n",
        "    return hidden_states[-1], weights, biases, [train_losses, val_losses, train_accuracies, val_accuracies]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rmsprop(trainX, trainy, number_hidden_layers=1, hidden_layer_size=4, eta=0.1, initial_weights='random',\n",
        "            activation_function='sigmoid', epochs=1, output_function='softmax', mini_batch_size=4,\n",
        "            loss_function='cross_entropy', weight_decay_const=0, wandb_flag=False):\n",
        "\n",
        "    layers = [{INPUT_KEY: input_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function}]\n",
        "    layers.extend([{INPUT_KEY: hidden_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function}\n",
        "                   for _ in range(number_hidden_layers - 1)])\n",
        "    layers.append({INPUT_KEY: hidden_layer_size, OUTPUT_KEY: output_layer_size, FUN_KEY: output_function})\n",
        "\n",
        "    # Initialize weights and biases\n",
        "    weights, biases = initialize_weights_and_biases(layers, number_hidden_layers, initial_weights)\n",
        "\n",
        "    # Determine batch processing details\n",
        "    num_samples = len(trainX)\n",
        "    num_batches = num_samples // mini_batch_size\n",
        "    mini_batches_X = np.array_split(trainX, num_batches)\n",
        "    mini_batches_y = np.array_split(trainy, num_batches)\n",
        "\n",
        "    # RMSProp accumulators\n",
        "    v_weights = [np.zeros_like(w) for w in weights]\n",
        "    v_biases = [np.zeros_like(b) for b in biases]\n",
        "\n",
        "    train_loss_list, val_loss_list, train_acc_list, val_acc_list = [], [], [], []\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        y_preds = []\n",
        "\n",
        "        for batch_X, batch_y in zip(mini_batches_X, mini_batches_y):\n",
        "            activations, h_states = forward_propagation(batch_X, weights, biases, number_hidden_layers, activation_function, output_function)\n",
        "            y_preds.append(h_states[-1])\n",
        "\n",
        "            # Compute loss\n",
        "            epoch_loss += cross_entropy(h_states[-1], batch_y) if loss_function == 'cross_entropy' else mean_squared_error(h_states[-1], batch_y)\n",
        "\n",
        "            # Backpropagation\n",
        "            del_W, del_b = backward_propagation(batch_y, batch_X, h_states[-1], activations, h_states, weights, number_hidden_layers, activation_function)\n",
        "\n",
        "            # RMSProp weight update\n",
        "            for i in range(len(weights)):\n",
        "                v_weights[i] = rmsprop_beta * v_weights[i] + (1 - rmsprop_beta) * (del_W[f'W{i+1}'] ** 2)\n",
        "                v_biases[i] = rmsprop_beta * v_biases[i] + (1 - rmsprop_beta) * (del_b[f'b{i+1}'] ** 2)\n",
        "\n",
        "                weights[i] -= eta * del_W[f'W{i+1}'] / (np.sqrt(v_weights[i] + epsilon_))\n",
        "                biases[i] -= eta * del_b[f'b{i+1}'] / (np.sqrt(v_biases[i] + epsilon_))\n",
        "\n",
        "        # Compute loss & accuracy\n",
        "        train_loss = (epoch_loss / num_batches) + calculate_regularizing_term(trainy, weight_decay_const, number_hidden_layers, weights)\n",
        "        val_loss_value = val_loss(valX, valiy, weights, biases, number_hidden_layers, activation_function, output_function, loss_function)\n",
        "        val_loss_value += calculate_regularizing_term(valiy, weight_decay_const, number_hidden_layers, weights)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Validation Loss = {val_loss_value:.4f}\")\n",
        "\n",
        "        # Accuracy Calculation\n",
        "        train_acc_list.append(train_accuracy(mini_batches_y, y_preds, trainy))\n",
        "        val_acc_list.append(test_accuracy(valX, valy, weights, biases, number_hidden_layers, activation_function, output_function))\n",
        "        train_loss_list.append(train_loss)\n",
        "        val_loss_list.append(val_loss_value)\n",
        "\n",
        "        if wandb_flag:\n",
        "            wandb.log({\"loss\": train_loss, \"val_loss\": val_loss_value, \"accuracy\": train_acc_list[-1], \"val_accuracy\": val_acc_list[-1], \"epoch\": epoch+1})\n",
        "\n",
        "    return h_states[-1], weights, biases, [train_loss_list, val_loss_list, train_acc_list, val_acc_list]\n",
        "\n"
      ],
      "metadata": {
        "id": "f2GGGlTHMrsR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nestrov_accelerated_gradient_descent(trainX, trainy, number_hidden_layers=1, hidden_layer_size=4, eta=0.1,\n",
        "                                         initial_weights='random', activation_function='sigmoid', epochs=1,\n",
        "                                         output_function='softmax', mini_batch_size=4, loss_function='cross_entropy',\n",
        "                                         weight_decay_const=0, wandb_flag=False):\n",
        "    \"\"\"\n",
        "    Implements Nesterov Accelerated Gradient Descent (NAG) for neural network optimization.\n",
        "    \"\"\"\n",
        "    # Define network layers dynamically\n",
        "    layers = [{INPUT_KEY: input_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function}]\n",
        "    layers.extend([{INPUT_KEY: hidden_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function}\n",
        "                   for _ in range(number_hidden_layers - 1)])\n",
        "    layers.append({INPUT_KEY: hidden_layer_size, OUTPUT_KEY: output_layer_size, FUN_KEY: output_function})\n",
        "\n",
        "    # Initialize weights and biases\n",
        "    weights, biases = initialize_weights_and_biases(layers, number_hidden_layers, initial_weights)\n",
        "\n",
        "    # Determine batch processing details\n",
        "    num_samples = len(trainX)\n",
        "    num_batches = num_samples // mini_batch_size\n",
        "    mini_batches_X = np.array_split(trainX, num_batches)\n",
        "    mini_batches_y = np.array_split(trainy, num_batches)\n",
        "\n",
        "    # Initialize past gradients for momentum update\n",
        "    momentum_weights = [np.zeros_like(w) for w in weights]\n",
        "    momentum_biases = [np.zeros_like(b) for b in biases]\n",
        "\n",
        "    # Lists to track training progress\n",
        "    loss_train, loss_val, acc_train, acc_val = [], [], [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        y_predictions = []\n",
        "\n",
        "        for batch_X, batch_y in zip(mini_batches_X, mini_batches_y):\n",
        "            # Compute lookahead weights and biases\n",
        "            lookahead_W = [weights[i] - (m_beta * momentum_weights[i]) for i in range(len(weights))]\n",
        "            lookahead_B = [biases[i] - (m_beta * momentum_biases[i]) for i in range(len(biases))]\n",
        "\n",
        "            activations, h_states = forward_propagation(batch_X, lookahead_W, lookahead_B, number_hidden_layers, activation_function, output_function)\n",
        "            y_predictions.append(h_states[-1])\n",
        "\n",
        "            # Compute loss\n",
        "            total_loss += cross_entropy(h_states[-1], batch_y) if loss_function == 'cross_entropy' else mean_squared_error(h_states[-1], batch_y)\n",
        "\n",
        "            # Compute gradients\n",
        "            grad_W, grad_B = backward_propagation(batch_y, batch_X, h_states[-1], activations, h_states, lookahead_W, number_hidden_layers, activation_function)\n",
        "\n",
        "            # Apply Nesterov update\n",
        "            for i in range(len(weights)):\n",
        "                momentum_weights[i] = (m_beta * momentum_weights[i]) + (eta * grad_W[f'W{i+1}'])\n",
        "                momentum_biases[i] = (m_beta * momentum_biases[i]) + (eta * grad_B[f'b{i+1}'])\n",
        "\n",
        "                weights[i] -= momentum_weights[i]\n",
        "                biases[i] -= momentum_biases[i]\n",
        "\n",
        "        # Compute regularized loss\n",
        "        reg_term_train = calculate_regularizing_term(trainy, weight_decay_const, number_hidden_layers, weights)\n",
        "        avg_train_loss = (total_loss / num_batches) + reg_term_train\n",
        "\n",
        "        # Validation loss calculation\n",
        "        val_loss_value = val_loss(valX, valiy, weights, biases, number_hidden_layers, activation_function, output_function, loss_function)\n",
        "        val_loss_value += calculate_regularizing_term(valiy, weight_decay_const, number_hidden_layers, weights)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}: Validation Loss = {val_loss_value:.4f}\")\n",
        "\n",
        "        # Compute accuracy metrics\n",
        "        train_acc = train_accuracy(mini_batches_y, y_predictions, trainy)\n",
        "        val_acc = test_accuracy(valX, valy, weights, biases, number_hidden_layers, activation_function, output_function)\n",
        "\n",
        "        # Store metrics\n",
        "        loss_train.append(avg_train_loss)\n",
        "        loss_val.append(val_loss_value)\n",
        "        acc_train.append(train_acc)\n",
        "        acc_val.append(val_acc)\n",
        "\n",
        "        # Log metrics if W&B logging is enabled\n",
        "        if wandb_flag:\n",
        "            wandb.log({\"loss\": avg_train_loss, \"val_loss\": val_loss_value, \"accuracy\": train_acc, \"val_accuracy\": val_acc, \"epoch\": epoch + 1})\n",
        "\n",
        "    return h_states[-1], weights, biases, [loss_train, loss_val, acc_train, acc_val]\n"
      ],
      "metadata": {
        "id": "ityBXn5zMk6X"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adam(trainX, trainy, number_hidden_layers=1, hidden_layer_size=4, eta=0.1, initial_weights='random',\n",
        "         activation_function='sigmoid', epochs=1, output_function='softmax', mini_batch_size=4,\n",
        "         loss_function='cross_entropy', weight_decay_const=0, wandb_flag=False):\n",
        "\n",
        "    layers = [{INPUT_KEY: input_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function}]\n",
        "    layers.extend([{INPUT_KEY: hidden_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function}\n",
        "                   for _ in range(number_hidden_layers - 1)])\n",
        "    layers.append({INPUT_KEY: hidden_layer_size, OUTPUT_KEY: output_layer_size, FUN_KEY: output_function})\n",
        "\n",
        "    # Initialize weights and biases\n",
        "    weights, biases = initialize_weights_and_biases(layers, number_hidden_layers, initial_weights)\n",
        "\n",
        "    # Determine batch processing details\n",
        "    num_samples = len(trainX)\n",
        "    num_batches = num_samples // mini_batch_size\n",
        "    mini_batches_X = np.array_split(trainX, num_batches)\n",
        "    mini_batches_y = np.array_split(trainy, num_batches)\n",
        "\n",
        "    # Adam accumulators\n",
        "    v_weights, v_biases = [np.zeros_like(w) for w in weights], [np.zeros_like(b) for b in biases]\n",
        "    m_weights, m_biases = [np.zeros_like(w) for w in weights], [np.zeros_like(b) for b in biases]\n",
        "\n",
        "    train_loss_list, val_loss_list, train_acc_list, val_acc_list = [], [], [], []\n",
        "    time_step = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        y_preds = []\n",
        "\n",
        "        for batch_X, batch_y in zip(mini_batches_X,mini_batches_y):\n",
        "            time_step += 1\n",
        "            activations, h_states = forward_propagation(batch_X, weights, biases, number_hidden_layers, activation_function, output_function)\n",
        "            y_preds.append(h_states[-1])\n",
        "\n",
        "            # Compute loss\n",
        "            epoch_loss += cross_entropy(h_states[-1], batch_y) if loss_function == 'cross_entropy' else mean_squared_error(h_states[-1], batch_y)\n",
        "\n",
        "            # Backpropagation\n",
        "            del_W, del_b = backward_propagation(batch_y, batch_X, h_states[-1], activations, h_states, weights, number_hidden_layers, activation_function)\n",
        "\n",
        "            # Adam weight update\n",
        "            for i in range(len(weights)):\n",
        "                m_weights[i] = beta_1 * m_weights[i] + (1 - beta_1) * del_W[f'W{i+1}']\n",
        "                m_biases[i] = beta_1 * m_biases[i] + (1 - beta_1) * del_b[f'b{i+1}']\n",
        "\n",
        "                v_weights[i] = beta_2 * v_weights[i] + (1 - beta_2) * (del_W[f'W{i+1}'] ** 2)\n",
        "                v_biases[i] = beta_2 * v_biases[i] + (1 - beta_2) * (del_b[f'b{i+1}'] ** 2)\n",
        "\n",
        "                # Bias correction\n",
        "                m_hat_w, v_hat_w = m_weights[i] / (1 - beta_1 ** time_step), v_weights[i] / (1 - beta_2 ** time_step)\n",
        "                m_hat_b, v_hat_b = m_biases[i] / (1 - beta_1 ** time_step), v_biases[i] / (1 - beta_2 ** time_step)\n",
        "\n",
        "                weights[i] -= eta * m_hat_w / (np.sqrt(v_hat_w) + epsilon_)\n",
        "                biases[i] -= eta * m_hat_b / (np.sqrt(v_hat_b) + epsilon_)\n",
        "\n",
        "        # Compute loss & accuracy\n",
        "        train_loss = (epoch_loss / num_batches) + calculate_regularizing_term(trainy, weight_decay_const, number_hidden_layers, weights)\n",
        "        val_loss_value = val_loss(valX, valiy, weights, biases, number_hidden_layers, activation_function, output_function, loss_function)\n",
        "        val_loss_value += calculate_regularizing_term(valiy, weight_decay_const, number_hidden_layers, weights)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Validation Loss = {val_loss_value:.4f}\")\n",
        "\n",
        "        # Store accuracy\n",
        "        train_acc_list.append(train_accuracy(mini_batches_y, y_preds, trainy))\n",
        "        val_acc_list.append(test_accuracy(valX, valy, weights, biases, number_hidden_layers, activation_function, output_function))\n",
        "\n",
        "        if wandb_flag:\n",
        "            wandb.log({\"loss\": train_loss, \"val_loss\": val_loss_value, \"accuracy\": train_acc_list[-1], \"val_accuracy\": val_acc_list[-1], \"epoch\": epoch+1})\n",
        "\n",
        "    return h_states[-1], weights, biases, [train_loss_list, val_loss_list, train_acc_list, val_acc_list]\n"
      ],
      "metadata": {
        "id": "9o9BAipvMx9p"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nadam(trainX, trainy, number_hidden_layers=1, hidden_layer_size=4, eta=0.1, initial_weights='random',\n",
        "          activation_function='sigmoid', epochs=1, output_function='softmax', mini_batch_size=4,\n",
        "          loss_function='cross_entropy', weight_decay_const=0, wandb_flag=False):\n",
        "\n",
        "    # Define network layers dynamically\n",
        "    layers = [{INPUT_KEY: input_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function}]\n",
        "    layers.extend([{INPUT_KEY: hidden_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function}\n",
        "                   for _ in range(number_hidden_layers - 1)])\n",
        "    layers.append({INPUT_KEY: hidden_layer_size, OUTPUT_KEY: output_layer_size, FUN_KEY: output_function})\n",
        "\n",
        "    # Initialize weights and biases\n",
        "    weights, biases = initialize_weights_and_biases(layers, number_hidden_layers, initial_weights)\n",
        "\n",
        "    # Determine batch processing details\n",
        "    num_samples = len(trainX)\n",
        "    num_batches = num_samples // mini_batch_size\n",
        "    mini_batches_X = np.array_split(trainX, num_batches)\n",
        "    mini_batches_y = np.array_split(trainy, num_batches)\n",
        "\n",
        "    # Initialize moving averages for Nadam\n",
        "    v_weights, v_biases = [np.zeros_like(w) for w in weights], [np.zeros_like(b) for b in biases]\n",
        "    m_weights, m_biases = [np.zeros_like(w) for w in weights], [np.zeros_like(b) for b in biases]\n",
        "\n",
        "    train_loss_list, val_loss_list, train_acc_list, val_acc_list = [], [], [], []\n",
        "    beta_1, beta_2, epsilon = 0.9, 0.999, 1e-3\n",
        "    t = 0  # Step counter\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_train_loss = 0\n",
        "        y_preds = []\n",
        "\n",
        "        for X_batch, y_batch in zip(mini_batches_X,mini_batches_y):\n",
        "            t += 1  # Increment time step\n",
        "\n",
        "            # Compute bias-corrected moving averages\n",
        "            v_hat_weights = [v / (1 - beta_2 ** t) for v in v_weights]\n",
        "            v_hat_biases = [v / (1 - beta_2 ** t) for v in v_biases]\n",
        "\n",
        "            m_hat_weights = [m / (1 - beta_1 ** t) for m in m_weights]\n",
        "            m_hat_biases = [m / (1 - beta_1 ** t) for m in m_biases]\n",
        "\n",
        "            # Compute \"lookahead\" weights & biases\n",
        "            lookahead_weights = [weights[i] - (m_hat_weights[i] / np.sqrt(v_hat_weights[i] + epsilon)) * eta\n",
        "                                 for i in range(len(weights))]\n",
        "            lookahead_biases = [biases[i] - (m_hat_biases[i] / np.sqrt(v_hat_biases[i] + epsilon)) * eta\n",
        "                                for i in range(len(biases))]\n",
        "\n",
        "            # Forward propagation\n",
        "            activations, outputs = forward_propagation(X_batch, lookahead_weights, lookahead_biases, number_hidden_layers, activation_function, output_function)\n",
        "            y_preds.append(outputs[-1])\n",
        "\n",
        "            # Compute loss\n",
        "            loss_func = cross_entropy if loss_function == 'cross_entropy' else mean_squared_error\n",
        "            total_train_loss += loss_func(outputs[-1], y_batch)\n",
        "\n",
        "            # Backpropagation\n",
        "            grad_W, grad_B = backward_propagation(y_batch, X_batch, outputs[-1], activations, outputs, lookahead_weights, number_hidden_layers, activation_function)\n",
        "\n",
        "            # Update moving averages & apply Nadam update rule\n",
        "            for i in range(len(weights)):\n",
        "                v_weights[i] = beta_2 * v_weights[i] + (1 - beta_2) * (grad_W[f'W{i+1}'] ** 2)\n",
        "                v_biases[i] = beta_2 * v_biases[i] + (1 - beta_2) * (grad_B[f'b{i+1}'] ** 2)\n",
        "\n",
        "                m_weights[i] = beta_1 * m_weights[i] + (1 - beta_1) * grad_W[f'W{i+1}']\n",
        "                m_biases[i] = beta_1 * m_biases[i] + (1 - beta_1) * grad_B[f'b{i+1}']\n",
        "\n",
        "                v_hat_w, v_hat_b = v_weights[i] / (1 - beta_2 ** t), v_biases[i] / (1 - beta_2 ** t)\n",
        "                m_hat_w, m_hat_b = m_weights[i] / (1 - beta_1 ** t), m_biases[i] / (1 - beta_1 ** t)\n",
        "\n",
        "                weights[i] -= (m_hat_w * eta) / np.sqrt(v_hat_w + epsilon)\n",
        "                biases[i] -= (m_hat_b * eta) / np.sqrt(v_hat_b + epsilon)\n",
        "\n",
        "        # Compute losses with regularization\n",
        "        reg_train = calculate_regularizing_term(trainy, weight_decay_const, number_hidden_layers, weights)\n",
        "        avg_train_loss = total_train_loss / num_batches + reg_train\n",
        "        val_loss_value = val_loss(valX, valiy, weights, biases, number_hidden_layers, activation_function, output_function, loss_function)\n",
        "        val_loss_value += calculate_regularizing_term(valiy, weight_decay_const, number_hidden_layers, weights)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}: Validation Loss = {val_loss_value:.4f}\")\n",
        "\n",
        "        # Compute accuracy\n",
        "        train_acc_list.append(train_accuracy(mini_batches_y, y_preds, trainy))\n",
        "        val_acc_list.append(test_accuracy(valX, valy, weights, biases, number_hidden_layers, activation_function, output_function))\n",
        "\n",
        "        # Store metrics\n",
        "        train_loss_list.append(avg_train_loss)\n",
        "        val_loss_list.append(val_loss_value)\n",
        "\n",
        "        if wandb_flag:\n",
        "            wandb.log({\n",
        "                \"loss\": avg_train_loss,\n",
        "                \"val_loss\": val_loss_value,\n",
        "                \"accuracy\": train_acc_list[-1],\n",
        "                \"val_accuracy\": val_acc_list[-1],\n",
        "                \"epoch\": epoch + 1\n",
        "            })\n",
        "\n",
        "    return outputs[-1], weights, biases, [train_loss_list, val_loss_list, train_acc_list, val_acc_list]\n"
      ],
      "metadata": {
        "id": "QpVBae0tM2YS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60cJwHKDzWnS",
        "outputId": "84e347f9-c4cb-4684-923b-23a0576bd8cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 70m0cjz9\n",
            "Sweep URL: https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/sweeps/70m0cjz9\n"
          ]
        }
      ],
      "source": [
        "def train(trainX, trainy, textX, testy, number_hidden_layers, hidden_layer_size, eta, init_type, activation_function,\n",
        "          epochs, mini_batch_size, loss_function, optimizer, output_function, weight_decay_const, wandb_flag=False):\n",
        "\n",
        "    wdc = weight_decay_const\n",
        "\n",
        "    optimizer_functions = {\n",
        "        'sgd': gradient_descent,\n",
        "        'momentum': momentum_based_gradient_descent,\n",
        "        'nag': nestrov_accelerated_gradient_descent,\n",
        "        'rmsprop': rmsprop,\n",
        "        'adam': adam,\n",
        "        'nadam': nadam\n",
        "    }\n",
        "\n",
        "    if optimizer not in optimizer_functions:\n",
        "        raise ValueError(f\"Invalid optimizer: {optimizer}. Choose from {list(optimizer_functions.keys())}.\")\n",
        "\n",
        "    hL, weights, biases, plot_list = optimizer_functions[optimizer](\n",
        "        trainX, trainy, number_hidden_layers, hidden_layer_size, eta, init_type, activation_function, epochs,\n",
        "        output_function, mini_batch_size, loss_function, wdc, wandb_flag\n",
        "    )\n",
        "\n",
        "    return [weights, biases, number_hidden_layers, activation_function, output_function]\n",
        "\n",
        "\n",
        "# Configuration for Bayesian Sweep Optimization\n",
        "sweep_config = {\n",
        "    \"name\": \"Hyperparameter Sweep\",\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\n",
        "        \"name\": \"val_accuracy\",\n",
        "        \"goal\": \"maximize\"\n",
        "    },\n",
        "    \"parameters\": {\n",
        "        \"epochs\": {\"values\": [5, 10]},\n",
        "        \"init_method\": {\"values\": [\"random\", \"Xavier\"]},\n",
        "        \"hidden_layers\": {\"values\": [3, 4, 5]},\n",
        "        \"layer_size\": {\"values\": [32, 64, 128]},\n",
        "        \"activation\": {\"values\": [\"tanh\", \"sigmoid\", \"relu\"]},\n",
        "        \"learning_rate\": {\"values\": [0.001, 0.0001]},\n",
        "        \"weight_decay\": {\"values\": [0, 0.0005, 0.5]},\n",
        "        \"optimizer\": {\"values\": [\"sgd\", \"momentum\", \"nag\", \"rmsprop\", \"adam\", \"nadam\"]},\n",
        "        \"batch_size\": {\"values\": [16, 32, 64]}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Initialize the sweep with W&B\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"DA6401_ASS-practice\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "VmteGi74zgqT"
      },
      "outputs": [],
      "source": [
        "def train_data(config = None):\n",
        "  config_defaults = dict(\n",
        "          e=10,\n",
        "          nlh=2,\n",
        "          sz=128,\n",
        "          w_d=0,\n",
        "          lr=1e-3,\n",
        "          o=\"sgd\",\n",
        "          b=32,\n",
        "          a=\"sigmoid\",\n",
        "          w_i=\"Xavier\",\n",
        "          l=\"cross_entropy\",\n",
        "      )\n",
        "  wandb.init(project=\"DA6401_ASS-practice\",config = config_defaults)\n",
        "  config = wandb.config\n",
        "  wandb.run.name = 'hl_'+str(config.nlh)+'_sz_'+str(config.sz)+'_bs_'+str(config.b)+'_ac_'+config.a+'_w_i_'+config.w_i+'_lr_'+str(config.lr)+'_wd_'+str(config.w_d)\n",
        "  train(trainX=trainX,\n",
        "      trainy=trainy,\n",
        "      textX=testX,\n",
        "      testy=needed_y_test,\n",
        "      number_hidden_layers=config.nlh,\n",
        "      hidden_layer_size=config.sz,\n",
        "      eta=config.lr,\n",
        "      init_type=config.w_i,\n",
        "      activation_function=config.a,\n",
        "      epochs=config.e,\n",
        "      mini_batch_size=config.b,\n",
        "      loss_function=config.l,\n",
        "      optimizer=config.o,\n",
        "      output_function='softmax',\n",
        "      weight_decay_const=config.w_d,\n",
        "      wandb_flag=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qcS0qoUt-vqm",
        "outputId": "e7f160f1-6305-43ab-fdda-d296238f2a83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yxak8e9a with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_method: random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m3628-pavitrakhare\u001b[0m (\u001b[33m3628-pavitrakhare-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Ignoring project 'DA6401_ASS-practice' when running a sweep."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250316_175112-yxak8e9a</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/yxak8e9a' target=\"_blank\">silvery-sweep-1</a></strong> to <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/sweeps/70m0cjz9' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/sweeps/70m0cjz9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/sweeps/70m0cjz9' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/sweeps/70m0cjz9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/yxak8e9a' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/yxak8e9a</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch :  1  validation loss :  1.885298395831047\n",
            "epoch :  2  validation loss :  1.6361207986209625\n",
            "epoch :  3  validation loss :  1.441008077305503\n",
            "epoch :  4  validation loss :  1.2885156038003878\n",
            "epoch :  5  validation loss :  1.169253062348122\n",
            "epoch :  6  validation loss :  1.075444672747091\n",
            "epoch :  7  validation loss :  1.0011935545074977\n",
            "epoch :  8  validation loss :  0.9417327450319068\n",
            "epoch :  9  validation loss :  0.8933034931859113\n",
            "epoch :  10  validation loss :  0.8530714986523487\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▅▆▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▆▅▄▃▂▂▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▅▅▆▆▇▇██</td></tr><tr><td>val_loss</td><td>█▆▅▄▃▃▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.7323</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.87475</td></tr><tr><td>val_accuracy</td><td>0.73981</td></tr><tr><td>val_loss</td><td>0.85307</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">hl_2_sz_128_bs_32_ac_sigmoid_w_i_Xavier_lr_0.001_wd_0</strong> at: <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/yxak8e9a' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/yxak8e9a</a><br> View project at: <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250316_175112-yxak8e9a/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: i3534uae with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_method: random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Ignoring project 'DA6401_ASS-practice' when running a sweep."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250316_175212-i3534uae</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/i3534uae' target=\"_blank\">swift-sweep-2</a></strong> to <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/sweeps/70m0cjz9' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/sweeps/70m0cjz9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/sweeps/70m0cjz9' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/sweeps/70m0cjz9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/i3534uae' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/i3534uae</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch :  1  validation loss :  1.9146885109931946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.51761</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>2.08604</td></tr><tr><td>val_accuracy</td><td>0.65982</td></tr><tr><td>val_loss</td><td>1.91469</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">hl_2_sz_128_bs_32_ac_sigmoid_w_i_Xavier_lr_0.001_wd_0</strong> at: <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/i3534uae' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/i3534uae</a><br> View project at: <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250316_175212-i3534uae/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "wandb.agent(sweep_id,train_data,count=250)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW2zr0B0AgU4"
      },
      "source": [
        "#QUES 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 846
        },
        "id": "7jcuHEde0u_V",
        "outputId": "3d8650de-049b-48fc-f7ff-8e81d7779967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m3628-pavitrakhare\u001b[0m (\u001b[33m3628-pavitrakhare-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_183642-z5unaofi</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/z5unaofi' target=\"_blank\">golden-fog-222</a></strong> to <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/z5unaofi' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/z5unaofi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Validation Loss = 0.4542\n",
            "Epoch 2: Validation Loss = 0.4137\n",
            "Epoch 3: Validation Loss = 0.3981\n",
            "Epoch 4: Validation Loss = 0.3941\n",
            "Epoch 5: Validation Loss = 0.3870\n",
            "Epoch 6: Validation Loss = 0.3849\n",
            "Epoch 7: Validation Loss = 0.3875\n",
            "Epoch 8: Validation Loss = 0.3900\n",
            "Epoch 9: Validation Loss = 0.3914\n",
            "Epoch 10: Validation Loss = 0.3954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▄▅▆▆▇▇▇██</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▂▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▅▅▆▆▇▇██</td></tr><tr><td>val_loss</td><td>█▄▂▂▁▁▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.91384</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>loss</td><td>0.24339</td></tr><tr><td>val_accuracy</td><td>0.88519</td></tr><tr><td>val_loss</td><td>0.39542</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Confusion Matrix</strong> at: <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/z5unaofi' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/z5unaofi</a><br> View project at: <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_183642-z5unaofi/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "run = wandb.init(project=\"DA6401_ASS-practice\")\n",
        "run.name = 'Confusion Matrix'\n",
        "\n",
        "params = train(trainX=trainX,\n",
        "    trainy=trainy,\n",
        "    textX=testX,\n",
        "    testy=needed_y_test,\n",
        "    number_hidden_layers=3,\n",
        "    hidden_layer_size=128,\n",
        "    eta=0.001,\n",
        "    init_type=\"Xavier\",\n",
        "    activation_function=\"relu\",\n",
        "    epochs=10,\n",
        "    mini_batch_size=32,\n",
        "    loss_function=\"cross_entropy\",\n",
        "    optimizer=\"nadam\",\n",
        "    output_function='softmax',\n",
        "    weight_decay_const=0.5,\n",
        "    wandb_flag=True)\n",
        "a,h = forward_propagation(testX,params[0],params[1],params[2], params[3], params[4])\n",
        "hL = h[-1]\n",
        "y_pred = np.zeros(len(hL))\n",
        "i = 0\n",
        "while(i!=len(hL)):\n",
        "  y_pred[i] = np.argmax(hL[i])\n",
        "  i+=1\n",
        "y_pred\n",
        "\n",
        "class_names = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
        "\n",
        "wandb.log({\"The Confusion Marix\": wandb.plot.confusion_matrix(preds = y_pred,y_true=test_Y,class_names=class_names)})\n",
        "wandb.save()\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QUES 8"
      ],
      "metadata": {
        "id": "rvT7I6SXDZes"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "id": "HtyZx2-31HVy",
        "outputId": "5a327294-c46f-4aaa-d74d-18184e57a0db"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Ignoring project 'sweep_CS24m031' when running a sweep."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250316_175236-i3534uae</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/i3534uae' target=\"_blank\">hl_2_sz_128_bs_32_ac_sigmoid_w_i_Xavier_lr_0.001_wd_0</a></strong> to <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/sweeps/70m0cjz9' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/sweeps/70m0cjz9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/sweeps/70m0cjz9' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/sweeps/70m0cjz9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/i3534uae' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/i3534uae</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Validation Loss = 0.0483\n",
            "Epoch 2: Validation Loss = 0.0503\n",
            "Epoch 3: Validation Loss = 0.0540\n",
            "Epoch 4: Validation Loss = 0.0584\n",
            "Epoch 5: Validation Loss = 0.0625\n",
            "Epoch 6: Validation Loss = 0.0671\n",
            "Epoch 7: Validation Loss = 0.0717\n",
            "Epoch 8: Validation Loss = 0.0763\n",
            "Epoch 9: Validation Loss = 0.0814\n",
            "Epoch 10: Validation Loss = 0.0863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy on the model =  87.74038461538461 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▄▅▆▆▇▇▇██</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▃▂▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▆▇▇████</td></tr><tr><td>val_loss</td><td>▁▁▂▃▄▄▅▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.9128</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>loss</td><td>0.02026</td></tr><tr><td>val_accuracy</td><td>0.88638</td></tr><tr><td>val_loss</td><td>0.08631</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">mean square error</strong> at: <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/i3534uae' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/i3534uae</a><br> View project at: <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250316_175236-i3534uae/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "run = wandb.init(project=\"sweep_CS24m031\")\n",
        "run.name = 'mean square error'\n",
        "\n",
        "params = train(trainX=trainX,\n",
        "    trainy=trainy,\n",
        "    textX=testX,\n",
        "    testy=needed_y_test,\n",
        "    number_hidden_layers=3,\n",
        "    hidden_layer_size=128,\n",
        "    eta=0.001,\n",
        "    init_type=\"Xavier\",\n",
        "    activation_function=\"relu\",\n",
        "    epochs=10,\n",
        "    mini_batch_size=32,\n",
        "    loss_function=\"mean_squared_error\",\n",
        "    optimizer=\"nadam\",\n",
        "    output_function='softmax',\n",
        "    weight_decay_const=0.5,\n",
        "    wandb_flag=True)\n",
        "\n",
        "test_ac = test_accuracy(testX,needed_y_test,params[0],params[1],params[2],params[3],params[4])\n",
        "print(\"Test accuracy on the model = \", test_ac*100,'%')\n",
        "wandb.save()\n",
        "wandb.finish()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AgLTTRywERsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "_UUWa51z7j9U",
        "outputId": "3da553f9-33db-477e-c4f3-fe29d4726449"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_184249-8izkk67f</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/sweep_CS24m031/runs/8izkk67f' target=\"_blank\">comfy-haze-621</a></strong> to <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/sweep_CS24m031' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/sweep_CS24m031' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/sweep_CS24m031</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/sweep_CS24m031/runs/8izkk67f' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/sweep_CS24m031/runs/8izkk67f</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Validation Loss = 0.0483\n",
            "Epoch 2: Validation Loss = 0.0509\n",
            "Epoch 3: Validation Loss = 0.0544\n",
            "Epoch 4: Validation Loss = 0.0584\n",
            "Epoch 5: Validation Loss = 0.0627\n",
            "Epoch 6: Validation Loss = 0.0674\n",
            "Epoch 7: Validation Loss = 0.0723\n",
            "Epoch 8: Validation Loss = 0.0768\n",
            "Epoch 9: Validation Loss = 0.0817\n",
            "Epoch 10: Validation Loss = 0.0867\n",
            "Test accuracy on the model =  87.96073717948718 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▄▅▆▆▇▇███</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▃▂▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▇▆▆▇██</td></tr><tr><td>val_loss</td><td>▁▁▂▃▄▄▅▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.91141</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>loss</td><td>0.02046</td></tr><tr><td>val_accuracy</td><td>0.88689</td></tr><tr><td>val_loss</td><td>0.08666</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Cross Entropy</strong> at: <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/sweep_CS24m031/runs/8izkk67f' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/sweep_CS24m031/runs/8izkk67f</a><br> View project at: <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/sweep_CS24m031' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/sweep_CS24m031</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_184249-8izkk67f/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "run = wandb.init(project=\"sweep_CS24m031\")\n",
        "run.name = 'Cross Entropy'\n",
        "\n",
        "params = train(trainX=trainX,\n",
        "    trainy=trainy,\n",
        "    textX=testX,\n",
        "    testy=needed_y_test,\n",
        "    number_hidden_layers=3,\n",
        "    hidden_layer_size=128,\n",
        "    eta=0.001,\n",
        "    init_type=\"Xavier\",\n",
        "    activation_function=\"relu\",\n",
        "    epochs=10,\n",
        "    mini_batch_size=32,\n",
        "    loss_function=\"mean_squared_error\",\n",
        "    optimizer=\"nadam\",\n",
        "    output_function='softmax',\n",
        "    weight_decay_const=0.5,\n",
        "    wandb_flag=True)\n",
        "\n",
        "test_ac = test_accuracy(testX,needed_y_test,params[0],params[1],params[2],params[3],params[4])\n",
        "print(\"Test accuracy on the model = \", test_ac*100,'%')\n",
        "wandb.save()\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ques 10"
      ],
      "metadata": {
        "id": "pmy8VncWjHMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#CONFIGURATION 1\n",
        "run = wandb.init(project=\"DA6401_ASS-practice_ques10\")\n",
        "run.name = 'model1'\n",
        "\n",
        "params = train(trainX=trainX,\n",
        "    trainy=trainy,\n",
        "    textX=testX,\n",
        "    testy=needed_y_test,\n",
        "    number_hidden_layers=3,\n",
        "    hidden_layer_size=128,\n",
        "    eta=0.001,\n",
        "    init_type=\"Xavier\",\n",
        "    activation_function=\"relu\",\n",
        "    epochs=10,\n",
        "    mini_batch_size=32,\n",
        "    loss_function=\"cross_entropy\",\n",
        "    optimizer=\"nadam\",\n",
        "    output_function='softmax',\n",
        "    weight_decay_const=0.5,\n",
        "    wandb_flag=True)\n",
        "\n",
        "test_ac = test_accuracy(testX,needed_y_test,params[0],params[1],params[2],params[3],params[4])\n",
        "print(\"Test accuracy on the model = \", test_ac*100,'%')\n",
        "wandb.save()\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "id": "VpDl9xTaOfCb",
        "outputId": "0d32aab8-6edc-44c0-caa2-ab0afe788b07"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Ignoring project 'DA6401_ASS-practice_ques10' when running a sweep."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250314_200102-kybq2846</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/kybq2846' target=\"_blank\">hl_5_sz_128_bs_64_ac_relu_w_i_Xavier_lr_0.001_wd_0</a></strong> to <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/sweeps/7fbr94tp' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/sweeps/7fbr94tp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/sweeps/7fbr94tp' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/sweeps/7fbr94tp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/kybq2846' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/kybq2846</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Validation Loss = 0.1748\n",
            "Epoch 2: Validation Loss = 0.1405\n",
            "Epoch 3: Validation Loss = 0.1244\n",
            "Epoch 4: Validation Loss = 0.1225\n",
            "Epoch 5: Validation Loss = 0.1245\n",
            "Epoch 6: Validation Loss = 0.1342\n",
            "Epoch 7: Validation Loss = 0.1329\n",
            "Epoch 8: Validation Loss = 0.1399\n",
            "Epoch 9: Validation Loss = 0.1475\n",
            "Epoch 10: Validation Loss = 0.1586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy on the model =  97.44591346153845 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▅▆▇▇█████</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▇█▇▇█▇▇</td></tr><tr><td>val_loss</td><td>█▃▁▁▁▃▂▃▄▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.99395</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>loss</td><td>0.02596</td></tr><tr><td>val_accuracy</td><td>0.97554</td></tr><tr><td>val_loss</td><td>0.15863</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">model1</strong> at: <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/kybq2846' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/kybq2846</a><br> View project at: <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250314_200102-kybq2846/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CONFIGURATION 2\n",
        "\n",
        "run = wandb.init(project=\"DA6401_ASS-practice_ques10\")\n",
        "run.name = 'model2'\n",
        "\n",
        "params = train(trainX=trainX,\n",
        "    trainy=trainy,\n",
        "    textX=testX,\n",
        "    testy=needed_y_test,\n",
        "    number_hidden_layers=3,\n",
        "    hidden_layer_size=128,\n",
        "    eta=0.001,\n",
        "    init_type=\"Xavier\",\n",
        "    activation_function=\"relu\",\n",
        "    epochs=10,\n",
        "    mini_batch_size=32,\n",
        "    loss_function=\"cross_entropy\",\n",
        "    optimizer=\"nag\",\n",
        "    output_function='softmax',\n",
        "    weight_decay_const=0.5,\n",
        "    wandb_flag=True)\n",
        "\n",
        "test_ac = test_accuracy(testX,needed_y_test,params[0],params[1],params[2],params[3],params[4])\n",
        "print(\"Test accuracy on the model = \", test_ac*100,'%')\n",
        "wandb.save()\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 861
        },
        "id": "Kl6rJrKnRXRC",
        "outputId": "45839c5f-2ba3-4d4a-965c-5a1510c646c8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Ignoring project 'DA6401_ASS-practice_ques10' when running a sweep."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250314_200816-kybq2846</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/kybq2846' target=\"_blank\">model1</a></strong> to <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/sweeps/7fbr94tp' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/sweeps/7fbr94tp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/sweeps/7fbr94tp' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/sweeps/7fbr94tp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/kybq2846' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/kybq2846</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Validation Loss = 0.1938\n",
            "Epoch 2: Validation Loss = 0.1502\n",
            "Epoch 3: Validation Loss = 0.1319\n",
            "Epoch 4: Validation Loss = 0.1244\n",
            "Epoch 5: Validation Loss = 0.1206\n",
            "Epoch 6: Validation Loss = 0.1179\n",
            "Epoch 7: Validation Loss = 0.1170\n",
            "Epoch 8: Validation Loss = 0.1181\n",
            "Epoch 9: Validation Loss = 0.1195\n",
            "Epoch 10: Validation Loss = 0.1240\n",
            "Test accuracy on the model =  97.68629807692307 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▅▆▆▇▇▇███</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▂▂▁▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.99395</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>loss</td><td>0.03178</td></tr><tr><td>val_accuracy</td><td>0.98166</td></tr><tr><td>val_loss</td><td>0.12398</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">model1</strong> at: <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/kybq2846' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice/runs/kybq2846</a><br> View project at: <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250314_200816-kybq2846/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CONFIGURATION 3\n",
        "\n",
        "run = wandb.init(project=\"DA6401_ASS-practice_ques10\")\n",
        "run.name = 'model1'\n",
        "\n",
        "params = train(trainX=trainX,\n",
        "    trainy=trainy,\n",
        "    textX=testX,\n",
        "    testy=needed_y_test,\n",
        "    number_hidden_layers=3,\n",
        "    hidden_layer_size=128,\n",
        "    eta=0.001,\n",
        "    init_type=\"Xavier\",\n",
        "    activation_function=\"relu\",\n",
        "    epochs=5,\n",
        "    mini_batch_size=32,\n",
        "    loss_function=\"cross_entropy\",\n",
        "    optimizer=\"adam\",\n",
        "    output_function='softmax',\n",
        "    weight_decay_const=0.5,\n",
        "    wandb_flag=True)\n",
        "\n",
        "test_ac = test_accuracy(testX,needed_y_test,params[0],params[1],params[2],params[3],params[4])\n",
        "print(\"Test accuracy on the model = \", test_ac*100,'%')\n",
        "wandb.save()\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "QI7QPJcoGs0X",
        "outputId": "36a5eccc-4693-4b41-f045-ba3de66f2ed1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Validation Loss = 0.1454\n",
            "Epoch 2: Validation Loss = 0.1052\n",
            "Epoch 3: Validation Loss = 0.0937\n",
            "Epoch 4: Validation Loss = 0.0964\n",
            "Epoch 5: Validation Loss = 0.0855\n",
            "Test accuracy on the model =  97.48597756410257 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▆▁▆▇██</td></tr><tr><td>epoch</td><td>▁▃▁▃▅▆█</td></tr><tr><td>loss</td><td>█▃█▃▂▁▁</td></tr><tr><td>val_accuracy</td><td>▃▆▁▄▆▇█</td></tr><tr><td>val_loss</td><td>▆▂█▃▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.98582</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>loss</td><td>0.04269</td></tr><tr><td>val_accuracy</td><td>0.97758</td></tr><tr><td>val_loss</td><td>0.08551</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">model1</strong> at: <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice_ques10/runs/5hlud9xg' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice_ques10/runs/5hlud9xg</a><br> View project at: <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice_ques10' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-practice_ques10</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_050647-5hlud9xg/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "import wandb\n",
        "import argparse\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"Computes the sigmoid activation function.\"\"\"\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "    \"\"\"Computes the hyperbolic tangent (tanh) activation function.\"\"\"\n",
        "    return np.tanh(x)\n",
        "\n",
        "def relu(x):\n",
        "    \"\"\"Computes the ReLU (Rectified Linear Unit) activation function.\"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def softmax(x):\n",
        "\n",
        "    x_shifted = x - np.max(x, axis=1, keepdims=True)  # Improve numerical stability\n",
        "    exp_x = np.exp(x_shifted)\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "def identity(x):\n",
        "    \"\"\"Computes the identity activation function (returns input as output).\"\"\"\n",
        "    return x\n",
        "\n",
        "def cross_entropy(y_hat, y):\n",
        "\n",
        "    epsilon = 1e-9  # Avoid log(0) errors\n",
        "    return -np.mean(np.sum(y * np.log(y_hat + epsilon), axis=1))\n",
        "\n",
        "def mean_squared_error(y_hat, y):\n",
        "\n",
        "    return np.mean((y - y_hat) ** 2)\n",
        "\n",
        "def activation_functions(x, fn_label=\"sigmoid\"):\n",
        "\n",
        "    activations = {\n",
        "        \"relu\": relu,\n",
        "        \"sigmoid\": sigmoid,\n",
        "        \"tanh\": tanh,\n",
        "        \"softmax\": softmax,\n",
        "        \"identity\": identity\n",
        "    }\n",
        "    return activations.get(fn_label, lambda x: \"error\")(x)\n",
        "\n",
        "def activation_derivative(x, fn_label=\"sigmoid\"):\n",
        "\n",
        "    derivatives = {\n",
        "        \"relu\": lambda x: np.where(x > 0, 1, 0),\n",
        "        \"tanh\": lambda x: 1.0 - np.tanh(x) ** 2,\n",
        "        \"sigmoid\": lambda x: sigmoid(x) * (1 - sigmoid(x)),\n",
        "        \"identity\": lambda x: np.ones_like(x)\n",
        "    }\n",
        "    return derivatives.get(fn_label, lambda x: \"error\")(x)\n",
        "\n",
        "\n",
        "\n",
        "def map_data_with_classes(labels):\n",
        "\n",
        "    num_samples = len(labels)\n",
        "    num_classes = max(labels) + 1\n",
        "    one_hot_matrix = np.zeros((num_samples, num_classes))\n",
        "\n",
        "    for idx, label in enumerate(labels):\n",
        "        one_hot_matrix[idx][label] = 1\n",
        "\n",
        "    return one_hot_matrix\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"-wp\",\"--wandb_project\",help=\"Project name used to track experiments in Weights & Biases dashboard\",default=\"DA6401_ass_1_run\")\n",
        "parser.add_argument(\"-we\",\"--wandb_entity\",help=\"Wandb Entity used to track experiments in the Weights & Biases dashboard.\",default=\"cs24m031\")\n",
        "parser.add_argument(\"-d\",\"--dataset\",help=\"choices: ['mnist', 'fashion_mnist']\",choices=['mnist', 'fashion_mnist'],default=\"fashion_mnist\")\n",
        "parser.add_argument(\"-e\",\"--epochs\",help=\"Number of epochs to train neural network.\",choices=['5','10'],default=10)\n",
        "parser.add_argument(\"-b\",\"--batch_size\",help=\"Batch size used to train neural network.\",choices=['16','32','64'],default=32)\n",
        "parser.add_argument(\"-l\",\"--loss\",help=\"choices: ['mean_squared_error', 'cross_entropy']\",choices=['cross_entropy','mean_squared_error'],default='cross_entropy')\n",
        "parser.add_argument(\"-o\",\"--optimizer\",help=\"choices: ['sgd', 'momentum', 'nag', 'rmsprop', 'adam', 'nadam']\",choices=['sgd','momentum','nag','rmsprop','adam','nadam'],default='nadam')\n",
        "parser.add_argument(\"-lr\",\"--learning_rate\",help=\"Learning rate used to optimize model parameters\",choices=['1e-3','1e-4'],default='1e-3')\n",
        "parser.add_argument(\"-m\",\"--momentum\",help=\"Momentum used by momentum and nag optimizers.\",choices=['0.5'],default=0.5)\n",
        "parser.add_argument(\"-beta\",\"--beta\",help=\"Beta used by rmsprop optimizer\",choices=['0.5'],default=0.5)\n",
        "parser.add_argument(\"-beta1\",\"--beta1\",help=\"Beta1 used by adam and nadam optimizers.\",choices=['0.5'],default=0.5)\n",
        "parser.add_argument(\"-beta2\",\"--beta2\",help=\"Beta2 used by adam and nadam optimizers.\",choices=['0.5'],default=0.5)\n",
        "parser.add_argument(\"-eps\",\"--epsilon\",help=\"Epsilon used by optimizers.\",choices=['0.000001'],default=0.001)\n",
        "parser.add_argument(\"-w_d\",\"--weight_decay\",help=\"Weight decay used by optimizers.\",choices=['0','0.0005','0.5'],default=0)\n",
        "parser.add_argument(\"-w_i\",\"--weight_init\",help=\"choices: ['random', 'Xavier']\",choices=['random','Xavier'],default='Xavier')\n",
        "parser.add_argument(\"-nhl\",\"--num_layers\",help=\"Number of hidden layers used in feedforward neural network.\",choices=['3','4','5'],default=3)\n",
        "parser.add_argument(\"-sz\",\"--hidden_size\",help=\"Number of hidden neurons in a feedforward layer.\",choices=['32','64','128'],default=128)\n",
        "parser.add_argument(\"-a\",\"--activation\",help=\"choices: ['identity', 'sigmoid', 'tanh', 'ReLU']\",choices=['identity','sigmoid','tanh','ReLU'],default='ReLU')\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "m_beta = args.beta\n",
        "rmsprop_beta = 0.9\n",
        "beta_1 = args.beta1\n",
        "beta_2 = args.beta2\n",
        "epsilon_ = args.epsilon\n",
        "wdc = args.weight_decay\n",
        "\n",
        "INPUT_KEY = 'input_size'\n",
        "OUTPUT_KEY = 'output_size'\n",
        "FUN_KEY = \"function\"\n",
        "\n",
        "#load the data----------------------------\n",
        "\n",
        "if args.dataset == 'fashion_mnist':\n",
        "    (train_X,train_Y),(test_X,test_Y) = fashion_mnist.load_data()\n",
        "else:\n",
        "    (train_X,train_Y),(test_X,test_Y) = mnist.load_data()\n",
        "\n",
        "\n",
        "train_X, test_X = train_X / 255.0, test_X / 255.0\n",
        "\n",
        "needed_y_train, needed_y_test = train_Y, test_Y\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "trainX, val_X, trainy, valy = train_test_split(train_X, train_Y, test_size=0.1, random_state=40)\n",
        "\n",
        "# Reshape images into 1D feature vectors\n",
        "trainX = trainX.reshape(trainX.shape[0], -1)\n",
        "testX = test_X.reshape(test_X.shape[0], -1)\n",
        "valX = val_X.reshape(val_X.shape[0], -1)\n",
        "\n",
        "# Adjust dataset size to be multiples of 128\n",
        "batch_size = 128\n",
        "trainX, testX, valX = (arr[:(len(arr) // batch_size) * batch_size] for arr in [trainX, testX, valX])\n",
        "trainy, test_Y, valy = (arr[:(len(arr) // batch_size) * batch_size] for arr in [trainy, test_Y, valy])\n",
        "\n",
        "# Convert class labels into one-hot encoded format\n",
        "trainy = map_data_with_classes(trainy)\n",
        "testy = map_data_with_classes(test_Y)\n",
        "valiy = map_data_with_classes(valy)\n",
        "\n",
        "# Determine input and output layer sizes\n",
        "input_layer_size = trainX.shape[1]\n",
        "output_layer_size = trainy.shape[1]\n",
        "\n",
        "# function to initialize the weights and biases\n",
        "def initialize_weights_and_biases(layers,number_hidden_layers = 1,init_type='random'):\n",
        "\n",
        "  weights ,biases = [],[]\n",
        "  OUTPUT_KEY = \"output_size\"\n",
        "  INPUT_KEY = \"input_size\"\n",
        "  if(init_type == 'random'):\n",
        "    for i in range(number_hidden_layers+1):\n",
        "      weights.append(np.random.normal(0,0.5,(layers[i][OUTPUT_KEY],layers[i][INPUT_KEY])))\n",
        "      biases.append(np.random.normal(0,0.5,(layers[i][OUTPUT_KEY],1)))\n",
        "  else:\n",
        "    # Xaviar\n",
        "    for i in range(number_hidden_layers+1):\n",
        "      right_x = np.sqrt(6/(layers[i][OUTPUT_KEY] + layers[i][INPUT_KEY]))\n",
        "      left_x = -1 * right_x\n",
        "      weights.append(np.random.uniform(left_x, right_x, size=(layers[i][OUTPUT_KEY], layers[i][INPUT_KEY])))\n",
        "      biases.append(np.random.uniform(left_x, right_x, size=(layers[i][OUTPUT_KEY], 1)))\n",
        "  return weights,biases\n",
        "\n",
        "\n",
        "def train_accuracy(batch_testy, y_predicted, trainy):\n",
        "\n",
        "    correct_count = 0\n",
        "\n",
        "    for i in range(len(batch_testy)):\n",
        "        for j in range(len(batch_testy[i])):\n",
        "            actual_label = np.argmax(batch_testy[i][j])\n",
        "            predicted_label = np.argmax(y_predicted[i][j])\n",
        "\n",
        "            if predicted_label == actual_label:\n",
        "                correct_count += 1\n",
        "\n",
        "    return correct_count / len(trainy)\n",
        "\n",
        "# calculate validation and test accuracy\n",
        "def test_accuracy(testX, testy, weights, biases, number_hidden_layers, activation_function, output_function):\n",
        "\n",
        "    _, activations = forward_propagation(testX, weights, biases, number_hidden_layers, activation_function, output_function)\n",
        "    y_pred = activations[-1]  # Get final layer activations\n",
        "    y_predicted = np.argmax(y_pred, axis=1)  # Convert probabilities to class indices\n",
        "\n",
        "    # Convert testy to class indices if one-hot encoded\n",
        "    if testy.ndim > 1 and testy.shape[1] > 1:\n",
        "        testy = np.argmax(testy, axis=1)\n",
        "\n",
        "    # Ensure both lists are of the same length\n",
        "    min_len = min(len(y_predicted), len(testy))\n",
        "    return np.mean(y_predicted[:min_len] == testy[:min_len])\n",
        "\n",
        "# calculating regularizing term that is to be added to loss\n",
        "def calculate_regularizing_term(y, weight_decay_const, number_hidden_layers, weights):\n",
        "\n",
        "    total_weight_sum = sum(np.sum(weight ** 2) for weight in weights)\n",
        "    return (weight_decay_const / (2 * len(y))) * total_weight_sum\n",
        "\n",
        "# calculating validation loss\n",
        "def val_loss(valX, valy, weights, biases, number_hidden_layers, activation_function, output_function, loss_function):\n",
        "\n",
        "\n",
        "    # Perform forward propagation\n",
        "    _, h = forward_propagation(valX, weights, biases, number_hidden_layers, activation_function, output_function)\n",
        "\n",
        "    # Retrieve the predicted values\n",
        "    y_pred = h[-1]\n",
        "\n",
        "    # Compute loss based on the chosen function\n",
        "    if loss_function == 'cross_entropy':\n",
        "        error = cross_entropy(y_pred, valy)\n",
        "    elif loss_function == 'mean_squared_error':\n",
        "        error = mean_squared_error(y_pred, valy)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid loss function. Choose 'cross_entropy' or 'mean_squared_error'.\")\n",
        "\n",
        "    return error\n",
        "\n",
        "\n",
        "\n",
        "def forward_propagation(input_x, W, B, hidden_layers, activ_label, op_label):\n",
        "\n",
        "\n",
        "    # Initialize lists for activations and hidden states\n",
        "    a, h = [], []\n",
        "\n",
        "    # Reshape input if necessary\n",
        "    input_x = input_x.reshape(len(input_x), -1)\n",
        "\n",
        "    # Compute activations for first hidden layer\n",
        "    first_activation = np.dot(W[0], input_x.T) + B[0]\n",
        "    a.append(first_activation)\n",
        "    h.append(activation_functions(first_activation, activ_label))\n",
        "\n",
        "    # Forward propagate through hidden layers\n",
        "    for i in range(1, hidden_layers):\n",
        "        layer_activation = np.dot(W[i], h[i-1]) + B[i]\n",
        "        a.append(layer_activation)\n",
        "        h.append(activation_functions(layer_activation, activ_label))\n",
        "\n",
        "    # Compute activations for the output layer\n",
        "    output_activation = np.dot(W[hidden_layers], h[-1]) + B[hidden_layers]\n",
        "    final_output = activation_functions(output_activation.T, op_label).T\n",
        "    a.append(output_activation)\n",
        "    h.append(final_output)\n",
        "\n",
        "    # Ensure all activations are transposed for consistency\n",
        "    a = [activation.T for activation in a]\n",
        "    h = [hidden.T for hidden in h]\n",
        "\n",
        "    return a, h\n",
        "\n",
        "def backward_propagation(batch_trainy, batch_trainX, y_hat, activations, hidden_states, weights, num_hidden_layers, derivative_function='sigmoid'):\n",
        "\n",
        "\n",
        "    weight_gradients, bias_gradients, activation_gradients, hidden_gradients = {}, {}, {}, {}\n",
        "\n",
        "    # Reshape batch_trainy to ensure correct dimensions\n",
        "    batch_trainy = batch_trainy.reshape(batch_trainy.shape[0], batch_trainy.shape[1])\n",
        "\n",
        "    epsilon = 1e-8  # Small value to prevent division errors\n",
        "    last_activation_key = f'a{num_hidden_layers + 1}'\n",
        "    last_hidden_key = f'h{num_hidden_layers + 1}'\n",
        "\n",
        "    activation_gradients[last_activation_key] = -(batch_trainy - y_hat)\n",
        "    hidden_gradients[last_hidden_key] = -(batch_trainy / (y_hat + epsilon))\n",
        "\n",
        "    num_samples = len(batch_trainX)\n",
        "\n",
        "    # Backpropagation from output layer to first hidden layer\n",
        "    for layer in range(num_hidden_layers + 1, 1, -1):\n",
        "        weight_key = f'W{layer}'\n",
        "        bias_key = f'b{layer}'\n",
        "        activation_key = f'a{layer}'\n",
        "        prev_activation_key = f'a{layer - 1}'\n",
        "        prev_hidden_key = f'h{layer - 1}'\n",
        "\n",
        "        # Compute weight gradient\n",
        "        weight_gradients[weight_key] = np.dot(activation_gradients[activation_key].T, hidden_states[layer - 2])\n",
        "\n",
        "        # Apply L2 regularization\n",
        "        weight_gradients[weight_key] += (wdc * weights[layer - 1])\n",
        "        weight_gradients[weight_key] /= num_samples\n",
        "\n",
        "        # Compute bias gradient\n",
        "        bias_gradients[bias_key] = activation_gradients[activation_key]\n",
        "\n",
        "        # Compute hidden gradients and activation gradients\n",
        "        hidden_gradients[prev_hidden_key] = np.dot(weights[layer - 1].T, activation_gradients[activation_key].T)\n",
        "        activation_gradients[prev_activation_key] = np.multiply(hidden_gradients[prev_hidden_key], activation_derivative(activations[layer - 2].T, derivative_function))\n",
        "        activation_gradients[prev_activation_key] = activation_gradients[prev_activation_key].T\n",
        "\n",
        "    # Compute gradients for the first layer (no hidden gradients needed)\n",
        "    weight_gradients['W1'] = np.dot(activation_gradients['a1'].T, batch_trainX)\n",
        "    bias_gradients['b1'] = activation_gradients['a1']\n",
        "\n",
        "    # Normalize biases across samples\n",
        "    for layer in range(1, len(bias_gradients) + 1):\n",
        "        bias_key = f'b{layer}'\n",
        "        bias_gradients[bias_key] = np.mean(bias_gradients[bias_key], axis=0).reshape(-1, 1)\n",
        "\n",
        "    return weight_gradients, bias_gradients\n",
        "\n",
        "def gradient_descent(trainX, trainy, number_hidden_layers = 1, hidden_layer_size = 4, eta = 0.1, initial_weights = 'random', activation_function = 'sigmoid', epochs = 1, output_function = 'softmax', mini_batch_size=4,loss_function = 'cross_entropy',weight_decay_const=0,wandb_flag=False):\n",
        "  layers = []\n",
        "  layers.append({INPUT_KEY : input_layer_size, OUTPUT_KEY : hidden_layer_size, FUN_KEY : activation_function})\n",
        "  for i in range(number_hidden_layers-1):\n",
        "    layers.append({INPUT_KEY : hidden_layer_size, OUTPUT_KEY : hidden_layer_size, FUN_KEY : activation_function})\n",
        "  layers.append({INPUT_KEY : hidden_layer_size, OUTPUT_KEY : output_layer_size, FUN_KEY : output_function})\n",
        "\n",
        "#initialize weights and biases\n",
        "  weights,biases = initialize_weights_and_biases(layers,number_hidden_layers,initial_weights)\n",
        "  x_val = len(trainX)\n",
        "  number_batches = x_val/mini_batch_size\n",
        "\n",
        "  mini_batch_trainX,mini_batch_trainy = np.array(np.array_split(trainX, number_batches)),np.array(np.array_split(trainy, number_batches))\n",
        "\n",
        "  train_loss_list,  val_loss_list,  train_acc_list,  val_acc_list = [],[],[],[]\n",
        "  h=None\n",
        "  for j in range(epochs):\n",
        "    tloss,vloss = 0,0\n",
        "    y_predicted = []\n",
        "    for k in range(len(mini_batch_trainX)):\n",
        "      a,h = forward_propagation(mini_batch_trainX[k],weights,biases,number_hidden_layers, activation_function, output_function)\n",
        "      y_predicted.append(h[-1])\n",
        "\n",
        "      if loss_function == 'cross_entropy':\n",
        "        tloss += cross_entropy(h[-1],mini_batch_trainy[k])\n",
        "      elif loss_function == 'mean_squared_error':\n",
        "        tloss += mean_squared_error( h[-1],mini_batch_trainy[k])\n",
        "      else:\n",
        "        print('wrong loss function')\n",
        "\n",
        "      del_W,del_b = backward_propagation(mini_batch_trainy[k],mini_batch_trainX[k],h[-1],a,h,weights,number_hidden_layers ,activation_function)\n",
        "\n",
        "      for i in range(len(weights)):\n",
        "        keyW = 'W'+str(i+1)\n",
        "        keyB = 'b'+str(i+1)\n",
        "        weights[i] -=  (del_W[keyW]*eta)\n",
        "        biases[i] -= (del_b[keyB]*eta)\n",
        "    reg_term_train = calculate_regularizing_term(trainy,weight_decay_const,number_hidden_layers,weights)\n",
        "    tr_loss = tloss/number_batches + reg_term_train\n",
        "    vloss = val_loss(valX,valiy,weights,biases,number_hidden_layers,activation_function,output_function,loss_function)\n",
        "    reg_term_val = calculate_regularizing_term(valiy,weight_decay_const,number_hidden_layers,weights)\n",
        "    vloss = vloss + reg_term_val\n",
        "    print(\"epoch : \",j+1,\" validation loss : \",vloss)\n",
        "\n",
        "    train_loss_list.append(tr_loss)\n",
        "    val_loss_list.append(vloss)\n",
        "    train_acc = train_accuracy(mini_batch_trainy,y_predicted,trainy)\n",
        "    train_acc_list.append(train_acc)\n",
        "    val_acc = test_accuracy(valX,valy,weights,biases,number_hidden_layers,activation_function,output_function)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    if wandb_flag == True:\n",
        "      wandb.log({\"loss\":tr_loss,\"val_loss\":vloss,\"accuracy\":train_acc,\"val_accuracy\":val_acc,\"epoch\":j})\n",
        "\n",
        "  plot_lists = [train_loss_list,val_loss_list,train_acc_list,val_acc_list]\n",
        "  return h[-1],weights,biases,plot_lists\n",
        "\n",
        "\n",
        "\n",
        "def momentum_based_gradient_descent(trainX, trainy, number_hidden_layers=1, hidden_layer_size=4, eta=0.1, initial_weights='random', activation_function='sigmoid', epochs=1, output_function='softmax', mini_batch_size=4, loss_function='cross_entropy', weight_decay_const=0, wandb_flag=False, m_beta=0.9):\n",
        "\n",
        "    # Define layer configurations\n",
        "    layers = []\n",
        "    input_layer = {'input_size': trainX.shape[1], 'output_size': hidden_layer_size, 'activation': activation_function}\n",
        "    layers.append(input_layer)\n",
        "\n",
        "    for _ in range(number_hidden_layers - 1):\n",
        "        hidden_layer = {'input_size': hidden_layer_size, 'output_size': hidden_layer_size, 'activation': activation_function}\n",
        "        layers.append(hidden_layer)\n",
        "\n",
        "    output_layer = {'input_size': hidden_layer_size, 'output_size': trainy.shape[1], 'activation': output_function}\n",
        "    layers.append(output_layer)\n",
        "\n",
        "    # Initialize weights and biases\n",
        "    weights, biases = initialize_weights_and_biases(layers,number_hidden_layers, initial_weights)\n",
        "\n",
        "    # Split data into mini-batches\n",
        "    num_batches = len(trainX) // mini_batch_size\n",
        "    mini_batches_X = np.array_split(trainX, num_batches)\n",
        "    mini_batches_y = np.array_split(trainy, num_batches)\n",
        "\n",
        "    # Initialize momentum terms\n",
        "    momentum_weights = [np.zeros_like(w) for w in weights]\n",
        "    momentum_biases = [np.zeros_like(b) for b in biases]\n",
        "\n",
        "    # Lists to store training and validation metrics\n",
        "    train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        predictions = []\n",
        "\n",
        "        for batch_X, batch_y in zip(mini_batches_X, mini_batches_y):\n",
        "            # Forward propagation\n",
        "            activations, hidden_states = forward_propagation(batch_X, weights, biases, number_hidden_layers, activation_function, output_function)\n",
        "            predictions.append(hidden_states[-1])\n",
        "\n",
        "            # Compute loss\n",
        "            if loss_function == 'cross_entropy':\n",
        "                batch_loss = cross_entropy(hidden_states[-1], batch_y)\n",
        "            elif loss_function == 'mean_squared_error':\n",
        "                batch_loss = mean_squared_error(hidden_states[-1], batch_y)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported loss function\")\n",
        "\n",
        "            epoch_loss += batch_loss\n",
        "\n",
        "            # Backward propagation\n",
        "            grad_weights, grad_biases = backward_propagation(batch_y, batch_X, hidden_states[-1], activations, hidden_states, weights, number_hidden_layers, activation_function)\n",
        "\n",
        "            # Update weights and biases with momentum\n",
        "            for i in range(len(weights)):\n",
        "                # Access gradients using keys (e.g., 'W1', 'b1', 'W2', 'b2', etc.)\n",
        "                keyW = 'W' + str(i + 1)\n",
        "                keyB = 'b' + str(i + 1)\n",
        "\n",
        "                # Momentum update for weights\n",
        "                momentum_weights[i] = momentum_weights[i] * m_beta + grad_weights[keyW] * eta\n",
        "                weights[i] -= momentum_weights[i]\n",
        "\n",
        "                # Momentum update for biases\n",
        "                momentum_biases[i] = momentum_biases[i] * m_beta + grad_biases[keyB] * eta\n",
        "                biases[i] -= momentum_biases[i]\n",
        "\n",
        "        # Calculate training accuracy and loss\n",
        "        train_acc = train_accuracy(mini_batches_y,predictions,trainy)\n",
        "        reg_term_train = calculate_regularizing_term(trainy,weight_decay_const,number_hidden_layers ,weights)\n",
        "        avg_train_loss = epoch_loss / num_batches + reg_term_train\n",
        "\n",
        "        # Calculate validation accuracy and loss\n",
        "        val_acc = test_accuracy(valX, valiy, weights, biases, number_hidden_layers, activation_function, output_function)\n",
        "        val_loss_value = val_loss(valX, valiy, weights, biases, number_hidden_layers, activation_function, output_function, loss_function)\n",
        "        reg_term_val = calculate_regularizing_term(valiy,weight_decay_const ,number_hidden_layers, weights)\n",
        "        val_loss_value += reg_term_val\n",
        "\n",
        "        # Store metrics\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(val_loss_value)\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        # Log metrics if wandb is enabled\n",
        "        if wandb_flag:\n",
        "            wandb.log({\n",
        "                \"loss\": avg_train_loss,\n",
        "                \"val_loss\": val_loss_value,\n",
        "                \"accuracy\": train_acc,\n",
        "                \"val_accuracy\": val_acc,\n",
        "                \"epoch\": epoch\n",
        "            })\n",
        "\n",
        "        print(f\"Epoch: {epoch + 1}, Validation Loss: {val_loss_value}\")\n",
        "\n",
        "    # Return final predictions, weights, biases, and metrics\n",
        "    return hidden_states[-1], weights, biases, [train_losses, val_losses, train_accuracies, val_accuracies]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def nestrov_accelerated_gradient_descent(trainX, trainy, number_hidden_layers=1, hidden_layer_size=4, eta=0.1,\n",
        "                                         initial_weights='random', activation_function='sigmoid', epochs=1,\n",
        "                                         output_function='softmax', mini_batch_size=4, loss_function='cross_entropy',\n",
        "                                         weight_decay_const=0, wandb_flag=False):\n",
        "    \"\"\"\n",
        "    Implements Nesterov Accelerated Gradient Descent (NAG) for neural network optimization.\n",
        "    \"\"\"\n",
        "    # Define network layers dynamically\n",
        "    layers = [{INPUT_KEY: input_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function}]\n",
        "    layers.extend([{INPUT_KEY: hidden_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function}\n",
        "                   for _ in range(number_hidden_layers - 1)])\n",
        "    layers.append({INPUT_KEY: hidden_layer_size, OUTPUT_KEY: output_layer_size, FUN_KEY: output_function})\n",
        "\n",
        "    # Initialize weights and biases\n",
        "    weights, biases = initialize_weights_and_biases(layers, number_hidden_layers, initial_weights)\n",
        "\n",
        "    # Determine batch processing details\n",
        "    num_samples = len(trainX)\n",
        "    num_batches = num_samples // mini_batch_size\n",
        "    mini_batches_X = np.array_split(trainX, num_batches)\n",
        "    mini_batches_y = np.array_split(trainy, num_batches)\n",
        "\n",
        "    # Initialize past gradients for momentum update\n",
        "    momentum_weights = [np.zeros_like(w) for w in weights]\n",
        "    momentum_biases = [np.zeros_like(b) for b in biases]\n",
        "\n",
        "    # Lists to track training progress\n",
        "    loss_train, loss_val, acc_train, acc_val = [], [], [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        y_predictions = []\n",
        "\n",
        "        for batch_X, batch_y in zip(mini_batches_X, mini_batches_y):\n",
        "            # Compute lookahead weights and biases\n",
        "            lookahead_W = [weights[i] - (m_beta * momentum_weights[i]) for i in range(len(weights))]\n",
        "            lookahead_B = [biases[i] - (m_beta * momentum_biases[i]) for i in range(len(biases))]\n",
        "\n",
        "            activations, h_states = forward_propagation(batch_X, lookahead_W, lookahead_B, number_hidden_layers, activation_function, output_function)\n",
        "            y_predictions.append(h_states[-1])\n",
        "\n",
        "            # Compute loss\n",
        "            total_loss += cross_entropy(h_states[-1], batch_y) if loss_function == 'cross_entropy' else mean_squared_error(h_states[-1], batch_y)\n",
        "\n",
        "            # Compute gradients\n",
        "            grad_W, grad_B = backward_propagation(batch_y, batch_X, h_states[-1], activations, h_states, lookahead_W, number_hidden_layers, activation_function)\n",
        "\n",
        "            # Apply Nesterov update\n",
        "            for i in range(len(weights)):\n",
        "                momentum_weights[i] = (m_beta * momentum_weights[i]) + (eta * grad_W[f'W{i+1}'])\n",
        "                momentum_biases[i] = (m_beta * momentum_biases[i]) + (eta * grad_B[f'b{i+1}'])\n",
        "\n",
        "                weights[i] -= momentum_weights[i]\n",
        "                biases[i] -= momentum_biases[i]\n",
        "\n",
        "        # Compute regularized loss\n",
        "        reg_term_train = calculate_regularizing_term(trainy, weight_decay_const, number_hidden_layers, weights)\n",
        "        avg_train_loss = (total_loss / num_batches) + reg_term_train\n",
        "\n",
        "        # Validation loss calculation\n",
        "        val_loss_value = val_loss(valX, valiy, weights, biases, number_hidden_layers, activation_function, output_function, loss_function)\n",
        "        val_loss_value += calculate_regularizing_term(valiy, weight_decay_const, number_hidden_layers, weights)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}: Validation Loss = {val_loss_value:.4f}\")\n",
        "\n",
        "        # Compute accuracy metrics\n",
        "        train_acc = train_accuracy(mini_batches_y, y_predictions, trainy)\n",
        "        val_acc = test_accuracy(valX, valy, weights, biases, number_hidden_layers, activation_function, output_function)\n",
        "\n",
        "        # Store metrics\n",
        "        loss_train.append(avg_train_loss)\n",
        "        loss_val.append(val_loss_value)\n",
        "        acc_train.append(train_acc)\n",
        "        acc_val.append(val_acc)\n",
        "\n",
        "        # Log metrics if W&B logging is enabled\n",
        "        if wandb_flag:\n",
        "            wandb.log({\"loss\": avg_train_loss, \"val_loss\": val_loss_value, \"accuracy\": train_acc, \"val_accuracy\": val_acc, \"epoch\": epoch + 1})\n",
        "\n",
        "    return h_states[-1], weights, biases, [loss_train, loss_val, acc_train, acc_val]\n",
        "\n",
        "\n",
        "def rmsprop(trainX, trainy, number_hidden_layers=1, hidden_layer_size=4, eta=0.1, initial_weights='random',\n",
        "            activation_function='sigmoid', epochs=1, output_function='softmax', mini_batch_size=4,\n",
        "            loss_function='cross_entropy', weight_decay_const=0, wandb_flag=False):\n",
        "\n",
        "    layers = [{INPUT_KEY: input_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function}]\n",
        "    layers.extend([{INPUT_KEY: hidden_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function}\n",
        "                   for _ in range(number_hidden_layers - 1)])\n",
        "    layers.append({INPUT_KEY: hidden_layer_size, OUTPUT_KEY: output_layer_size, FUN_KEY: output_function})\n",
        "\n",
        "    # Initialize weights and biases\n",
        "    weights, biases = initialize_weights_and_biases(layers, number_hidden_layers, initial_weights)\n",
        "\n",
        "    # Determine batch processing details\n",
        "    num_samples = len(trainX)\n",
        "    num_batches = num_samples // mini_batch_size\n",
        "    mini_batches_X = np.array_split(trainX, num_batches)\n",
        "    mini_batches_y = np.array_split(trainy, num_batches)\n",
        "\n",
        "    # RMSProp accumulators\n",
        "    v_weights = [np.zeros_like(w) for w in weights]\n",
        "    v_biases = [np.zeros_like(b) for b in biases]\n",
        "\n",
        "    train_loss_list, val_loss_list, train_acc_list, val_acc_list = [], [], [], []\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        y_preds = []\n",
        "\n",
        "        for batch_X, batch_y in zip(mini_batches_X, mini_batches_y):\n",
        "            activations, h_states = forward_propagation(batch_X, weights, biases, number_hidden_layers, activation_function, output_function)\n",
        "            y_preds.append(h_states[-1])\n",
        "\n",
        "            # Compute loss\n",
        "            epoch_loss += cross_entropy(h_states[-1], batch_y) if loss_function == 'cross_entropy' else mean_squared_error(h_states[-1], batch_y)\n",
        "\n",
        "            # Backpropagation\n",
        "            del_W, del_b = backward_propagation(batch_y, batch_X, h_states[-1], activations, h_states, weights, number_hidden_layers, activation_function)\n",
        "\n",
        "            # RMSProp weight update\n",
        "            for i in range(len(weights)):\n",
        "                v_weights[i] = rmsprop_beta * v_weights[i] + (1 - rmsprop_beta) * (del_W[f'W{i+1}'] ** 2)\n",
        "                v_biases[i] = rmsprop_beta * v_biases[i] + (1 - rmsprop_beta) * (del_b[f'b{i+1}'] ** 2)\n",
        "\n",
        "                weights[i] -= eta * del_W[f'W{i+1}'] / (np.sqrt(v_weights[i] + epsilon_))\n",
        "                biases[i] -= eta * del_b[f'b{i+1}'] / (np.sqrt(v_biases[i] + epsilon_))\n",
        "\n",
        "        # Compute loss & accuracy\n",
        "        train_loss = (epoch_loss / num_batches) + calculate_regularizing_term(trainy, weight_decay_const, number_hidden_layers, weights)\n",
        "        val_loss_value = val_loss(valX, valiy, weights, biases, number_hidden_layers, activation_function, output_function, loss_function)\n",
        "        val_loss_value += calculate_regularizing_term(valiy, weight_decay_const, number_hidden_layers, weights)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Validation Loss = {val_loss_value:.4f}\")\n",
        "\n",
        "        # Accuracy Calculation\n",
        "        train_acc_list.append(train_accuracy(mini_batches_y, y_preds, trainy))\n",
        "        val_acc_list.append(test_accuracy(valX, valy, weights, biases, number_hidden_layers, activation_function, output_function))\n",
        "        train_loss_list.append(train_loss)\n",
        "        val_loss_list.append(val_loss_value)\n",
        "\n",
        "        if wandb_flag:\n",
        "            wandb.log({\"loss\": train_loss, \"val_loss\": val_loss_value, \"accuracy\": train_acc_list[-1], \"val_accuracy\": val_acc_list[-1], \"epoch\": epoch+1})\n",
        "\n",
        "    return h_states[-1], weights, biases, [train_loss_list, val_loss_list, train_acc_list, val_acc_list]\n",
        "\n",
        "def adam(trainX, trainy, number_hidden_layers=1, hidden_layer_size=4, eta=0.1, initial_weights='random',\n",
        "         activation_function='sigmoid', epochs=1, output_function='softmax', mini_batch_size=4,\n",
        "         loss_function='cross_entropy', weight_decay_const=0, wandb_flag=False):\n",
        "\n",
        "    layers = [{INPUT_KEY: input_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function}]\n",
        "    layers.extend([{INPUT_KEY: hidden_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function}\n",
        "                   for _ in range(number_hidden_layers - 1)])\n",
        "    layers.append({INPUT_KEY: hidden_layer_size, OUTPUT_KEY: output_layer_size, FUN_KEY: output_function})\n",
        "\n",
        "    # Initialize weights and biases\n",
        "    weights, biases = initialize_weights_and_biases(layers, number_hidden_layers, initial_weights)\n",
        "\n",
        "    # Determine batch processing details\n",
        "    num_samples = len(trainX)\n",
        "    num_batches = num_samples // mini_batch_size\n",
        "    mini_batches_X = np.array_split(trainX, num_batches)\n",
        "    mini_batches_y = np.array_split(trainy, num_batches)\n",
        "\n",
        "    # Adam accumulators\n",
        "    v_weights, v_biases = [np.zeros_like(w) for w in weights], [np.zeros_like(b) for b in biases]\n",
        "    m_weights, m_biases = [np.zeros_like(w) for w in weights], [np.zeros_like(b) for b in biases]\n",
        "\n",
        "    train_loss_list, val_loss_list, train_acc_list, val_acc_list = [], [], [], []\n",
        "    time_step = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        y_preds = []\n",
        "\n",
        "        for batch_X, batch_y in zip(mini_batches_X,mini_batches_y):\n",
        "            time_step += 1\n",
        "            activations, h_states = forward_propagation(batch_X, weights, biases, number_hidden_layers, activation_function, output_function)\n",
        "            y_preds.append(h_states[-1])\n",
        "\n",
        "            # Compute loss\n",
        "            epoch_loss += cross_entropy(h_states[-1], batch_y) if loss_function == 'cross_entropy' else mean_squared_error(h_states[-1], batch_y)\n",
        "\n",
        "            # Backpropagation\n",
        "            del_W, del_b = backward_propagation(batch_y, batch_X, h_states[-1], activations, h_states, weights, number_hidden_layers, activation_function)\n",
        "\n",
        "            # Adam weight update\n",
        "            for i in range(len(weights)):\n",
        "                m_weights[i] = beta_1 * m_weights[i] + (1 - beta_1) * del_W[f'W{i+1}']\n",
        "                m_biases[i] = beta_1 * m_biases[i] + (1 - beta_1) * del_b[f'b{i+1}']\n",
        "\n",
        "                v_weights[i] = beta_2 * v_weights[i] + (1 - beta_2) * (del_W[f'W{i+1}'] ** 2)\n",
        "                v_biases[i] = beta_2 * v_biases[i] + (1 - beta_2) * (del_b[f'b{i+1}'] ** 2)\n",
        "\n",
        "                # Bias correction\n",
        "                m_hat_w, v_hat_w = m_weights[i] / (1 - beta_1 ** time_step), v_weights[i] / (1 - beta_2 ** time_step)\n",
        "                m_hat_b, v_hat_b = m_biases[i] / (1 - beta_1 ** time_step), v_biases[i] / (1 - beta_2 ** time_step)\n",
        "\n",
        "                weights[i] -= eta * m_hat_w / (np.sqrt(v_hat_w) + epsilon_)\n",
        "                biases[i] -= eta * m_hat_b / (np.sqrt(v_hat_b) + epsilon_)\n",
        "\n",
        "        # Compute loss & accuracy\n",
        "        train_loss = (epoch_loss / num_batches) + calculate_regularizing_term(trainy, weight_decay_const, number_hidden_layers, weights)\n",
        "        val_loss_value = val_loss(valX, valiy, weights, biases, number_hidden_layers, activation_function, output_function, loss_function)\n",
        "        val_loss_value += calculate_regularizing_term(valiy, weight_decay_const, number_hidden_layers, weights)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Validation Loss = {val_loss_value:.4f}\")\n",
        "\n",
        "        # Store accuracy\n",
        "        train_acc_list.append(train_accuracy(mini_batches_y, y_preds, trainy))\n",
        "        val_acc_list.append(test_accuracy(valX, valy, weights, biases, number_hidden_layers, activation_function, output_function))\n",
        "\n",
        "        if wandb_flag:\n",
        "            wandb.log({\"loss\": train_loss, \"val_loss\": val_loss_value, \"accuracy\": train_acc_list[-1], \"val_accuracy\": val_acc_list[-1], \"epoch\": epoch+1})\n",
        "\n",
        "    return h_states[-1], weights, biases, [train_loss_list, val_loss_list, train_acc_list, val_acc_list]\n",
        "\n",
        "def nadam(trainX, trainy, number_hidden_layers=1, hidden_layer_size=4, eta=0.1, initial_weights='random',\n",
        "          activation_function='sigmoid', epochs=1, output_function='softmax', mini_batch_size=4,\n",
        "          loss_function='cross_entropy', weight_decay_const=0, wandb_flag=False):\n",
        "\n",
        "    # Define network layers dynamically\n",
        "    layers = [{INPUT_KEY: input_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function}]\n",
        "    layers.extend([{INPUT_KEY: hidden_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function}\n",
        "                   for _ in range(number_hidden_layers - 1)])\n",
        "    layers.append({INPUT_KEY: hidden_layer_size, OUTPUT_KEY: output_layer_size, FUN_KEY: output_function})\n",
        "\n",
        "    # Initialize weights and biases\n",
        "    weights, biases = initialize_weights_and_biases(layers, number_hidden_layers, initial_weights)\n",
        "\n",
        "    # Determine batch processing details\n",
        "    num_samples = len(trainX)\n",
        "    num_batches = num_samples // mini_batch_size\n",
        "    mini_batches_X = np.array_split(trainX, num_batches)\n",
        "    mini_batches_y = np.array_split(trainy, num_batches)\n",
        "\n",
        "    # Initialize moving averages for Nadam\n",
        "    v_weights, v_biases = [np.zeros_like(w) for w in weights], [np.zeros_like(b) for b in biases]\n",
        "    m_weights, m_biases = [np.zeros_like(w) for w in weights], [np.zeros_like(b) for b in biases]\n",
        "\n",
        "    train_loss_list, val_loss_list, train_acc_list, val_acc_list = [], [], [], []\n",
        "    beta_1, beta_2, epsilon = 0.9, 0.999, 1e-3\n",
        "    t = 0  # Step counter\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_train_loss = 0\n",
        "        y_preds = []\n",
        "\n",
        "        for X_batch, y_batch in zip(mini_batches_X,mini_batches_y):\n",
        "            t += 1  # Increment time step\n",
        "\n",
        "            # Compute bias-corrected moving averages\n",
        "            v_hat_weights = [v / (1 - beta_2 ** t) for v in v_weights]\n",
        "            v_hat_biases = [v / (1 - beta_2 ** t) for v in v_biases]\n",
        "\n",
        "            m_hat_weights = [m / (1 - beta_1 ** t) for m in m_weights]\n",
        "            m_hat_biases = [m / (1 - beta_1 ** t) for m in m_biases]\n",
        "\n",
        "            # Compute \"lookahead\" weights & biases\n",
        "            lookahead_weights = [weights[i] - (m_hat_weights[i] / np.sqrt(v_hat_weights[i] + epsilon)) * eta\n",
        "                                 for i in range(len(weights))]\n",
        "            lookahead_biases = [biases[i] - (m_hat_biases[i] / np.sqrt(v_hat_biases[i] + epsilon)) * eta\n",
        "                                for i in range(len(biases))]\n",
        "\n",
        "            # Forward propagation\n",
        "            activations, outputs = forward_propagation(X_batch, lookahead_weights, lookahead_biases, number_hidden_layers, activation_function, output_function)\n",
        "            y_preds.append(outputs[-1])\n",
        "\n",
        "            # Compute loss\n",
        "            loss_func = cross_entropy if loss_function == 'cross_entropy' else mean_squared_error\n",
        "            total_train_loss += loss_func(outputs[-1], y_batch)\n",
        "\n",
        "            # Backpropagation\n",
        "            grad_W, grad_B = backward_propagation(y_batch, X_batch, outputs[-1], activations, outputs, lookahead_weights, number_hidden_layers, activation_function)\n",
        "\n",
        "            # Update moving averages & apply Nadam update rule\n",
        "            for i in range(len(weights)):\n",
        "                v_weights[i] = beta_2 * v_weights[i] + (1 - beta_2) * (grad_W[f'W{i+1}'] ** 2)\n",
        "                v_biases[i] = beta_2 * v_biases[i] + (1 - beta_2) * (grad_B[f'b{i+1}'] ** 2)\n",
        "\n",
        "                m_weights[i] = beta_1 * m_weights[i] + (1 - beta_1) * grad_W[f'W{i+1}']\n",
        "                m_biases[i] = beta_1 * m_biases[i] + (1 - beta_1) * grad_B[f'b{i+1}']\n",
        "\n",
        "                v_hat_w, v_hat_b = v_weights[i] / (1 - beta_2 ** t), v_biases[i] / (1 - beta_2 ** t)\n",
        "                m_hat_w, m_hat_b = m_weights[i] / (1 - beta_1 ** t), m_biases[i] / (1 - beta_1 ** t)\n",
        "\n",
        "                weights[i] -= (m_hat_w * eta) / np.sqrt(v_hat_w + epsilon)\n",
        "                biases[i] -= (m_hat_b * eta) / np.sqrt(v_hat_b + epsilon)\n",
        "\n",
        "        # Compute losses with regularization\n",
        "        reg_train = calculate_regularizing_term(trainy, weight_decay_const, number_hidden_layers, weights)\n",
        "        avg_train_loss = total_train_loss / num_batches + reg_train\n",
        "        val_loss_value = val_loss(valX, valiy, weights, biases, number_hidden_layers, activation_function, output_function, loss_function)\n",
        "        val_loss_value += calculate_regularizing_term(valiy, weight_decay_const, number_hidden_layers, weights)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}: Validation Loss = {val_loss_value:.4f}\")\n",
        "\n",
        "        # Compute accuracy\n",
        "        train_acc_list.append(train_accuracy(mini_batches_y, y_preds, trainy))\n",
        "        val_acc_list.append(test_accuracy(valX, valy, weights, biases, number_hidden_layers, activation_function, output_function))\n",
        "\n",
        "        # Store metrics\n",
        "        train_loss_list.append(avg_train_loss)\n",
        "        val_loss_list.append(val_loss_value)\n",
        "\n",
        "        if wandb_flag:\n",
        "            wandb.log({\n",
        "                \"loss\": avg_train_loss,\n",
        "                \"val_loss\": val_loss_value,\n",
        "                \"accuracy\": train_acc_list[-1],\n",
        "                \"val_accuracy\": val_acc_list[-1],\n",
        "                \"epoch\": epoch + 1\n",
        "            })\n",
        "\n",
        "    return outputs[-1], weights, biases, [train_loss_list, val_loss_list, train_acc_list, val_acc_list]\n",
        "\n",
        "\n",
        "\n",
        "def train(trainX, trainy, textX, testy, number_hidden_layers, hidden_layer_size, eta, init_type, activation_function,\n",
        "          epochs, mini_batch_size, loss_function, optimizer, output_function, weight_decay_const, wandb_flag=False):\n",
        "\n",
        "    wdc = weight_decay_const\n",
        "\n",
        "    optimizer_functions = {\n",
        "        'sgd': gradient_descent,\n",
        "        'momentum': momentum_based_gradient_descent,\n",
        "        'nag': nestrov_accelerated_gradient_descent,\n",
        "        'rmsprop': rmsprop,\n",
        "        'adam': adam,\n",
        "        'nadam': nadam\n",
        "    }\n",
        "\n",
        "    if optimizer not in optimizer_functions:\n",
        "        raise ValueError(f\"Invalid optimizer: {optimizer}. Choose from {list(optimizer_functions.keys())}.\")\n",
        "\n",
        "    hL, weights, biases, plot_list = optimizer_functions[optimizer](\n",
        "        trainX, trainy, number_hidden_layers, hidden_layer_size, eta, init_type, activation_function, epochs,\n",
        "        output_function, mini_batch_size, loss_function, wdc, wandb_flag\n",
        "    )\n",
        "\n",
        "    return [weights, biases, number_hidden_layers, activation_function, output_function]\n",
        "\n",
        "if type(args.num_layers)==str:\n",
        "  args.num_layers = int(args.num_layers)\n",
        "if type(args.hidden_size)==str:\n",
        "  args.hidden_size = int(args.hidden_size)\n",
        "if type(args.learning_rate)==str:\n",
        "  args.learning_rate = float(args.learning_rate)\n",
        "if type(args.epochs)==str:\n",
        "  args.epochs = int(args.epochs)\n",
        "if type(args.batch_size)==str:\n",
        "  args.batch_size = int(args.batch_size)\n",
        "if type(args.weight_decay)==str:\n",
        "  args.weight_decay = int(args.weight_decay)\n",
        "\n",
        "run = wandb.init(project=args.wandb_project,entity=args.wandb_entity)\n",
        "params = train(trainX=trainX,\n",
        "    trainy=trainy,\n",
        "    textX=testX,\n",
        "    testy=needed_y_test,\n",
        "    number_hidden_layers=args.num_layers,\n",
        "    hidden_layer_size=args.hidden_size,\n",
        "    eta=args.learning_rate,\n",
        "    init_type=args.weight_init,\n",
        "    activation_function=args.activation,\n",
        "    epochs=args.epochs,\n",
        "    mini_batch_size=args.batch_size,\n",
        "    loss_function=args.loss,\n",
        "    optimizer=args.optimizer,\n",
        "    output_function='softmax',\n",
        "    weight_decay_const=args.weight_decay,\n",
        "    wandb_flag=True)\n",
        "\n",
        "test_ac = test_accuracy(testX,needed_y_test,params[0],params[1],params[2],params[3],params[4])\n",
        "\n",
        "print(\"Test accuracy on the model = \", test_ac*100,'%')\n",
        "wandb.save()\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "R2lxpERVES-T",
        "outputId": "127bf23e-03b0-417a-b953-7a3b5c7f94f0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [-wp WANDB_PROJECT] [-we WANDB_ENTITY]\n",
            "                                [-d {mnist,fashion_mnist}] [-e {5,10}] [-b {16,32,64}]\n",
            "                                [-l {cross_entropy,mean_squared_error}]\n",
            "                                [-o {sgd,momentum,nag,rmsprop,adam,nadam}] [-lr {1e-3,1e-4}]\n",
            "                                [-m {0.5}] [-beta {0.5}] [-beta1 {0.5}] [-beta2 {0.5}]\n",
            "                                [-eps {0.000001}] [-w_d {0,0.0005,0.5}] [-w_i {random,Xavier}]\n",
            "                                [-nhl {3,4,5}] [-sz {32,64,128}] [-a {identity,sigmoid,tanh,ReLU}]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-124d94a7-0e0f-4c94-9d09-3ae98f86a4b7.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ya2Bx8BNEThv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}