{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pavitra-khare/DA6401_Deep_learning/blob/main/DA6401_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "381edf3f-a1eb-40d7-bb12-399a73d447bf",
      "metadata": {
        "id": "381edf3f-a1eb-40d7-bb12-399a73d447bf",
        "outputId": "1cc4c670-2619-4bff-b49f-2f3f405eb806"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting keras\n",
            "  Downloading keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting absl-py (from keras)\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras) (13.3.5)\n",
            "Collecting namex (from keras)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
            "Requirement already satisfied: h5py in /opt/anaconda3/lib/python3.12/site-packages (from keras) (3.11.0)\n",
            "Collecting optree (from keras)\n",
            "  Downloading optree-0.14.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 kB\u001b[0m \u001b[31m134.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting ml-dtypes (from keras)\n",
            "  Downloading ml_dtypes-0.5.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (21 kB)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from keras) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from optree->keras) (4.11.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras) (0.1.0)\n",
            "Downloading keras-3.8.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m258.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m146.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.5.1-cp312-cp312-macosx_10_9_universal2.whl (670 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.4/670.4 kB\u001b[0m \u001b[31m200.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Downloading optree-0.14.0-cp312-cp312-macosx_11_0_arm64.whl (335 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.8/335.8 kB\u001b[0m \u001b[31m178.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: namex, optree, ml-dtypes, absl-py, keras\n",
            "Successfully installed absl-py-2.1.0 keras-3.8.0 ml-dtypes-0.5.1 namex-0.0.8 optree-0.14.0\n"
          ]
        }
      ],
      "source": [
        "!pip install keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f307325a-7e48-4442-800f-73e35f075681",
      "metadata": {
        "id": "f307325a-7e48-4442-800f-73e35f075681",
        "outputId": "ec4dee08-8a20-4bd5-ef6c-bcca4f308169"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.18.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
            "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
            "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.32.2)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (69.5.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow)\n",
            "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.14.1)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
            "  Downloading grpcio-1.70.0-cp312-cp312-macosx_10_14_universal2.whl.metadata (3.9 kB)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
            "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.4.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (20 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.3.5)\n",
            "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.6.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.4.1)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Downloading tensorflow-2.18.0-cp312-cp312-macosx_12_0_arm64.whl (239.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 MB\u001b[0m \u001b[31m167.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:39\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m225.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.70.0-cp312-cp312-macosx_10_14_universal2.whl (11.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m159.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m202.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.4.1-cp312-cp312-macosx_10_9_universal2.whl (405 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.1/405.1 kB\u001b[0m \u001b[31m121.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m186.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m164.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
            "Installing collected packages: libclang, flatbuffers, termcolor, tensorboard-data-server, opt-einsum, ml-dtypes, grpcio, google-pasta, gast, astunparse, tensorboard, tensorflow\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml_dtypes 0.5.1\n",
            "    Uninstalling ml_dtypes-0.5.1:\n",
            "      Successfully uninstalled ml_dtypes-0.5.1\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.70.0 libclang-18.1.1 ml-dtypes-0.4.1 opt-einsum-3.4.0 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 termcolor-2.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6YrhAjbxPeM",
        "outputId": "537b6073-0f13-4d22-8673-68e7b27bda57"
      },
      "id": "x6YrhAjbxPeM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.7)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.22.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlQ_3RxXxVV0",
        "outputId": "83d85d0e-f803-4d82-c99a-799af9fad15f"
      },
      "id": "ZlQ_3RxXxVV0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m3628-pavitrakhare\u001b[0m (\u001b[33m3628-pavitrakhare-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7QCrzQsgxedB"
      },
      "id": "7QCrzQsgxedB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "# Initialize Weights & Biases (W&B) for experiment tracking\n",
        "wandb.init(project=\"DA6401-assignment_1\", name=\"Question-1\")\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Define class names for Fashion MNIST\n",
        "class_names = [\n",
        "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
        "]\n",
        "\n",
        "# Select one sample image for each class\n",
        "selected_images = []\n",
        "selected_labels = set()\n",
        "\n",
        "for i, label in enumerate(train_labels):\n",
        "    if label not in selected_labels:\n",
        "        selected_images.append((train_images[i], class_names[label]))\n",
        "        selected_labels.add(label)\n",
        "    if len(selected_images) == 10:\n",
        "        break\n",
        "\n",
        "# Plot sample images\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "fig.suptitle(\"Sample Images from Fashion MNIST\", fontsize=16)\n",
        "\n",
        "for ax, (image, label) in zip(axes.flatten(), selected_images):\n",
        "    ax.imshow(image, cmap='gray')\n",
        "    ax.set_title(label)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Log the figure to W&B\n",
        "wandb.log({\"Sample MNIST Images\": fig})\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "ZBch0x4-TT34",
        "outputId": "7717f26f-8278-41c0-8171-32b8efc95523"
      },
      "id": "ZBch0x4-TT34",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Abort",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAbort\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-763a6adbab37>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Initialize Weights & Biases (W&B) for experiment tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"DA6401-assignment_1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Question-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load the Fashion MNIST dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[1;32m   1483\u001b[0m         \u001b[0;31m# Need to build delay into this sentry capture because our exit hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m         \u001b[0;31m# mess with sentry's ability to send out errors before the program ends.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m         \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1486\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# should never get here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/analytics/sentry.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self, exc)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# this will messily add this \"reraise\" function to the stack trace,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# but hopefully it's not too bad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_safe_noop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mwi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_WandbInit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_telemetry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1434\u001b[0m         \u001b[0mrun_settings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_run_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36mmaybe_login\u001b[0;34m(self, init_settings)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         wandb_login._login(\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0manonymous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manonymous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_login\u001b[0;34m(anonymous, key, relogin, host, force, timeout, verify, _silent, _disable_warning)\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mkey_is_pre_configured\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mApiKeyStatus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;34m\"\"\"Updates the global API key by prompting the user.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mApiKeyStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTTY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             directive = (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_prompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                 key = apikey.prompt_api_key(\n\u001b[0m\u001b[1;32m    207\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                     \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/apikey.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(settings, api, input_callback, browser_callback, no_offline, no_create, local)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;34mf\"You can find your API key in your browser here: {app_url}/authorize\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             )\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_ask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mLOGIN_CHOICE_NOTTY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m# TODO: Needs refactor as this needs to be handled by caller\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/click/termui.py\u001b[0m in \u001b[0;36mprompt\u001b[0;34m(text, default, hide_input, confirmation_prompt, type, value_proc, prompt_suffix, show_default, err, show_choices)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/click/termui.py\u001b[0m in \u001b[0;36mprompt_func\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhide_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mecho\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAbort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalue_proc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAbort\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c21950ab-3ad0-400f-b317-46581baeb552",
      "metadata": {
        "id": "c21950ab-3ad0-400f-b317-46581baeb552"
      },
      "source": [
        "# QUESTION 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation Functions\n",
        "def sigmoid(x):\n",
        "    \"\"\"Computes the sigmoid activation function.\"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "    \"\"\"Computes the hyperbolic tangent activation function.\"\"\"\n",
        "    return np.tanh(x)\n",
        "\n",
        "def relu(x):\n",
        "    \"\"\"Computes the ReLU activation function.\"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Computes the softmax activation function for a given input.\n",
        "\n",
        "    Parameters:\n",
        "    - x: Input array.\n",
        "\n",
        "    Returns:\n",
        "    - Softmax-transformed array.\n",
        "    \"\"\"\n",
        "    output = []\n",
        "    for row in x:\n",
        "        row -= np.max(row)  # Stability trick to prevent large exponentials\n",
        "        exp_values = np.exp(row)\n",
        "        output.append(exp_values / np.sum(exp_values))\n",
        "    return np.array(output)\n",
        "\n",
        "# Loss Functions\n",
        "def cross_entropy(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Computes cross-entropy loss.\n",
        "\n",
        "    Parameters:\n",
        "    - y_pred: Predicted probabilities.\n",
        "    - y_true: Actual labels (one-hot encoded).\n",
        "\n",
        "    Returns:\n",
        "    - Cross-entropy loss value.\n",
        "    \"\"\"\n",
        "    epsilon = 1e-9  # To avoid log(0)\n",
        "    return -np.sum(y_true * np.log(y_pred + epsilon)) / len(y_pred)\n",
        "\n",
        "def mean_squared_error(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Computes the mean squared error (MSE) loss.\n",
        "\n",
        "    Parameters:\n",
        "    - y_pred: Predicted values.\n",
        "    - y_true: Actual values.\n",
        "\n",
        "    Returns:\n",
        "    - MSE loss value.\n",
        "    \"\"\"\n",
        "    return np.sum((y_true - y_pred) ** 2) / (len(y_true) + len(y_pred))\n",
        "\n",
        "# Activation Function Selector\n",
        "def apply_activation(x, activation=\"sigmoid\"):\n",
        "    \"\"\"\n",
        "    Applies a specified activation function to the input.\n",
        "\n",
        "    Parameters:\n",
        "    - x: Input array.\n",
        "    - activation: Type of activation function (default: sigmoid).\n",
        "\n",
        "    Returns:\n",
        "    - Transformed array after applying the activation function.\n",
        "    \"\"\"\n",
        "    activations = {\n",
        "        \"ReLU\": relu,\n",
        "        \"sigmoid\": sigmoid,\n",
        "        \"tanh\": tanh,\n",
        "        \"softmax\": softmax\n",
        "    }\n",
        "    return activations.get(activation, lambda x: \"Invalid Activation\")(x)\n",
        "\n",
        "# Activation Function Derivatives\n",
        "def activation_derivative(x, activation=\"sigmoid\"):\n",
        "    \"\"\"\n",
        "    Computes the derivative of the specified activation function.\n",
        "\n",
        "    Parameters:\n",
        "    - x: Input array.\n",
        "    - activation: Type of activation function (default: sigmoid).\n",
        "\n",
        "    Returns:\n",
        "    - Array of derivatives.\n",
        "    \"\"\"\n",
        "    derivatives = {\n",
        "        \"ReLU\": lambda x: (x > 0).astype(float),\n",
        "        \"tanh\": lambda x: 1 - tanh(x) ** 2,\n",
        "        \"sigmoid\": lambda x: sigmoid(x) * (1 - sigmoid(x))\n",
        "    }\n",
        "    return derivatives.get(activation, lambda x: \"Invalid Activation\")(x)\n"
      ],
      "metadata": {
        "id": "EVqI0XoFke8w"
      },
      "id": "EVqI0XoFke8w",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist, mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Configuration Parameters\n",
        "DATASET_NAME = 'fashion_mnist'  # Change to 'mnist' if needed\n",
        "BETA_1 = 0.9\n",
        "BETA_2 = 0.999\n",
        "MOMENTUM_BETA = 0.9\n",
        "RMSPROP_BETA = 0.9\n",
        "EPSILON = 1e-3\n",
        "WEIGHT_DECAY = 0\n",
        "\n",
        "# Constants\n",
        "INPUT_KEY = 'input_size'\n",
        "OUTPUT_KEY = 'output_size'\n",
        "FUN_KEY = \"function\"\n",
        "\n",
        "def one_hot_encode(labels, num_classes=None):\n",
        "    \"\"\"\n",
        "    Converts label vector into a one-hot encoded matrix.\n",
        "\n",
        "    Parameters:\n",
        "    - labels (array): Array of class labels.\n",
        "    - num_classes (int): Number of unique classes. If None, inferred from labels.\n",
        "\n",
        "    Returns:\n",
        "    - np.ndarray: One-hot encoded matrix.\n",
        "    \"\"\"\n",
        "    num_classes = num_classes or (max(labels) + 1)\n",
        "    one_hot_matrix = np.zeros((len(labels), num_classes))\n",
        "    one_hot_matrix[np.arange(len(labels)), labels] = 1\n",
        "    return one_hot_matrix\n",
        "\n",
        "# Load the selected dataset\n",
        "if DATASET_NAME == 'fashion_mnist':\n",
        "    (train_X, train_Y), (test_X, test_Y) = fashion_mnist.load_data()\n",
        "elif DATASET_NAME == 'mnist':\n",
        "    (train_X, train_Y), (test_X, test_Y) = mnist.load_data()\n",
        "else:\n",
        "    raise ValueError(\"Invalid dataset name. Choose 'fashion_mnist' or 'mnist'.\")\n",
        "\n",
        "# Normalize pixel values to range [0,1]\n",
        "train_X, test_X = train_X / 255.0, test_X / 255.0\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "train_X, val_X, train_Y, val_Y = train_test_split(train_X, train_Y, test_size=0.1, random_state=40)\n",
        "\n",
        "# Reshape images into 1D feature vectors\n",
        "train_X = train_X.reshape(train_X.shape[0], -1)\n",
        "test_X = test_X.reshape(test_X.shape[0], -1)\n",
        "val_X = val_X.reshape(val_X.shape[0], -1)\n",
        "\n",
        "# Ensure data is a multiple of batch size (128)\n",
        "BATCH_SIZE = 128\n",
        "train_X = train_X[:(len(train_X) // BATCH_SIZE) * BATCH_SIZE, :]\n",
        "test_X = test_X[:(len(test_X) // BATCH_SIZE) * BATCH_SIZE, :]\n",
        "val_X = val_X[:(len(val_X) // BATCH_SIZE) * BATCH_SIZE, :]\n",
        "\n",
        "train_Y = train_Y[:(len(train_Y) // BATCH_SIZE) * BATCH_SIZE]\n",
        "test_Y = test_Y[:(len(test_Y) // BATCH_SIZE) * BATCH_SIZE]\n",
        "val_Y = val_Y[:(len(val_Y) // BATCH_SIZE) * BATCH_SIZE]\n",
        "\n",
        "# Convert labels to one-hot encoded format\n",
        "train_Y = one_hot_encode(train_Y)\n",
        "test_Y = one_hot_encode(test_Y)\n",
        "val_Y = one_hot_encode(val_Y)\n",
        "\n",
        "# Define input and output layer sizes\n",
        "input_layer_size = train_X.shape[1]\n",
        "output_layer_size = train_Y.shape[1]\n",
        "\n",
        "# Output dataset information\n",
        "print(f\"Training Data: {train_X.shape}, Labels: {train_Y.shape}\")\n",
        "print(f\"Validation Data: {val_X.shape}, Labels: {val_Y.shape}\")\n",
        "print(f\"Test Data: {test_X.shape}, Labels: {test_Y.shape}\")\n"
      ],
      "metadata": {
        "id": "9wMn710l8Gtn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d267ee45-1c11-4552-e7bb-506e6385515f"
      },
      "id": "9wMn710l8Gtn",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Training Data: (53888, 784), Labels: (53888, 10)\n",
            "Validation Data: (5888, 784), Labels: (5888, 10)\n",
            "Test Data: (9984, 784), Labels: (9984, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 3\n"
      ],
      "metadata": {
        "id": "J7zJCBJG9oTd"
      },
      "id": "J7zJCBJG9oTd"
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_model_params(input_size, hidden_units, output_size):\n",
        "    np.random.seed(42)\n",
        "    layer_structure = [input_size] + hidden_units + [output_size]\n",
        "    model_params = {}\n",
        "\n",
        "    for idx in range(1, len(layer_structure)):\n",
        "        model_params[f'W{idx}'] = np.random.randn(layer_structure[idx-1], layer_structure[idx]) * 0.01\n",
        "        model_params[f'b{idx}'] = np.zeros((1, layer_structure[idx]))\n",
        "\n",
        "    return model_params\n"
      ],
      "metadata": {
        "id": "2rGv7e-lVg5I"
      },
      "id": "2rGv7e-lVg5I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_error(actual, predicted):\n",
        "    samples = actual.shape[0]\n",
        "    return -np.sum(actual * np.log(predicted + 1e-8)) / samples"
      ],
      "metadata": {
        "id": "tKkR5DovVhjT"
      },
      "id": "tKkR5DovVhjT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def activation_sigmoid_derivative(x):\n",
        "    return x * (1 - x)"
      ],
      "metadata": {
        "id": "4zZW9jXLwEij"
      },
      "id": "4zZW9jXLwEij",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backpropagate(X, y_true, parameters, hidden_layers, optimizer, learning_rate, beta=0.9, beta2=0.999, epsilon=1e-8):\n",
        "    activations = forward_pass(X, parameters, hidden_layers)\n",
        "    m = X.shape[0]\n",
        "    grads = {}\n",
        "    v = {key: np.zeros_like(value) for key, value in parameters.items()}\n",
        "    s = {key: np.zeros_like(value) for key, value in parameters.items()}\n",
        "    L = len(hidden_layers) + 1\n",
        "\n",
        "\n",
        "    dZ = activations[f'A{L}'] - y_true\n",
        "\n",
        "    for i in reversed(range(1, L + 1)):\n",
        "        grads[f'dW{i}'] = np.dot(activations[f'A{i-1}'].T, dZ) / m\n",
        "        grads[f'db{i}'] = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "        if i > 1:\n",
        "            dZ = np.dot(dZ, parameters[f'W{i}'].T) * activations[f'A{i-1}'] * (1 - activations[f'A{i-1}'])\n",
        "\n",
        "\n",
        "    for key in parameters:\n",
        "        grad_key = f'd{key}'\n",
        "        if optimizer == \"sgd\":\n",
        "            parameters[key] -= learning_rate * grads.get(grad_key, 0)\n",
        "        elif optimizer == \"momentum\":\n",
        "            v[key] = beta * v[key] + (1 - beta) * grads.get(grad_key, 0)\n",
        "            parameters[key] -= learning_rate * v[key]\n",
        "        elif optimizer == \"nesterov\":\n",
        "            v[key] = beta * v[key] + (1 - beta) * grads.get(grad_key, 0)\n",
        "            parameters[key] -= learning_rate * beta * v[key]\n",
        "        elif optimizer == \"rmsprop\":\n",
        "            s[key] = beta * s[key] + (1 - beta) * np.square(grads.get(grad_key, 0))\n",
        "            parameters[key] -= learning_rate * grads.get(grad_key, 0) / (np.sqrt(s[key]) + epsilon)\n",
        "        elif optimizer == \"adam\":\n",
        "            v[key] = beta * v[key] + (1 - beta) * grads.get(grad_key, 0)\n",
        "            s[key] = beta2 * s[key] + (1 - beta2) * np.square(grads.get(grad_key, 0))\n",
        "            parameters[key] -= learning_rate * v[key] / (np.sqrt(s[key]) + epsilon)\n",
        "        elif optimizer == \"nadam\":\n",
        "            v[key] = beta * v[key] + (1 - beta) * grads.get(grad_key, 0)\n",
        "            s[key] = beta2 * s[key] + (1 - beta2) * np.square(grads.get(grad_key, 0))\n",
        "            parameters[key] -= learning_rate * (beta * v[key] + (1 - beta) * grads.get(grad_key, 0)) / (np.sqrt(s[key]) + epsilon)\n",
        "\n",
        "    return parameters\n"
      ],
      "metadata": {
        "id": "KtAYLKDq3tRf"
      },
      "id": "KtAYLKDq3tRf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = images[:5].reshape(5, -1)  # Flatten images\n",
        "y_train = np.eye(10)[labels_index[:5]]  # One-hot encode labels\n",
        "\n",
        "parameters = setup_model_params(X_train.shape[1], [128, 64], 10)\n",
        "\n",
        "for epoch in range(10):\n",
        "    parameters = backpropagate(X_train, y_train, parameters, [128, 64], optimizer=\"adam\", learning_rate=0.01)\n",
        "    predictions = forward_pass(X_train, parameters, [128, 64])[f\"A{len([128, 64]) + 1}\"]  # Ensure correct indexing\n",
        "    loss = calculate_error(y_train, predictions)\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyCzuSmy9tLS",
        "outputId": "ca0bbe7a-5a36-4db5-ac2f-c64b08ac587b"
      },
      "id": "pyCzuSmy9tLS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.3602\n",
            "Epoch 2, Loss: 1.2778\n",
            "Epoch 3, Loss: 1.1213\n",
            "Epoch 4, Loss: 1.2231\n",
            "Epoch 5, Loss: 1.0618\n",
            "Epoch 6, Loss: 1.0684\n",
            "Epoch 7, Loss: 0.9595\n",
            "Epoch 8, Loss: 0.7840\n",
            "Epoch 9, Loss: 0.8232\n",
            "Epoch 10, Loss: 0.6037\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}