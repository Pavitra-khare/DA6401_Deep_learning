{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pavitra-khare/DA6401_Deep_learning/blob/main/DA6401_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "381edf3f-a1eb-40d7-bb12-399a73d447bf",
      "metadata": {
        "id": "381edf3f-a1eb-40d7-bb12-399a73d447bf",
        "outputId": "1cc4c670-2619-4bff-b49f-2f3f405eb806"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting keras\n",
            "  Downloading keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting absl-py (from keras)\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras) (13.3.5)\n",
            "Collecting namex (from keras)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
            "Requirement already satisfied: h5py in /opt/anaconda3/lib/python3.12/site-packages (from keras) (3.11.0)\n",
            "Collecting optree (from keras)\n",
            "  Downloading optree-0.14.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 kB\u001b[0m \u001b[31m134.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting ml-dtypes (from keras)\n",
            "  Downloading ml_dtypes-0.5.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (21 kB)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from keras) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from optree->keras) (4.11.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras) (0.1.0)\n",
            "Downloading keras-3.8.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m258.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m146.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.5.1-cp312-cp312-macosx_10_9_universal2.whl (670 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.4/670.4 kB\u001b[0m \u001b[31m200.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Downloading optree-0.14.0-cp312-cp312-macosx_11_0_arm64.whl (335 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.8/335.8 kB\u001b[0m \u001b[31m178.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: namex, optree, ml-dtypes, absl-py, keras\n",
            "Successfully installed absl-py-2.1.0 keras-3.8.0 ml-dtypes-0.5.1 namex-0.0.8 optree-0.14.0\n"
          ]
        }
      ],
      "source": [
        "!pip install keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f307325a-7e48-4442-800f-73e35f075681",
      "metadata": {
        "id": "f307325a-7e48-4442-800f-73e35f075681",
        "outputId": "ec4dee08-8a20-4bd5-ef6c-bcca4f308169"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.18.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
            "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
            "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.32.2)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (69.5.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow)\n",
            "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.14.1)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
            "  Downloading grpcio-1.70.0-cp312-cp312-macosx_10_14_universal2.whl.metadata (3.9 kB)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
            "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.4.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (20 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.3.5)\n",
            "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.6.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.4.1)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Downloading tensorflow-2.18.0-cp312-cp312-macosx_12_0_arm64.whl (239.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 MB\u001b[0m \u001b[31m167.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:39\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m225.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.70.0-cp312-cp312-macosx_10_14_universal2.whl (11.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m159.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m202.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.4.1-cp312-cp312-macosx_10_9_universal2.whl (405 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.1/405.1 kB\u001b[0m \u001b[31m121.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m186.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m164.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
            "Installing collected packages: libclang, flatbuffers, termcolor, tensorboard-data-server, opt-einsum, ml-dtypes, grpcio, google-pasta, gast, astunparse, tensorboard, tensorflow\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml_dtypes 0.5.1\n",
            "    Uninstalling ml_dtypes-0.5.1:\n",
            "      Successfully uninstalled ml_dtypes-0.5.1\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.70.0 libclang-18.1.1 ml-dtypes-0.4.1 opt-einsum-3.4.0 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 termcolor-2.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6YrhAjbxPeM",
        "outputId": "bc9c8b3b-2619-422d-8b4a-85e7251a5802"
      },
      "id": "x6YrhAjbxPeM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.7)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.22.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlQ_3RxXxVV0",
        "outputId": "83d85d0e-f803-4d82-c99a-799af9fad15f"
      },
      "id": "ZlQ_3RxXxVV0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m3628-pavitrakhare\u001b[0m (\u001b[33m3628-pavitrakhare-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7QCrzQsgxedB"
      },
      "id": "7QCrzQsgxedB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "# Initialize Weights & Biases (W&B) for experiment tracking\n",
        "wandb.init(project=\"DA6401_ASS-1\", name=\"Question-1\")\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Define class names for Fashion MNIST\n",
        "class_names = [\n",
        "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
        "]\n",
        "\n",
        "# Select one sample image for each class\n",
        "selected_images = []\n",
        "selected_labels = set()\n",
        "\n",
        "for i, label in enumerate(train_labels):\n",
        "    if label not in selected_labels:\n",
        "        selected_images.append((train_images[i], class_names[label]))\n",
        "        selected_labels.add(label)\n",
        "    if len(selected_images) == 10:\n",
        "        break\n",
        "\n",
        "# Plot sample images\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "fig.suptitle(\"Sample Images from Fashion MNIST\", fontsize=16)\n",
        "\n",
        "for ax, (image, label) in zip(axes.flatten(), selected_images):\n",
        "    ax.imshow(image, cmap='gray')\n",
        "    ax.set_title(label)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Log the figure to W&B\n",
        "wandb.log({\"Sample MNIST Images\": fig})\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZBch0x4-TT34",
        "outputId": "ea2c6639-b0bb-40a5-a5d5-8d294248a6fe"
      },
      "id": "ZBch0x4-TT34",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m3628-pavitrakhare\u001b[0m (\u001b[33m3628-pavitrakhare-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250307_194813-yc4zglf7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-1/runs/yc4zglf7' target=\"_blank\">Question-1</a></strong> to <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-1' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-1/runs/yc4zglf7' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-1/runs/yc4zglf7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Question-1</strong> at: <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-1/runs/yc4zglf7' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-1/runs/yc4zglf7</a><br> View project at: <a href='https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-1' target=\"_blank\">https://wandb.ai/3628-pavitrakhare-indian-institute-of-technology-madras/DA6401_ASS-1</a><br>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250307_194813-yc4zglf7/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c21950ab-3ad0-400f-b317-46581baeb552",
      "metadata": {
        "id": "c21950ab-3ad0-400f-b317-46581baeb552"
      },
      "source": [
        "# QUESTION 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation functions\n",
        "def sigmoid(x):\n",
        "    \"\"\"Computes the sigmoid activation.\"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "    \"\"\"Computes the hyperbolic tangent activation.\"\"\"\n",
        "    return np.tanh(x)\n",
        "\n",
        "def relu(x):\n",
        "    \"\"\"Computes the ReLU activation.\"\"\"\n",
        "    return np.where(x > 0, x, 0)\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Computes the softmax activation.\n",
        "\n",
        "    Parameters:\n",
        "    - x: Input array.\n",
        "\n",
        "    Returns:\n",
        "    - Softmax-transformed output.\n",
        "    \"\"\"\n",
        "    x_shifted = x - np.max(x, axis=1, keepdims=True)  # Improve numerical stability\n",
        "    exp_x = np.exp(x_shifted)\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "# Loss functions\n",
        "def cross_entropy(y_hat, y):\n",
        "    \"\"\"\n",
        "    Computes the cross-entropy loss.\n",
        "\n",
        "    Parameters:\n",
        "    - y_hat: Predicted probabilities.\n",
        "    - y: True labels (one-hot encoded).\n",
        "\n",
        "    Returns:\n",
        "    - Cross-entropy loss value.\n",
        "    \"\"\"\n",
        "    epsilon = 1e-9  # Small constant to prevent log(0)\n",
        "    return -np.sum(y * np.log(y_hat + epsilon)) / len(y_hat)\n",
        "\n",
        "def mean_squared_error(y_hat, y):\n",
        "    \"\"\"\n",
        "    Computes the mean squared error (MSE).\n",
        "\n",
        "    Parameters:\n",
        "    - y_hat: Predicted values.\n",
        "    - y: True values.\n",
        "\n",
        "    Returns:\n",
        "    - MSE loss.\n",
        "    \"\"\"\n",
        "    return np.mean((y - y_hat) ** 2)\n",
        "\n",
        "# Activation function selector\n",
        "def activation_functions(x, fn_label=\"sigmoid\"):\n",
        "    \"\"\"\n",
        "    Applies the specified activation function.\n",
        "\n",
        "    Parameters:\n",
        "    - x: Input array.\n",
        "    - fn_label: Activation function name.\n",
        "\n",
        "    Returns:\n",
        "    - Activated output.\n",
        "    \"\"\"\n",
        "    activations = {\n",
        "        \"ReLU\": relu,\n",
        "        \"sigmoid\": sigmoid,\n",
        "        \"tanh\": tanh,\n",
        "        \"softmax\": softmax\n",
        "    }\n",
        "    return activations.get(fn_label, lambda x: \"error\")(x)\n",
        "\n",
        "# Derivatives of activation functions\n",
        "def activation_derivative(x, fn_label=\"sigmoid\"):\n",
        "    \"\"\"\n",
        "    Computes the derivative of the specified activation function.\n",
        "\n",
        "    Parameters:\n",
        "    - x: Input array.\n",
        "    - fn_label: Activation function name.\n",
        "\n",
        "    Returns:\n",
        "    - Derivative of the function.\n",
        "    \"\"\"\n",
        "    derivatives = {\n",
        "        \"ReLU\": lambda x: (x > 0).astype(float),\n",
        "        \"tanh\": lambda x: 1 - np.tanh(x) ** 2,\n",
        "        \"sigmoid\": lambda x: sigmoid(x) * (1 - sigmoid(x))\n",
        "    }\n",
        "    return derivatives.get(fn_label, lambda x: \"error\")(x)\n"
      ],
      "metadata": {
        "id": "EVqI0XoFke8w"
      },
      "id": "EVqI0XoFke8w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist, mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Configuration Parameters\n",
        "DATASET_NAME = 'fashion_mnist'  # Change to 'mnist' if needed\n",
        "BETA_1 = 0.9\n",
        "BETA_2 = 0.999\n",
        "MOMENTUM_BETA = 0.9\n",
        "RMSPROP_BETA = 0.9\n",
        "EPSILON = 1e-3\n",
        "WEIGHT_DECAY = 0\n",
        "\n",
        "# Constants\n",
        "INPUT_KEY = 'input_size'\n",
        "OUTPUT_KEY = 'output_size'\n",
        "FUN_KEY = \"function\"\n",
        "\n",
        "def one_hot_encode(labels, num_classes=None):\n",
        "    \"\"\"\n",
        "    Converts label vector into a one-hot encoded matrix.\n",
        "\n",
        "    Parameters:\n",
        "    - labels (array): Array of class labels.\n",
        "    - num_classes (int): Number of unique classes. If None, inferred from labels.\n",
        "\n",
        "    Returns:\n",
        "    - np.ndarray: One-hot encoded matrix.\n",
        "    \"\"\"\n",
        "    num_classes = num_classes or (max(labels) + 1)\n",
        "    one_hot_matrix = np.zeros((len(labels), num_classes))\n",
        "    one_hot_matrix[np.arange(len(labels)), labels] = 1\n",
        "    return one_hot_matrix\n",
        "\n",
        "# Load the selected dataset\n",
        "if DATASET_NAME == 'fashion_mnist':\n",
        "    (train_X, train_Y), (test_X, test_Y) = fashion_mnist.load_data()\n",
        "elif DATASET_NAME == 'mnist':\n",
        "    (train_X, train_Y), (test_X, test_Y) = mnist.load_data()\n",
        "else:\n",
        "    raise ValueError(\"Invalid dataset name. Choose 'fashion_mnist' or 'mnist'.\")\n",
        "\n",
        "# Normalize pixel values to range [0,1]\n",
        "train_X, test_X = train_X / 255.0, test_X / 255.0\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "train_X, val_X, train_Y, val_Y = train_test_split(train_X, train_Y, test_size=0.1, random_state=40)\n",
        "\n",
        "# Reshape images into 1D feature vectors\n",
        "train_X = train_X.reshape(train_X.shape[0], -1)\n",
        "test_X = test_X.reshape(test_X.shape[0], -1)\n",
        "val_X = val_X.reshape(val_X.shape[0], -1)\n",
        "\n",
        "# Ensure data is a multiple of batch size (128)\n",
        "BATCH_SIZE = 128\n",
        "train_X = train_X[:(len(train_X) // BATCH_SIZE) * BATCH_SIZE, :]\n",
        "test_X = test_X[:(len(test_X) // BATCH_SIZE) * BATCH_SIZE, :]\n",
        "val_X = val_X[:(len(val_X) // BATCH_SIZE) * BATCH_SIZE, :]\n",
        "\n",
        "train_Y = train_Y[:(len(train_Y) // BATCH_SIZE) * BATCH_SIZE]\n",
        "test_Y = test_Y[:(len(test_Y) // BATCH_SIZE) * BATCH_SIZE]\n",
        "val_Y = val_Y[:(len(val_Y) // BATCH_SIZE) * BATCH_SIZE]\n",
        "\n",
        "# Convert labels to one-hot encoded format\n",
        "train_Y = one_hot_encode(train_Y)\n",
        "test_Y = one_hot_encode(test_Y)\n",
        "val_Y = one_hot_encode(val_Y)\n",
        "\n",
        "# Define input and output layer sizes\n",
        "input_layer_size = train_X.shape[1]\n",
        "output_layer_size = train_Y.shape[1]\n",
        "\n",
        "# Output dataset information\n",
        "print(f\"Training Data: {train_X.shape}, Labels: {train_Y.shape}\")\n",
        "print(f\"Validation Data: {val_X.shape}, Labels: {val_Y.shape}\")\n",
        "print(f\"Test Data: {test_X.shape}, Labels: {test_Y.shape}\")\n"
      ],
      "metadata": {
        "id": "9wMn710l8Gtn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d267ee45-1c11-4552-e7bb-506e6385515f"
      },
      "id": "9wMn710l8Gtn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Training Data: (53888, 784), Labels: (53888, 10)\n",
            "Validation Data: (5888, 784), Labels: (5888, 10)\n",
            "Test Data: (9984, 784), Labels: (9984, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(input_x, W, B, hidden_layers, activ_label, op_label):\n",
        "    \"\"\"\n",
        "    Performs forward propagation through a neural network.\n",
        "\n",
        "    Parameters:\n",
        "    - input_x (array): Input data.\n",
        "    - W (list): List of weight matrices.\n",
        "    - B (list): List of bias vectors.\n",
        "    - hidden_layers (int): Number of hidden layers.\n",
        "    - activ_label (str): Activation function for hidden layers.\n",
        "    - op_label (str): Activation function for the output layer.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: Lists containing activations (a) and hidden states (h) for all layers.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize lists for activations and hidden states\n",
        "    a, h = [], []\n",
        "\n",
        "    # Reshape input data into a 2D matrix if necessary\n",
        "    batch_trainX = np.reshape(input_x, (len(input_x), -1))\n",
        "\n",
        "    # Compute first layer's activation and output\n",
        "    a1 = np.dot(W[0], batch_trainX.T) + B[0]\n",
        "    h1 = activation_functions(a1, activ_label)\n",
        "\n",
        "    a.append(a1.T)\n",
        "    h.append(h1.T)\n",
        "\n",
        "    # Compute activations and outputs for hidden layers\n",
        "    for i in range(1, hidden_layers):\n",
        "        an = np.dot(W[i], h[i - 1].T) + B[i]\n",
        "        hn = activation_functions(an, activ_label)\n",
        "\n",
        "        a.append(an.T)\n",
        "        h.append(hn.T)\n",
        "\n",
        "    # Compute activation and output for the final layer\n",
        "    aL = np.dot(W[hidden_layers], h[-1].T) + B[hidden_layers]\n",
        "    hL = activation_functions(aL.T, op_label)\n",
        "\n",
        "    a.append(aL.T)\n",
        "    h.append(hL)\n",
        "\n",
        "    return a, h\n"
      ],
      "metadata": {
        "id": "Z2iUVR-qMIa_"
      },
      "id": "Z2iUVR-qMIa_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 3\n"
      ],
      "metadata": {
        "id": "J7zJCBJG9oTd"
      },
      "id": "J7zJCBJG9oTd"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jCvmYJ0PMGLg"
      },
      "id": "jCvmYJ0PMGLg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights_and_biases(layers, number_hidden_layers=1, init_type='random'):\n",
        "    \"\"\"\n",
        "    Initializes weights and biases for a neural network.\n",
        "\n",
        "    Parameters:\n",
        "    - layers (list): A list of dictionaries defining layer configurations (input and output sizes).\n",
        "    - number_hidden_layers (int): The count of hidden layers in the network (default: 1).\n",
        "    - init_type (str): Specifies the initialization method ('random' or 'Xavier').\n",
        "\n",
        "    Returns:\n",
        "    - weights (list): List of weight matrices for each layer.\n",
        "    - biases (list): List of bias vectors for each layer.\n",
        "    \"\"\"\n",
        "\n",
        "    weights, biases = [], []\n",
        "    OUTPUT_KEY = \"output_size\"\n",
        "    INPUT_KEY = \"input_size\"\n",
        "\n",
        "    for i in range(number_hidden_layers + 1):\n",
        "        input_size = layers[i][INPUT_KEY]\n",
        "        output_size = layers[i][OUTPUT_KEY]\n",
        "\n",
        "        if init_type == 'random':\n",
        "            weights.append(np.random.normal(0, 0.5, (output_size, input_size)))\n",
        "            biases.append(np.random.normal(0, 0.5, (output_size, 1)))\n",
        "        else:\n",
        "            # Xavier Initialization\n",
        "            limit = np.sqrt(6 / (input_size + output_size))\n",
        "            weights.append(np.random.uniform(-limit, limit, size=(output_size, input_size)))\n",
        "            biases.append(np.random.uniform(-limit, limit, size=(output_size, 1)))\n",
        "\n",
        "    return weights, biases\n",
        "\n"
      ],
      "metadata": {
        "id": "2rGv7e-lVg5I"
      },
      "id": "2rGv7e-lVg5I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def calculate_error(actual, predicted):\n",
        "#     samples = actual.shape[0]\n",
        "#     return -np.sum(actual * np.log(predicted + 1e-8)) / samples"
      ],
      "metadata": {
        "id": "tKkR5DovVhjT"
      },
      "id": "tKkR5DovVhjT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def activation_sigmoid_derivative(x):\n",
        "#     return x * (1 - x)"
      ],
      "metadata": {
        "id": "4zZW9jXLwEij"
      },
      "id": "4zZW9jXLwEij",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_accuracy(batch_testy, y_predicted, trainy):\n",
        "    \"\"\"\n",
        "    Computes the training accuracy by comparing predicted and actual labels.\n",
        "\n",
        "    Parameters:\n",
        "    - batch_testy (list/array): True labels in one-hot encoded format.\n",
        "    - y_predicted (list/array): Predicted probability distributions.\n",
        "    - trainy (list/array): Training labels (for normalization of accuracy).\n",
        "\n",
        "    Returns:\n",
        "    - float: Training accuracy as a fraction of correctly predicted samples.\n",
        "    \"\"\"\n",
        "\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for i in range(len(batch_testy)):\n",
        "        for j in range(len(batch_testy[i])):\n",
        "            true_label = np.argmax(batch_testy[i][j])  # Get index of the correct class\n",
        "            predicted_label = np.argmax(y_predicted[i][j])  # Get index of the highest probability\n",
        "\n",
        "            if predicted_label == true_label:\n",
        "                correct_predictions += 1\n",
        "\n",
        "    return correct_predictions / len(trainy)\n"
      ],
      "metadata": {
        "id": "WShgH71FLWjl"
      },
      "id": "WShgH71FLWjl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_accuracy(testX, testy, weights, biases, number_hidden_layers, activation_function, output_function):\n",
        "    \"\"\"\n",
        "    Computes the accuracy of a neural network model on test data.\n",
        "\n",
        "    Parameters:\n",
        "    - testX (array): Input test data.\n",
        "    - testy (array): True test labels.\n",
        "    - weights (list): List of weight matrices for each layer.\n",
        "    - biases (list): List of bias vectors for each layer.\n",
        "    - number_hidden_layers (int): Number of hidden layers in the network.\n",
        "    - activation_function (str): Activation function used in hidden layers.\n",
        "    - output_function (str): Activation function for the output layer.\n",
        "\n",
        "    Returns:\n",
        "    - float: Test accuracy as a fraction of correctly predicted samples.\n",
        "    \"\"\"\n",
        "\n",
        "    # Perform forward propagation to get predictions\n",
        "    _, activations = forward_propagation(testX, weights, biases, number_hidden_layers, activation_function, output_function)\n",
        "\n",
        "    # Extract output layer predictions\n",
        "    y_pred = activations[-1]\n",
        "\n",
        "    # Convert probabilities to class predictions\n",
        "    y_predicted = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = np.mean(y_predicted == testy)\n",
        "\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "dvYjj8QrLgxQ"
      },
      "id": "dvYjj8QrLgxQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def backward_propagation(batch_trainy, batch_trainX, y_hat, a, h, weights, number_hidden_layers, derivative_function='sigmoid'):\n",
        "    \"\"\"\n",
        "    Performs backpropagation to compute gradients of weights and biases.\n",
        "\n",
        "    Parameters:\n",
        "    - batch_trainy: True labels (one-hot encoded).\n",
        "    - batch_trainX: Input training data.\n",
        "    - y_hat: Predicted output.\n",
        "    - a: List of activation values from forward propagation.\n",
        "    - h: List of hidden state values.\n",
        "    - weights: List of weight matrices.\n",
        "    - number_hidden_layers: Number of hidden layers in the network.\n",
        "    - derivative_function: Activation function for computing derivatives.\n",
        "\n",
        "    Returns:\n",
        "    - del_W: Dictionary of gradients for weights.\n",
        "    - del_b: Dictionary of gradients for biases.\n",
        "    \"\"\"\n",
        "    del_a, del_W, del_b, del_h = {}, {}, {}, {}\n",
        "\n",
        "    batch_trainy = batch_trainy.reshape(batch_trainy.shape[0], batch_trainy.shape[1])\n",
        "\n",
        "    epsilon = 1e-8\n",
        "    last_layer_a = f'a{number_hidden_layers + 1}'\n",
        "    last_layer_h = f'h{number_hidden_layers + 1}'\n",
        "\n",
        "    del_a[last_layer_a] = -(batch_trainy - y_hat)\n",
        "    del_h[last_layer_h] = -(batch_trainy / (y_hat + epsilon))\n",
        "\n",
        "    num_samples = len(batch_trainX)\n",
        "\n",
        "    # Backpropagation from the output layer to the first hidden layer\n",
        "    for i in range(number_hidden_layers + 1, 1, -1):\n",
        "        weight_key = f'W{i}'\n",
        "        bias_key = f'b{i}'\n",
        "        activation_key = f'a{i}'\n",
        "        prev_activation_key = f'a{i-1}'\n",
        "        prev_hidden_key = f'h{i-1}'\n",
        "\n",
        "        # Compute gradient of weights using del_a and hidden activations\n",
        "        del_W[weight_key] = np.dot(del_a[activation_key].T, h[i - 2])\n",
        "\n",
        "        # L2 regularization\n",
        "        del_W[weight_key] += wdc * weights[i - 1]\n",
        "        del_W[weight_key] /= num_samples\n",
        "\n",
        "        # Compute gradient of biases\n",
        "        del_b[bias_key] = del_a[activation_key]\n",
        "\n",
        "        # Compute del_h and del_a for the previous layer\n",
        "        del_h[prev_hidden_key] = np.dot(weights[i - 1].T, del_a[activation_key].T)\n",
        "        del_a[prev_activation_key] = np.multiply(del_h[prev_hidden_key], activation_derivative(a[i - 2].T, derivative_function))\n",
        "        del_a[prev_activation_key] = del_a[prev_activation_key].T\n",
        "\n",
        "    # Compute gradients for the first layer (no del_h or del_a needed)\n",
        "    del_W['W1'] = np.dot(del_a['a1'].T, batch_trainX)\n",
        "    del_b['b1'] = del_a['a1']\n",
        "\n",
        "    # Normalize biases over the number of samples\n",
        "    for j in range(1, len(del_b) + 1):\n",
        "        bias_key = f'b{j}'\n",
        "        bias_grad = np.mean(del_b[bias_key], axis=0).reshape(-1, 1)\n",
        "        del_b[bias_key] = bias_grad\n",
        "\n",
        "    return del_W, del_b\n"
      ],
      "metadata": {
        "id": "KtAYLKDq3tRf"
      },
      "id": "KtAYLKDq3tRf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train = images[:5].reshape(5, -1)  # Flatten images\n",
        "# y_train = np.eye(10)[labels_index[:5]]  # One-hot encode labels\n",
        "\n",
        "# parameters = setup_model_params(X_train.shape[1], [128, 64], 10)\n",
        "\n",
        "# for epoch in range(10):\n",
        "#     parameters = backpropagate(X_train, y_train, parameters, [128, 64], optimizer=\"adam\", learning_rate=0.01)\n",
        "#     predictions = forward_pass(X_train, parameters, [128, 64])[f\"A{len([128, 64]) + 1}\"]  # Ensure correct indexing\n",
        "#     loss = calculate_error(y_train, predictions)\n",
        "#     print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyCzuSmy9tLS",
        "outputId": "ca0bbe7a-5a36-4db5-ac2f-c64b08ac587b"
      },
      "id": "pyCzuSmy9tLS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.3602\n",
            "Epoch 2, Loss: 1.2778\n",
            "Epoch 3, Loss: 1.1213\n",
            "Epoch 4, Loss: 1.2231\n",
            "Epoch 5, Loss: 1.0618\n",
            "Epoch 6, Loss: 1.0684\n",
            "Epoch 7, Loss: 0.9595\n",
            "Epoch 8, Loss: 0.7840\n",
            "Epoch 9, Loss: 0.8232\n",
            "Epoch 10, Loss: 0.6037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUES 3"
      ],
      "metadata": {
        "id": "fEeTgSR9Q-C4"
      },
      "id": "fEeTgSR9Q-C4"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_regularizing_term(y, weight_decay_const, number_hidden_layers, weights):\n",
        "    \"\"\"\n",
        "    Computes the L2 regularization term.\n",
        "\n",
        "    Parameters:\n",
        "    - y: True labels (for determining the number of samples).\n",
        "    - weight_decay_const: Regularization constant (λ).\n",
        "    - number_hidden_layers: Number of hidden layers in the network.\n",
        "    - weights: List of weight matrices.\n",
        "\n",
        "    Returns:\n",
        "    - reg_term: Computed regularization term.\n",
        "    \"\"\"\n",
        "    reg_term = sum(np.sum(w**2) for w in weights[:number_hidden_layers + 1])\n",
        "    return (weight_decay_const / (2 * len(y))) * reg_term\n"
      ],
      "metadata": {
        "id": "mrNlNBvDRBq_"
      },
      "id": "mrNlNBvDRBq_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_loss(valX, valy, weights, biases, number_hidden_layers, activation_function, output_function, loss_function):\n",
        "    \"\"\"\n",
        "    Computes validation loss based on the specified loss function.\n",
        "\n",
        "    Parameters:\n",
        "    - valX: Validation input data.\n",
        "    - valy: True labels for validation data.\n",
        "    - weights: List of weight matrices.\n",
        "    - biases: List of bias vectors.\n",
        "    - number_hidden_layers: Number of hidden layers in the network.\n",
        "    - activation_function: Activation function for hidden layers.\n",
        "    - output_function: Activation function for the output layer.\n",
        "    - loss_function: Type of loss function ('cross_entropy' or 'mean_squared_error').\n",
        "\n",
        "    Returns:\n",
        "    - error: Computed validation loss.\n",
        "    \"\"\"\n",
        "    _, h = forward_propagation(valX, weights, biases, number_hidden_layers, activation_function, output_function)\n",
        "    y_hat = h[-1]\n",
        "\n",
        "    if loss_function == 'cross_entropy':\n",
        "        return cross_entropy(y_hat, valy)\n",
        "    elif loss_function == 'mean_squared_error':\n",
        "        return mean_squared_error(y_hat, valy)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid loss function. Choose 'cross_entropy' or 'mean_squared_error'.\")\n"
      ],
      "metadata": {
        "id": "1pECAMH8RPdn"
      },
      "id": "1pECAMH8RPdn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(trainX, trainy, number_hidden_layers=1, hidden_layer_size=4, eta=0.1,\n",
        "                     initial_weights='random', activation_function='sigmoid', epochs=1,\n",
        "                     output_function='softmax', mini_batch_size=4, loss_function='cross_entropy',\n",
        "                     weight_decay_const=0, wandb_flag=False):\n",
        "    \"\"\"\n",
        "    Implements the gradient descent optimization algorithm for training a neural network.\n",
        "\n",
        "    Parameters:\n",
        "    - trainX: Training input data.\n",
        "    - trainy: Training labels.\n",
        "    - number_hidden_layers: Number of hidden layers in the network.\n",
        "    - hidden_layer_size: Number of neurons in each hidden layer.\n",
        "    - eta: Learning rate for weight updates.\n",
        "    - initial_weights: Initialization method for weights ('random' or pre-trained values).\n",
        "    - activation_function: Activation function for hidden layers.\n",
        "    - epochs: Number of training iterations.\n",
        "    - output_function: Activation function for the output layer.\n",
        "    - mini_batch_size: Size of mini-batches for training.\n",
        "    - loss_function: Type of loss function ('cross_entropy' or 'mean_squared_error').\n",
        "    - weight_decay_const: Weight decay constant for L2 regularization.\n",
        "    - wandb_flag: Flag to log training metrics using Weights & Biases.\n",
        "\n",
        "    Returns:\n",
        "    - y_pred: Predicted output after final training iteration.\n",
        "    - weights: Updated weight matrices.\n",
        "    - biases: Updated bias vectors.\n",
        "    - plot_lists: Lists of training loss, validation loss, training accuracy, and validation accuracy.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the architecture of the network\n",
        "    layers = [\n",
        "        {INPUT_KEY: input_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function}\n",
        "    ]\n",
        "\n",
        "    for _ in range(number_hidden_layers - 1):\n",
        "        layers.append({INPUT_KEY: hidden_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function})\n",
        "\n",
        "    layers.append({INPUT_KEY: hidden_layer_size, OUTPUT_KEY: output_layer_size, FUN_KEY: output_function})\n",
        "\n",
        "    # Initialize weights and biases\n",
        "    weights, biases = initialize_weights_and_biases(layers, number_hidden_layers, initial_weights)\n",
        "\n",
        "    # Compute number of mini-batches\n",
        "    x_val = len(trainX)\n",
        "    number_batches = x_val // mini_batch_size\n",
        "\n",
        "    # Split training data into mini-batches\n",
        "    mini_batch_trainX = np.array_split(trainX, number_batches)\n",
        "    mini_batch_trainy = np.array_split(trainy, number_batches)\n",
        "\n",
        "    # Lists to store training progress\n",
        "    train_loss_list, val_loss_list, train_acc_list, val_acc_list = [], [], [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_train_loss = 0\n",
        "        y_predicted = []\n",
        "\n",
        "        for batch_X, batch_y in zip(mini_batch_trainX, mini_batch_trainy):\n",
        "            # Forward propagation\n",
        "            a, h = forward_propagation(batch_X, weights, biases, number_hidden_layers, activation_function, output_function)\n",
        "            y_predicted.append(h[-1])\n",
        "\n",
        "            # Compute loss\n",
        "            if loss_function == 'cross_entropy':\n",
        "                total_train_loss += cross_entropy(h[-1], batch_y)\n",
        "            elif loss_function == 'mean_squared_error':\n",
        "                total_train_loss += mean_squared_error(h[-1], batch_y)\n",
        "            else:\n",
        "                raise ValueError(\"Invalid loss function. Choose 'cross_entropy' or 'mean_squared_error'.\")\n",
        "\n",
        "            # Backward propagation\n",
        "            del_W, del_b = backward_propagation(batch_y, batch_X, h[-1], a, h, weights, number_hidden_layers, activation_function)\n",
        "\n",
        "            # Update weights and biases using gradient descent\n",
        "            for i in range(len(weights)):\n",
        "                weights[i] -= eta * del_W[f'W{i+1}']\n",
        "                biases[i] -= eta * del_b[f'b{i+1}']\n",
        "\n",
        "        # Compute training loss with regularization\n",
        "        reg_term_train = calculate_regularizing_term(trainy, weight_decay_const, number_hidden_layers, weights)\n",
        "        train_loss = total_train_loss / number_batches + reg_term_train\n",
        "\n",
        "        # Compute validation loss\n",
        "        val_loss = val_loss(valX, valy, weights, biases, number_hidden_layers, activation_function, output_function, loss_function)\n",
        "        reg_term_val = calculate_regularizing_term(valy, weight_decay_const, number_hidden_layers, weights)\n",
        "        val_loss += reg_term_val\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}: Validation Loss = {val_loss:.4f}\")\n",
        "\n",
        "        # Store metrics\n",
        "        train_loss_list.append(train_loss)\n",
        "        val_loss_list.append(val_loss)\n",
        "        train_acc_list.append(train_accuracy(mini_batch_trainy, y_predicted, trainy))\n",
        "        val_acc_list.append(test_accuracy(valX, valy, weights, biases, number_hidden_layers, activation_function, output_function))\n",
        "\n",
        "        # Log metrics using Weights & Biases if enabled\n",
        "        if wandb_flag:\n",
        "            wandb.log({\n",
        "                \"loss\": train_loss,\n",
        "                \"val_loss\": val_loss,\n",
        "                \"accuracy\": train_acc_list[-1],\n",
        "                \"val_accuracy\": val_acc_list[-1],\n",
        "                \"epoch\": epoch + 1\n",
        "            })\n",
        "\n",
        "    return h[-1], weights, biases, [train_loss_list, val_loss_list, train_acc_list, val_acc_list]\n"
      ],
      "metadata": {
        "id": "rD8_Q-RNRihj"
      },
      "id": "rD8_Q-RNRihj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def momentum_based_gradient_descent(trainX, trainy, number_hidden_layers=1, hidden_layer_size=4, eta=0.1,\n",
        "                                    initial_weights='random', activation_function='sigmoid', epochs=1,\n",
        "                                    output_function='softmax', mini_batch_size=4, loss_function='cross_entropy',\n",
        "                                    weight_decay_const=0, m_beta=0.9, wandb_flag=False):\n",
        "    \"\"\"\n",
        "    Implements Momentum-Based Gradient Descent for training a neural network.\n",
        "    \"\"\"\n",
        "    # Initialize layers of the neural network\n",
        "    layers = [{INPUT_KEY: input_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function}]\n",
        "\n",
        "    for _ in range(number_hidden_layers - 1):\n",
        "        layers.append({INPUT_KEY: hidden_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function})\n",
        "\n",
        "    layers.append({INPUT_KEY: hidden_layer_size, OUTPUT_KEY: output_layer_size, FUN_KEY: output_function})\n",
        "\n",
        "    # Initialize weights and biases\n",
        "    number_batches = len(trainX) // mini_batch_size\n",
        "    weights, biases = initialize_weights_and_biases(layers, number_hidden_layers, initial_weights)\n",
        "    mini_batch_trainX = np.array_split(trainX, number_batches)\n",
        "    mini_batch_trainy = np.array_split(trainy, number_batches)\n",
        "\n",
        "    past_weights = [np.zeros_like(w) for w in weights]\n",
        "    past_biases = [np.zeros_like(b) for b in biases]\n",
        "\n",
        "    # Lists to store training progress\n",
        "    train_loss_list, val_loss_list, train_acc_list, val_acc_list = [], [], [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_train_loss = 0\n",
        "        y_predicted = []\n",
        "\n",
        "        for batch_X, batch_y in zip(mini_batch_trainX, mini_batch_trainy):\n",
        "            # Forward propagation\n",
        "            a, h = forward_propagation(batch_X, weights, biases, number_hidden_layers, activation_function, output_function)\n",
        "            y_predicted.append(h[-1])\n",
        "\n",
        "            # Compute loss\n",
        "            if loss_function == 'cross_entropy':\n",
        "                total_train_loss += cross_entropy(h[-1], batch_y)\n",
        "            elif loss_function == 'mean_squared_error':\n",
        "                total_train_loss += mean_squared_error(h[-1], batch_y)\n",
        "            else:\n",
        "                raise ValueError(\"Invalid loss function. Choose 'cross_entropy' or 'mean_squared_error'.\")\n",
        "\n",
        "            # Backward propagation\n",
        "            del_W, del_b = backward_propagation(batch_y, batch_X, h[-1], a, h, weights, number_hidden_layers, activation_function)\n",
        "\n",
        "            # Update weights and biases using momentum-based gradient descent\n",
        "            for i in range(len(weights)):\n",
        "                past_weights[i] = (m_beta * past_weights[i]) + (eta * del_W[f'W{i+1}'])\n",
        "                past_biases[i] = (m_beta * past_biases[i]) + (eta * del_b[f'b{i+1}'])\n",
        "\n",
        "                weights[i] -= past_weights[i]\n",
        "                biases[i] -= past_biases[i]\n",
        "\n",
        "        # Compute training and validation losses\n",
        "        reg_term_train = calculate_regularizing_term(trainy, weight_decay_const, number_hidden_layers, weights)\n",
        "        train_loss = (total_train_loss / number_batches) + reg_term_train\n",
        "        val_loss = val_loss(valX, valy, weights, biases, number_hidden_layers, activation_function, output_function, loss_function)\n",
        "        val_loss += calculate_regularizing_term(valy, weight_decay_const, number_hidden_layers, weights)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}: Validation Loss = {val_loss:.4f}\")\n",
        "\n",
        "        # Compute accuracies\n",
        "        train_acc = train_accuracy(mini_batch_trainy, y_predicted, trainy)\n",
        "        val_acc = test_accuracy(valX, valy, weights, biases, number_hidden_layers, activation_function, output_function)\n",
        "\n",
        "        # Store metrics\n",
        "        train_loss_list.append(train_loss)\n",
        "        val_loss_list.append(val_loss)\n",
        "        train_acc_list.append(train_acc)\n",
        "        val_acc_list.append(val_acc)\n",
        "\n",
        "        # Log metrics using Weights & Biases if enabled\n",
        "        if wandb_flag:\n",
        "            wandb.log({\"loss\": train_loss, \"val_loss\": val_loss, \"accuracy\": train_acc, \"val_accuracy\": val_acc, \"epoch\": epoch + 1})\n",
        "\n",
        "    return h[-1], weights, biases, [train_loss_list, val_loss_list, train_acc_list, val_acc_list]\n"
      ],
      "metadata": {
        "id": "Gt4GMd2zR0_u"
      },
      "id": "Gt4GMd2zR0_u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def nestrov_accelerated_gradient_descent(trainX, trainy, number_hidden_layers=1, hidden_layer_size=4, eta=0.1,\n",
        "                                          initial_weights='random', activation_function='sigmoid', epochs=1,\n",
        "                                          output_function='softmax', mini_batch_size=4, loss_function='cross_entropy',\n",
        "                                          weight_decay_const=0, m_beta=0.9, wandb_flag=False):\n",
        "    \"\"\"\n",
        "    Implements Nesterov Accelerated Gradient Descent for training a neural network.\n",
        "    \"\"\"\n",
        "    layers = [{INPUT_KEY: input_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function}]\n",
        "\n",
        "    for _ in range(number_hidden_layers - 1):\n",
        "        layers.append({INPUT_KEY: hidden_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function})\n",
        "\n",
        "    layers.append({INPUT_KEY: hidden_layer_size, OUTPUT_KEY: output_layer_size, FUN_KEY: output_function})\n",
        "\n",
        "    # Initialize weights and biases\n",
        "    weights, biases = initialize_weights_and_biases(layers, number_hidden_layers, initial_weights)\n",
        "    number_batches = len(trainX) // mini_batch_size\n",
        "\n",
        "    mini_batch_trainX = np.array_split(trainX, number_batches)\n",
        "    mini_batch_trainy = np.array_split(trainy, number_batches)\n",
        "\n",
        "    past_weights = [np.zeros_like(w) for w in weights]\n",
        "    past_biases = [np.zeros_like(b) for b in biases]\n",
        "\n",
        "    train_loss_list, val_loss_list, train_acc_list, val_acc_list = [], [], [], []\n",
        "    h = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_train_loss = 0\n",
        "        y_predicted = []\n",
        "\n",
        "        for batch_X, batch_y in zip(mini_batch_trainX, mini_batch_trainy):\n",
        "            lookahead_weights = [w - (m_beta * pw) for w, pw in zip(weights, past_weights)]\n",
        "            lookahead_biases = [b - (m_beta * pb) for b, pb in zip(biases, past_biases)]\n",
        "\n",
        "            a, h = forward_propagation(batch_X, lookahead_weights, lookahead_biases, number_hidden_layers, activation_function, output_function)\n",
        "            y_predicted.append(h[-1])\n",
        "\n",
        "            if loss_function == 'cross_entropy':\n",
        "                total_train_loss += cross_entropy(h[-1], batch_y)\n",
        "            elif loss_function == 'mean_squared_error':\n",
        "                total_train_loss += mean_squared_error(h[-1], batch_y)\n",
        "            else:\n",
        "                raise ValueError(\"Invalid loss function. Choose 'cross_entropy' or 'mean_squared_error'.\")\n",
        "\n",
        "            del_W, del_b = backward_propagation(batch_y, batch_X, h[-1], a, h, lookahead_weights, number_hidden_layers, activation_function)\n",
        "\n",
        "            for i in range(len(weights)):\n",
        "                past_weights[i] = (m_beta * past_weights[i]) + (eta * del_W[f'W{i+1}'])\n",
        "                past_biases[i] = (m_beta * past_biases[i]) + (eta * del_b[f'b{i+1}'])\n",
        "\n",
        "                weights[i] -= past_weights[i]\n",
        "                biases[i] -= past_biases[i]\n",
        "\n",
        "        reg_term_train = calculate_regularizing_term(trainy, weight_decay_const, number_hidden_layers, weights)\n",
        "        train_loss = (total_train_loss / number_batches) + reg_term_train\n",
        "        val_loss_value = val_loss(valX, valy, weights, biases, number_hidden_layers, activation_function, output_function, loss_function)\n",
        "        reg_term_val = calculate_regularizing_term(valy, weight_decay_const, number_hidden_layers, weights)\n",
        "        val_loss_value += reg_term_val\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}: Validation Loss = {val_loss_value:.4f}\")\n",
        "\n",
        "        train_acc = train_accuracy(mini_batch_trainy, y_predicted, trainy)\n",
        "        val_acc = test_accuracy(valX, valy, weights, biases, number_hidden_layers, activation_function, output_function)\n",
        "\n",
        "        train_loss_list.append(train_loss)\n",
        "        val_loss_list.append(val_loss_value)\n",
        "        train_acc_list.append(train_acc)\n",
        "        val_acc_list.append(val_acc)\n",
        "\n",
        "        if wandb_flag:\n",
        "            wandb.log({\"loss\": train_loss, \"val_loss\": val_loss_value, \"accuracy\": train_acc, \"val_accuracy\": val_acc, \"epoch\": epoch + 1})\n",
        "\n",
        "    return h[-1], weights, biases, [train_loss_list, val_loss_list, train_acc_list, val_acc_list]\n"
      ],
      "metadata": {
        "id": "9Yl2CsqcSWlu"
      },
      "id": "9Yl2CsqcSWlu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def rmsprop(trainX, trainy, number_hidden_layers=1, hidden_layer_size=4, eta=0.1, initial_weights='random',\n",
        "            activation_function='sigmoid', epochs=1, output_function='softmax', mini_batch_size=4,\n",
        "            loss_function='cross_entropy', weight_decay_const=0, wandb_flag=False):\n",
        "    \"\"\"\n",
        "    Implements RMSprop optimization for training a neural network.\n",
        "    \"\"\"\n",
        "    layers = [{INPUT_KEY: input_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function}]\n",
        "\n",
        "    for _ in range(number_hidden_layers - 1):\n",
        "        layers.append({INPUT_KEY: hidden_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function})\n",
        "\n",
        "    layers.append({INPUT_KEY: hidden_layer_size, OUTPUT_KEY: output_layer_size, FUN_KEY: output_function})\n",
        "\n",
        "    # Initialize weights and biases\n",
        "    weights, biases = initialize_weights_and_biases(layers, number_hidden_layers, initial_weights)\n",
        "\n",
        "    number_batches = len(trainX) // mini_batch_size\n",
        "    mini_batch_trainX = np.array_split(trainX, number_batches)\n",
        "    mini_batch_trainy = np.array_split(trainy, number_batches)\n",
        "\n",
        "    v_weights = [np.zeros_like(w) for w in weights]\n",
        "    v_biases = [np.zeros_like(b) for b in biases]\n",
        "\n",
        "    train_loss_list, val_loss_list, train_acc_list, val_acc_list = [], [], [], []\n",
        "    h = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_train_loss = 0\n",
        "        y_predicted = []\n",
        "\n",
        "        for batch_X, batch_y in zip(mini_batch_trainX, mini_batch_trainy):\n",
        "            a, h = forward_propagation(batch_X, weights, biases, number_hidden_layers, activation_function, output_function)\n",
        "            y_predicted.append(h[-1])\n",
        "\n",
        "            if loss_function == 'cross_entropy':\n",
        "                total_train_loss += cross_entropy(h[-1], batch_y)\n",
        "            elif loss_function == 'mean_squared_error':\n",
        "                total_train_loss += mean_squared_error(h[-1], batch_y)\n",
        "            else:\n",
        "                raise ValueError(\"Invalid loss function. Choose 'cross_entropy' or 'mean_squared_error'.\")\n",
        "\n",
        "            del_W, del_b = backward_propagation(batch_y, batch_X, h[-1], a, h, weights, number_hidden_layers, activation_function)\n",
        "\n",
        "            for i in range(len(weights)):\n",
        "                v_weights[i] = (rmsprop_beta * v_weights[i]) + ((1 - rmsprop_beta) * (del_W[f'W{i+1}'] ** 2))\n",
        "                v_biases[i] = (rmsprop_beta * v_biases[i]) + ((1 - rmsprop_beta) * (del_b[f'b{i+1}'] ** 2))\n",
        "\n",
        "                weights[i] -= (eta * del_W[f'W{i+1}']) / (np.sqrt(v_weights[i]) + epsilon_)\n",
        "                biases[i] -= (eta * del_b[f'b{i+1}']) / (np.sqrt(v_biases[i]) + epsilon_)\n",
        "\n",
        "        reg_term_train = calculate_regularizing_term(trainy, weight_decay_const, number_hidden_layers, weights)\n",
        "        train_loss = (total_train_loss / number_batches) + reg_term_train\n",
        "\n",
        "        val_loss_value = val_loss(valX, valy, weights, biases, number_hidden_layers, activation_function, output_function, loss_function)\n",
        "        reg_term_val = calculate_regularizing_term(valy, weight_decay_const, number_hidden_layers, weights)\n",
        "        val_loss_value += reg_term_val\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}: Validation Loss = {val_loss_value:.4f}\")\n",
        "\n",
        "        train_acc = train_accuracy(mini_batch_trainy, y_predicted, trainy)\n",
        "        val_acc = test_accuracy(valX, valy, weights, biases, number_hidden_layers, activation_function, output_function)\n",
        "\n",
        "        train_loss_list.append(train_loss)\n",
        "        val_loss_list.append(val_loss_value)\n",
        "        train_acc_list.append(train_acc)\n",
        "        val_acc_list.append(val_acc)\n",
        "\n",
        "        if wandb_flag:\n",
        "            wandb.log({\"loss\": train_loss, \"val_loss\": val_loss_value, \"accuracy\": train_acc, \"val_accuracy\": val_acc, \"epoch\": epoch + 1})\n",
        "\n",
        "    return h[-1], weights, biases, [train_loss_list, val_loss_list, train_acc_list, val_acc_list]"
      ],
      "metadata": {
        "id": "CEOUdLTaS1qR"
      },
      "id": "CEOUdLTaS1qR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def adam(trainX, trainy, number_hidden_layers=1, hidden_layer_size=4, eta=0.1, initial_weights='random',\n",
        "         activation_function='sigmoid', epochs=1, output_function='softmax', mini_batch_size=4,\n",
        "         loss_function='cross_entropy', weight_decay_const=0, wandb_flag=False):\n",
        "    \"\"\"\n",
        "    Implements the Adam optimization algorithm for training a neural network.\n",
        "    \"\"\"\n",
        "    layers = [{INPUT_KEY: input_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function}]\n",
        "\n",
        "    for _ in range(number_hidden_layers - 1):\n",
        "        layers.append({INPUT_KEY: hidden_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function})\n",
        "\n",
        "    layers.append({INPUT_KEY: hidden_layer_size, OUTPUT_KEY: output_layer_size, FUN_KEY: output_function})\n",
        "\n",
        "    # Initialize weights and biases\n",
        "    weights, biases = initialize_weights_and_biases(layers, number_hidden_layers, initial_weights)\n",
        "\n",
        "    number_batches = len(trainX) // mini_batch_size\n",
        "    mini_batch_trainX = np.array_split(trainX, number_batches)\n",
        "    mini_batch_trainy = np.array_split(trainy, number_batches)\n",
        "\n",
        "    # Initialize first and second moment vectors\n",
        "    m_weights = [np.zeros_like(w) for w in weights]\n",
        "    m_biases = [np.zeros_like(b) for b in biases]\n",
        "    v_weights = [np.zeros_like(w) for w in weights]\n",
        "    v_biases = [np.zeros_like(b) for b in biases]\n",
        "\n",
        "    train_loss_list, val_loss_list, train_acc_list, val_acc_list = [], [], [], []\n",
        "    h = None\n",
        "    beta1, beta2 = 0.9, 0.999\n",
        "    epsilon_ = 1e-8\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_train_loss = 0\n",
        "        y_predicted = []\n",
        "        t = 0  # Time step for bias correction\n",
        "\n",
        "        for batch_X, batch_y in zip(mini_batch_trainX, mini_batch_trainy):\n",
        "            t += 1\n",
        "            a, h = forward_propagation(batch_X, weights, biases, number_hidden_layers, activation_function, output_function)\n",
        "            y_predicted.append(h[-1])\n",
        "\n",
        "            if loss_function == 'cross_entropy':\n",
        "                total_train_loss += cross_entropy(h[-1], batch_y)\n",
        "            elif loss_function == 'mean_squared_error':\n",
        "                total_train_loss += mean_squared_error(h[-1], batch_y)\n",
        "            else:\n",
        "                raise ValueError(\"Invalid loss function. Choose 'cross_entropy' or 'mean_squared_error'.\")\n",
        "\n",
        "            del_W, del_b = backward_propagation(batch_y, batch_X, h[-1], a, h, weights, number_hidden_layers, activation_function)\n",
        "\n",
        "            for i in range(len(weights)):\n",
        "                m_weights[i] = beta1 * m_weights[i] + (1 - beta1) * del_W[f'W{i+1}']\n",
        "                m_biases[i] = beta1 * m_biases[i] + (1 - beta1) * del_b[f'b{i+1}']\n",
        "\n",
        "                v_weights[i] = beta2 * v_weights[i] + (1 - beta2) * (del_W[f'W{i+1}'] ** 2)\n",
        "                v_biases[i] = beta2 * v_biases[i] + (1 - beta2) * (del_b[f'b{i+1}'] ** 2)\n",
        "\n",
        "                m_hat_weights = m_weights[i] / (1 - beta1 ** t)\n",
        "                m_hat_biases = m_biases[i] / (1 - beta1 ** t)\n",
        "\n",
        "                v_hat_weights = v_weights[i] / (1 - beta2 ** t)\n",
        "                v_hat_biases = v_biases[i] / (1 - beta2 ** t)\n",
        "\n",
        "                weights[i] -= eta * m_hat_weights / (np.sqrt(v_hat_weights) + epsilon_)\n",
        "                biases[i] -= eta * m_hat_biases / (np.sqrt(v_hat_biases) + epsilon_)\n",
        "\n",
        "        reg_term_train = calculate_regularizing_term(trainy, weight_decay_const, number_hidden_layers, weights)\n",
        "        train_loss = (total_train_loss / number_batches) + reg_term_train\n",
        "\n",
        "        val_loss_value = val_loss(valX, valy, weights, biases, number_hidden_layers, activation_function, output_function, loss_function)\n",
        "        reg_term_val = calculate_regularizing_term(valy, weight_decay_const, number_hidden_layers, weights)\n",
        "        val_loss_value += reg_term_val\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}: Validation Loss = {val_loss_value:.4f}\")\n",
        "\n",
        "        train_acc = train_accuracy(mini_batch_trainy, y_predicted, trainy)\n",
        "        val_acc = test_accuracy(valX, valy, weights, biases, number_hidden_layers, activation_function, output_function)\n",
        "\n",
        "        train_loss_list.append(train_loss)\n",
        "        val_loss_list.append(val_loss_value)\n",
        "        train_acc_list.append(train_acc)\n",
        "        val_acc_list.append(val_acc)\n",
        "\n",
        "        if wandb_flag:\n",
        "            wandb.log({\"loss\": train_loss, \"val_loss\": val_loss_value, \"accuracy\": train_acc, \"val_accuracy\": val_acc, \"epoch\": epoch + 1})\n",
        "\n",
        "    return h[-1], weights, biases, [train_loss_list, val_loss_list, train_acc_list, val_acc_list]\n"
      ],
      "metadata": {
        "id": "i2WM48c-CU6s"
      },
      "id": "i2WM48c-CU6s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def nadam(trainX, trainy, number_hidden_layers=1, hidden_layer_size=4, eta=0.1, initial_weights='random',\n",
        "          activation_function='sigmoid', epochs=1, output_function='softmax', mini_batch_size=4,\n",
        "          loss_function='cross_entropy', weight_decay_const=0, wandb_flag=False):\n",
        "    \"\"\"\n",
        "    Implements the NAdam optimization algorithm for training a neural network.\n",
        "    \"\"\"\n",
        "    layers = [{INPUT_KEY: input_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function}]\n",
        "\n",
        "    for _ in range(number_hidden_layers - 1):\n",
        "        layers.append({INPUT_KEY: hidden_layer_size, OUTPUT_KEY: hidden_layer_size, FUN_KEY: activation_function})\n",
        "\n",
        "    layers.append({INPUT_KEY: hidden_layer_size, OUTPUT_KEY: output_layer_size, FUN_KEY: output_function})\n",
        "\n",
        "    # Initialize weights and biases\n",
        "    weights, biases = initialize_weights_and_biases(layers, number_hidden_layers, initial_weights)\n",
        "\n",
        "    number_batches = len(trainX) // mini_batch_size\n",
        "    mini_batch_trainX = np.array_split(trainX, number_batches)\n",
        "    mini_batch_trainy = np.array_split(trainy, number_batches)\n",
        "\n",
        "    # Initialize moment vectors\n",
        "    m_weights = [np.zeros_like(w) for w in weights]\n",
        "    m_biases = [np.zeros_like(b) for b in biases]\n",
        "    v_weights = [np.zeros_like(w) for w in weights]\n",
        "    v_biases = [np.zeros_like(b) for b in biases]\n",
        "\n",
        "    train_loss_list, val_loss_list, train_acc_list, val_acc_list = [], [], [], []\n",
        "    h = None\n",
        "    beta1, beta2 = 0.9, 0.999\n",
        "    epsilon_ = 1e-8\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_train_loss = 0\n",
        "        y_predicted = []\n",
        "        t = 0  # Time step for bias correction\n",
        "\n",
        "        for batch_X, batch_y in zip(mini_batch_trainX, mini_batch_trainy):\n",
        "            t += 1\n",
        "\n",
        "            # Lookahead step\n",
        "            m_hat_weights = [(beta1 * m) / (1 - beta1 ** t) for m in m_weights]\n",
        "            m_hat_biases = [(beta1 * b) / (1 - beta1 ** t) for b in m_biases]\n",
        "            v_hat_weights = [(beta2 * v) / (1 - beta2 ** t) for v in v_weights]\n",
        "            v_hat_biases = [(beta2 * v) / (1 - beta2 ** t) for v in v_biases]\n",
        "\n",
        "            adjusted_weights = [w - eta * m / (np.sqrt(v) + epsilon_) for w, m, v in zip(weights, m_hat_weights, v_hat_weights)]\n",
        "            adjusted_biases = [b - eta * m / (np.sqrt(v) + epsilon_) for b, m, v in zip(biases, m_hat_biases, v_hat_biases)]\n",
        "\n",
        "            a, h = forward_propagation(batch_X, adjusted_weights, adjusted_biases, number_hidden_layers, activation_function, output_function)\n",
        "            y_predicted.append(h[-1])\n",
        "\n",
        "            total_train_loss += cross_entropy(h[-1], batch_y) if loss_function == 'cross_entropy' else mean_squared_error(h[-1], batch_y)\n",
        "\n",
        "            del_W, del_b = backward_propagation(batch_y, batch_X, h[-1], a, h, adjusted_weights, number_hidden_layers, activation_function)\n",
        "\n",
        "            for i in range(len(weights)):\n",
        "                m_weights[i] = beta1 * m_weights[i] + (1 - beta1) * del_W[f'W{i+1}']\n",
        "                m_biases[i] = beta1 * m_biases[i] + (1 - beta1) * del_b[f'b{i+1}']\n",
        "                v_weights[i] = beta2 * v_weights[i] + (1 - beta2) * (del_W[f'W{i+1}'] ** 2)\n",
        "                v_biases[i] = beta2 * v_biases[i] + (1 - beta2) * (del_b[f'b{i+1}'] ** 2)\n",
        "\n",
        "                weights[i] -= eta * m_weights[i] / (np.sqrt(v_weights[i]) + epsilon_)\n",
        "                biases[i] -= eta * m_biases[i] / (np.sqrt(v_biases[i]) + epsilon_)\n",
        "\n",
        "        reg_term_train = calculate_regularizing_term(trainy, weight_decay_const, number_hidden_layers, weights)\n",
        "        train_loss = (total_train_loss / number_batches) + reg_term_train\n",
        "\n",
        "        val_loss_value = val_loss(valX, valy, weights, biases, number_hidden_layers, activation_function, output_function, loss_function)\n",
        "        reg_term_val = calculate_regularizing_term(valy, weight_decay_const, number_hidden_layers, weights)\n",
        "        val_loss_value += reg_term_val\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}: Validation Loss = {val_loss_value:.4f}\")\n",
        "\n",
        "        train_acc = train_accuracy(mini_batch_trainy, y_predicted, trainy)\n",
        "        val_acc = test_accuracy(valX, valy, weights, biases, number_hidden_layers, activation_function, output_function)\n",
        "\n",
        "        train_loss_list.append(train_loss)\n",
        "        val_loss_list.append(val_loss_value)\n",
        "        train_acc_list.append(train_acc)\n",
        "        val_acc_list.append(val_acc)\n",
        "\n",
        "        if wandb_flag:\n",
        "            wandb.log({\"loss\": train_loss, \"val_loss\": val_loss_value, \"accuracy\": train_acc, \"val_accuracy\": val_acc, \"epoch\": epoch + 1})\n",
        "\n",
        "    return h[-1], weights, biases, [train_loss_list, val_loss_list, train_acc_list, val_acc_list]"
      ],
      "metadata": {
        "id": "gSgA12EuC1Oq"
      },
      "id": "gSgA12EuC1Oq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "def train(trainX, trainy, testX, testy, num_hidden_layers, hidden_layer_size, learning_rate, init_type, activation_function, epochs, batch_size, loss_function, optimizer, output_function, weight_decay, wandb_flag=False):\n",
        "    \"\"\"\n",
        "    Trains a neural network using different optimization algorithms.\n",
        "    \"\"\"\n",
        "    if optimizer == 'sgd':\n",
        "        hL, weights, biases, plot_list = gradient_descent(trainX, trainy, num_hidden_layers, hidden_layer_size, learning_rate, init_type, activation_function, epochs, output_function, batch_size, loss_function, weight_decay, wandb_flag)\n",
        "    elif optimizer == 'momentum':\n",
        "        hL, weights, biases, plot_list = momentum_based_gradient_descent(trainX, trainy, num_hidden_layers, hidden_layer_size, learning_rate, init_type, activation_function, epochs, output_function, batch_size, loss_function, weight_decay, wandb_flag)\n",
        "    elif optimizer == 'nag':\n",
        "        hL, weights, biases, plot_list = nestrov_accelerated_gradient_descent(trainX, trainy, num_hidden_layers, hidden_layer_size, learning_rate, init_type, activation_function, epochs, output_function, batch_size, loss_function, weight_decay, wandb_flag)\n",
        "    elif optimizer == 'rmsprop':\n",
        "        hL, weights, biases, plot_list = rmsprop(trainX, trainy, num_hidden_layers, hidden_layer_size, learning_rate, init_type, activation_function, epochs, output_function, batch_size, loss_function, weight_decay, wandb_flag)\n",
        "    elif optimizer == 'adam':\n",
        "        hL, weights, biases, plot_list = adam(trainX, trainy, num_hidden_layers, hidden_layer_size, learning_rate, init_type, activation_function, epochs, output_function, batch_size, loss_function, weight_decay, wandb_flag)\n",
        "    elif optimizer == 'nadam':\n",
        "        hL, weights, biases, plot_list = nadam(trainX, trainy, num_hidden_layers, hidden_layer_size, learning_rate, init_type, activation_function, epochs, output_function, batch_size, loss_function, weight_decay, wandb_flag)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid optimizer selected\")\n",
        "\n",
        "    return [weights, biases, num_hidden_layers, activation_function, output_function]\n",
        "\n",
        "sweep_config = {\n",
        "    \"name\": \"Bayesian Sweep\",\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\n",
        "        \"name\": \"val_accuracy\",\n",
        "        \"goal\": \"maximize\"\n",
        "    },\n",
        "    \"parameters\": {\n",
        "        \"epochs\": {\"values\": [5, 10]},\n",
        "        \"init_type\": {\"values\": [\"random\", \"Xavier\"]},\n",
        "        \"num_hidden_layers\": {\"values\": [3, 4, 5]},\n",
        "        \"hidden_layer_size\": {\"values\": [32, 64, 128]},\n",
        "        \"activation_function\": {\"values\": ['tanh', 'sigmoid', 'ReLU']},\n",
        "        \"learning_rate\": {\"values\": [0.001, 0.0001]},\n",
        "        \"weight_decay\": {\"values\": [0, 0.0005, 0.5]},\n",
        "        \"optimizer\": {\"values\": [\"sgd\", \"momentum\", \"nag\", \"rmsprop\", \"adam\", \"nadam\"]},\n",
        "        \"batch_size\": {\"values\": [16, 32, 64]}\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"DL_Assignment_1\", entity='cs23m008')\n",
        "\n",
        "def train_data(config=None):\n",
        "    default_config = {\n",
        "        \"epochs\": 10,\n",
        "        \"num_hidden_layers\": 2,\n",
        "        \"hidden_layer_size\": 128,\n",
        "        \"weight_decay\": 0,\n",
        "        \"learning_rate\": 1e-3,\n",
        "        \"optimizer\": \"sgd\",\n",
        "        \"batch_size\": 32,\n",
        "        \"activation_function\": \"sigmoid\",\n",
        "        \"init_type\": \"Xavier\",\n",
        "        \"loss_function\": \"cross_entropy\",\n",
        "    }\n",
        "    wandb.init(project='DL_Assignment_1', entity='cs23m008', config=default_config)\n",
        "    config = wandb.config\n",
        "\n",
        "    wandb.run.name = f'hl_{config.num_hidden_layers}_sz_{config.hidden_layer_size}_bs_{config.batch_size}_ac_{config.activation_function}_w_i_{config.init_type}_lr_{config.learning_rate}_wd_{config.weight_decay}'\n",
        "\n",
        "    train(\n",
        "        trainX=trainX,\n",
        "        trainy=trainy,\n",
        "        testX=testX,\n",
        "        testy=testy,\n",
        "        num_hidden_layers=config.num_hidden_layers,\n",
        "        hidden_layer_size=config.hidden_layer_size,\n",
        "        learning_rate=config.learning_rate,\n",
        "        init_type=config.init_type,\n",
        "        activation_function=config.activation_function,\n",
        "        epochs=config.epochs,\n",
        "        batch_size=config.batch_size,\n",
        "        loss_function=config.loss_function,\n",
        "        optimizer=config.optimizer,\n",
        "        output_function='softmax',\n",
        "        weight_decay=config.weight_decay,\n",
        "        wandb_flag=True\n",
        "    )\n"
      ],
      "metadata": {
        "id": "442Tu1rkE5EH"
      },
      "id": "442Tu1rkE5EH",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}